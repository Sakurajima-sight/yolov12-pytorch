# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import contextlib
import pickle
import re
import types
from copy import deepcopy
from pathlib import Path

import thop
import torch
import torch.nn as nn

from ultralytics.nn.modules import (
    AIFI,
    C1,
    C2,
    C2PSA,
    C3,
    C3TR,
    ELAN1,
    OBB,
    PSA,
    SPP,
    SPPELAN,
    SPPF,
    AConv,
    ADown,
    Bottleneck,
    BottleneckCSP,
    C2f,
    C2fAttn,
    C2fCIB,
    C2fPSA,
    C3Ghost,
    C3k2,
    C3x,
    CBFuse,
    CBLinear,
    Classify,
    Concat,
    Conv,
    Conv2,
    ConvTranspose,
    Detect,
    DWConv,
    DWConvTranspose2d,
    Focus,
    GhostBottleneck,
    GhostConv,
    HGBlock,
    HGStem,
    ImagePoolingAttn,
    Index,
    Pose,
    RepC3,
    RepConv,
    RepNCSPELAN4,
    RepVGGDW,
    ResNetLayer,
    RTDETRDecoder,
    SCDown,
    Segment,
    TorchVision,
    WorldDetect,
    v10Detect,
    A2C2f,
)
from ultralytics.utils import DEFAULT_CFG_DICT, DEFAULT_CFG_KEYS, LOGGER, colorstr, emojis, yaml_load
from ultralytics.utils.checks import check_requirements, check_suffix, check_yaml
from ultralytics.utils.loss import (
    E2EDetectLoss,
    v8ClassificationLoss,
    v8DetectionLoss,
    v8OBBLoss,
    v8PoseLoss,
    v8SegmentationLoss,
)
from ultralytics.utils.ops import make_divisible
from ultralytics.utils.plotting import feature_visualization
from ultralytics.utils.torch_utils import (
    fuse_conv_and_bn,
    fuse_deconv_and_bn,
    initialize_weights,
    intersect_dicts,
    model_info,
    scale_img,
    time_sync,
)


class BaseModel(nn.Module):
    """BaseModel ç±»ä½œä¸ºæ‰€æœ‰ Ultralytics YOLO ç³»åˆ—æ¨¡å‹çš„åŸºç±»ã€‚"""

    def forward(self, x, *args, **kwargs):
        """
        æ‰§è¡Œæ¨¡å‹çš„å‰å‘ä¼ æ’­è¿‡ç¨‹ï¼Œå¯ä»¥ç”¨äºè®­ç»ƒæˆ–æ¨ç†ã€‚

        å¦‚æœ x æ˜¯å­—å…¸ï¼Œåˆ™è®¡ç®—å¹¶è¿”å›è®­ç»ƒçš„æŸå¤±ã€‚å¦åˆ™ï¼Œè¿”å›æ¨ç†çš„é¢„æµ‹ç»“æœã€‚

        å‚æ•°ï¼š
            x (torch.Tensor | dict): è¾“å…¥å¼ é‡ç”¨äºæ¨ç†ï¼Œæˆ–è€…åŒ…å«å›¾åƒå¼ é‡å’Œæ ‡ç­¾çš„å­—å…¸ç”¨äºè®­ç»ƒã€‚
            *args (Any): å¯å˜é•¿åº¦çš„ä½ç½®å‚æ•°ã€‚
            **kwargs (Any): ä»»æ„çš„å…³é”®å­—å‚æ•°ã€‚

        è¿”å›ï¼š
            (torch.Tensor): å¦‚æœ x æ˜¯å­—å…¸ï¼ˆè®­ç»ƒï¼‰ï¼Œåˆ™è¿”å›æŸå¤±ï¼›å¦åˆ™è¿”å›ç½‘ç»œçš„é¢„æµ‹ç»“æœï¼ˆæ¨ç†ï¼‰ã€‚
        """
        if isinstance(x, dict):  # ç”¨äºè®­ç»ƒå’ŒéªŒè¯çš„æƒ…å†µ
            return self.loss(x, *args, **kwargs)
        return self.predict(x, *args, **kwargs)

    def predict(self, x, profile=False, visualize=False, augment=False, embed=None):
        """
        æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­é€šè¿‡ç½‘ç»œã€‚

        å‚æ•°ï¼š
            x (torch.Tensor): è¾“å…¥çš„å¼ é‡ã€‚
            profile (bool): å¦‚æœä¸º Trueï¼Œæ‰“å°æ¯ä¸€å±‚çš„è®¡ç®—æ—¶é—´ï¼Œé»˜è®¤ä¸º Falseã€‚
            visualize (bool): å¦‚æœä¸º Trueï¼Œä¿å­˜æ¨¡å‹çš„ç‰¹å¾å›¾ï¼Œé»˜è®¤ä¸º Falseã€‚
            augment (bool): åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ˜¯å¦å¯¹å›¾åƒè¿›è¡Œå¢å¼ºï¼Œé»˜è®¤ä¸º Falseã€‚
            embed (list, å¯é€‰): ä¸€ä¸ªç‰¹å¾å‘é‡/åµŒå…¥çš„åˆ—è¡¨æ¥è¿”å›ã€‚

        è¿”å›ï¼š
            (torch.Tensor): æ¨¡å‹çš„æœ€åè¾“å‡ºã€‚
        """
        if augment:
            return self._predict_augment(x)
        return self._predict_once(x, profile, visualize, embed)

    def _predict_once(self, x, profile=False, visualize=False, embed=None):
        """
        æ‰§è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­é€šè¿‡ç½‘ç»œã€‚

        å‚æ•°ï¼š
            x (torch.Tensor): è¾“å…¥çš„å¼ é‡ã€‚
            profile (bool): å¦‚æœä¸º Trueï¼Œæ‰“å°æ¯ä¸€å±‚çš„è®¡ç®—æ—¶é—´ï¼Œé»˜è®¤ä¸º Falseã€‚
            visualize (bool): å¦‚æœä¸º Trueï¼Œä¿å­˜æ¨¡å‹çš„ç‰¹å¾å›¾ï¼Œé»˜è®¤ä¸º Falseã€‚
            embed (list, å¯é€‰): ä¸€ä¸ªç‰¹å¾å‘é‡/åµŒå…¥çš„åˆ—è¡¨æ¥è¿”å›ã€‚

        è¿”å›ï¼š
            (torch.Tensor): æ¨¡å‹çš„æœ€åè¾“å‡ºã€‚
        """
        y, dt, embeddings = [], [], []  # è¾“å‡º
        for m in self.model:
            if m.f != -1:  # å¦‚æœä¸æ˜¯æ¥è‡ªå‰ä¸€å±‚
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # æ¥è‡ªä¹‹å‰çš„å±‚
            if profile:
                self._profile_one_layer(m, x, dt)
            x = m(x)  # æ‰§è¡Œ
            y.append(x if m.i in self.save else None)  # ä¿å­˜è¾“å‡º
            if visualize:
                feature_visualization(x, m.type, m.i, save_dir=visualize)
            if embed and m.i in embed:
                embeddings.append(nn.functional.adaptive_avg_pool2d(x, (1, 1)).squeeze(-1).squeeze(-1))  # æ‰å¹³åŒ–
                if m.i == max(embed):
                    return torch.unbind(torch.cat(embeddings, 1), dim=0)
        return x

    def _predict_augment(self, x):
        """å¯¹è¾“å…¥å›¾åƒ x æ‰§è¡Œå¢å¼ºæ“ä½œå¹¶è¿”å›å¢å¼ºåçš„æ¨ç†ç»“æœã€‚"""
        LOGGER.warning(
            f"WARNING âš ï¸ {self.__class__.__name__} ä¸æ”¯æŒ 'augment=True' æ¨ç†ã€‚"
            f"å›é€€åˆ°å•å°ºåº¦æ¨ç†ã€‚"
        )
        return self._predict_once(x)

    def _profile_one_layer(self, m, x, dt):
        """
        å¯¹æ¨¡å‹çš„å•ä¸ªå±‚è¿›è¡Œè®¡ç®—æ—¶é—´å’Œ FLOPsï¼ˆæ¯ç§’æµ®ç‚¹è¿ç®—æ•°ï¼‰çš„åˆ†æï¼Œå¹¶å°†ç»“æœæ·»åŠ åˆ°æä¾›çš„åˆ—è¡¨ä¸­ã€‚

        å‚æ•°ï¼š
            m (nn.Module): éœ€è¦åˆ†æçš„å±‚ã€‚
            x (torch.Tensor): è¾“å…¥æ•°æ®ã€‚
            dt (list): ç”¨äºå­˜å‚¨è¯¥å±‚è®¡ç®—æ—¶é—´çš„åˆ—è¡¨ã€‚

        è¿”å›ï¼š
            None
        """
        c = m == self.model[-1] and isinstance(x, list)  # æ˜¯æœ€åä¸€å±‚ä¸”è¾“å…¥æ˜¯åˆ—è¡¨ï¼Œå¤åˆ¶è¾“å…¥ä»¥ä¿®æ­£ in-place
        flops = thop.profile(m, inputs=[x.copy() if c else x], verbose=False)[0] / 1e9 * 2 if thop else 0  # GFLOPs
        t = time_sync()
        for _ in range(10):
            m(x.copy() if c else x)
        dt.append((time_sync() - t) * 100)
        if m == self.model[0]:
            LOGGER.info(f"{'time (ms)':>10s} {'GFLOPs':>10s} {'params':>10s}  module")
        LOGGER.info(f"{dt[-1]:10.2f} {flops:10.2f} {m.np:10.0f}  {m.type}")
        if c:
            LOGGER.info(f"{sum(dt):10.2f} {'-':>10s} {'-':>10s}  Total")

    def fuse(self, verbose=True):
        """
        å°†æ¨¡å‹ä¸­çš„ `Conv2d()` å’Œ `BatchNorm2d()` å±‚èåˆä¸ºä¸€ä¸ªå±‚ï¼Œä»¥æé«˜è®¡ç®—æ•ˆç‡ã€‚

        è¿”å›ï¼š
            (nn.Module): è¿”å›èåˆåçš„æ¨¡å‹ã€‚
        """
        if not self.is_fused():
            for m in self.model.modules():
                if isinstance(m, (Conv, Conv2, DWConv)) and hasattr(m, "bn"):
                    if isinstance(m, Conv2):
                        m.fuse_convs()
                    m.conv = fuse_conv_and_bn(m.conv, m.bn)  # æ›´æ–°å·ç§¯å±‚
                    delattr(m, "bn")  # ç§»é™¤ batchnorm
                    m.forward = m.forward_fuse  # æ›´æ–°å‰å‘ä¼ æ’­
                if isinstance(m, ConvTranspose) and hasattr(m, "bn"):
                    m.conv_transpose = fuse_deconv_and_bn(m.conv_transpose, m.bn)
                    delattr(m, "bn")  # ç§»é™¤ batchnorm
                    m.forward = m.forward_fuse  # æ›´æ–°å‰å‘ä¼ æ’­
                if isinstance(m, RepConv):
                    m.fuse_convs()
                    m.forward = m.forward_fuse  # æ›´æ–°å‰å‘ä¼ æ’­
                if isinstance(m, RepVGGDW):
                    m.fuse()
                    m.forward = m.forward_fuse
            self.info(verbose=verbose)

        return self

    def is_fused(self, thresh=10):
        """
        æ£€æŸ¥æ¨¡å‹ä¸­çš„ BatchNorm å±‚æ˜¯å¦å°‘äºç»™å®šçš„é˜ˆå€¼ã€‚

        å‚æ•°ï¼š
            thresh (int, å¯é€‰): BatchNorm å±‚çš„é˜ˆå€¼ï¼Œé»˜è®¤æ˜¯ 10ã€‚

        è¿”å›ï¼š
            (bool): å¦‚æœæ¨¡å‹ä¸­çš„ BatchNorm å±‚æ•°å°‘äºé˜ˆå€¼ï¼Œåˆ™è¿”å› Trueï¼Œå¦åˆ™è¿”å› Falseã€‚
        """
        bn = tuple(v for k, v in nn.__dict__.items() if "Norm" in k)  # æ­£åˆ™åŒ–å±‚ï¼Œä¾‹å¦‚ BatchNorm2d()
        return sum(isinstance(v, bn) for v in self.modules()) < thresh  # å¦‚æœ BatchNorm å±‚æ•°å°‘äº 'thresh'ï¼Œåˆ™è¿”å› True

    def info(self, detailed=False, verbose=True, imgsz=640):
        """
        æ‰“å°æ¨¡å‹ä¿¡æ¯ã€‚

        å‚æ•°ï¼š
            detailed (bool): å¦‚æœä¸º Trueï¼Œæ‰“å°æ¨¡å‹çš„è¯¦ç»†ä¿¡æ¯ã€‚é»˜è®¤æ˜¯ Falseã€‚
            verbose (bool): å¦‚æœä¸º Trueï¼Œæ‰“å°æ¨¡å‹ä¿¡æ¯ã€‚é»˜è®¤æ˜¯ Falseã€‚
            imgsz (int): æ¨¡å‹è®­ç»ƒæ—¶ä½¿ç”¨çš„å›¾åƒå°ºå¯¸ã€‚é»˜è®¤æ˜¯ 640ã€‚
        """
        return model_info(self, detailed=detailed, verbose=verbose, imgsz=imgsz)

    def _apply(self, fn):
        """
        å°†ä¸€ä¸ªå‡½æ•°åº”ç”¨åˆ°æ¨¡å‹ä¸­æ‰€æœ‰éå‚æ•°æˆ–æ³¨å†Œç¼“å†²åŒºçš„å¼ é‡ä¸Šã€‚

        å‚æ•°ï¼š
            fn (function): è¦åº”ç”¨åˆ°æ¨¡å‹çš„å‡½æ•°ã€‚

        è¿”å›ï¼š
            (BaseModel): æ›´æ–°åçš„ BaseModel å¯¹è±¡ã€‚
        """
        self = super()._apply(fn)
        m = self.model[-1]  # Detect()
        if isinstance(m, Detect):  # åŒ…æ‹¬æ‰€æœ‰ Detect å­ç±»ï¼Œå¦‚ Segmentã€Poseã€OBBã€WorldDetect
            m.stride = fn(m.stride)
            m.anchors = fn(m.anchors)
            m.strides = fn(m.strides)
        return self

    def load(self, weights, verbose=True):
        """
        åŠ è½½é¢„è®­ç»ƒæƒé‡åˆ°æ¨¡å‹ä¸­ã€‚

        å‚æ•°:
            weights (dict | torch.nn.Module): éœ€è¦åŠ è½½çš„é¢„è®­ç»ƒæƒé‡ã€‚
            verbose (bool, optional): æ˜¯å¦æ‰“å°åŠ è½½è¿›åº¦ã€‚é»˜è®¤ä¸ºTrueã€‚
        """
        model = weights["model"] if isinstance(weights, dict) else weights  # torchvisionæ¨¡å‹ä¸æ˜¯å­—å…¸ç±»å‹
        csd = model.float().state_dict()  # ä»¥FP32æ ¼å¼è·å–æ£€æŸ¥ç‚¹çš„state_dict
        csd = intersect_dicts(csd, self.state_dict())  # è¿›è¡Œå­—å…¸äº¤é›†
        self.load_state_dict(csd, strict=False)  # åŠ è½½æƒé‡
        if verbose:
            LOGGER.info(f"å·²ä»é¢„è®­ç»ƒæƒé‡ä¸­è½¬ç§» {len(csd)}/{len(self.model.state_dict())} é¡¹")

    def loss(self, batch, preds=None):
        """
        è®¡ç®—æŸå¤±ã€‚

        å‚æ•°:
            batch (dict): ç”¨äºè®¡ç®—æŸå¤±çš„æ‰¹æ¬¡æ•°æ®
            preds (torch.Tensor | List[torch.Tensor]): é¢„æµ‹ç»“æœã€‚
        """
        if getattr(self, "criterion", None) is None:
            self.criterion = self.init_criterion()

        preds = self.forward(batch["img"]) if preds is None else preds
        return self.criterion(preds, batch)

    def init_criterion(self):
        """åˆå§‹åŒ–BaseModelçš„æŸå¤±å‡½æ•°ã€‚"""
        raise NotImplementedError("compute_loss() éœ€è¦åœ¨ä»»åŠ¡å¤´éƒ¨å®ç°")


class DetectionModel(BaseModel):
    """YOLOv8æ£€æµ‹æ¨¡å‹ã€‚"""

    def __init__(self, cfg="yolov8n.yaml", ch=3, nc=None, verbose=True):  # æ¨¡å‹ï¼Œè¾“å…¥é€šé“æ•°ï¼Œç±»åˆ«æ•°
        """ä½¿ç”¨ç»™å®šçš„é…ç½®å’Œå‚æ•°åˆå§‹åŒ–YOLOv8æ£€æµ‹æ¨¡å‹ã€‚"""
        super().__init__()
        self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)  # é…ç½®å­—å…¸
        if self.yaml["backbone"][0][2] == "Silence":
            LOGGER.warning(
                "è­¦å‘Š âš ï¸ YOLOv9 `Silence` æ¨¡å—å·²è¢«å¼ƒç”¨ï¼Œæ”¹ä¸ºä½¿ç”¨ nn.Identityã€‚"
                "è¯·åˆ é™¤æœ¬åœ° *.pt æ–‡ä»¶å¹¶é‡æ–°ä¸‹è½½æœ€æ–°çš„æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚"
            )
            self.yaml["backbone"][0][2] = "nn.Identity"

        # å®šä¹‰æ¨¡å‹
        ch = self.yaml["ch"] = self.yaml.get("ch", ch)  # è¾“å…¥é€šé“æ•°
        if nc and nc != self.yaml["nc"]:
            LOGGER.info(f"å°†æ¨¡å‹é…ç½®ä¸­çš„ nc={self.yaml['nc']} è¦†ç›–ä¸º nc={nc}")
            self.yaml["nc"] = nc  # è¦†ç›–YAMLé…ç½®ä¸­çš„ç±»åˆ«æ•°
        self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # æ¨¡å‹å’Œä¿å­˜åˆ—è¡¨
        self.names = {i: f"{i}" for i in range(self.yaml["nc"])}  # é»˜è®¤çš„ç±»åˆ«åç§°å­—å…¸
        self.inplace = self.yaml.get("inplace", True)
        self.end2end = getattr(self.model[-1], "end2end", False)

        # æ„å»ºæ­¥å¹…
        m = self.model[-1]  # Detect()
        if isinstance(m, Detect):  # åŒ…æ‹¬æ‰€æœ‰Detectå­ç±»ï¼Œå¦‚Segmentã€Poseã€OBBã€WorldDetectç­‰
            s = 256  # 2å€æœ€å°æ­¥å¹…
            m.inplace = self.inplace

            def _forward(x):
                """æ‰§è¡Œæ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œå¤„ç†ä¸åŒDetectå­ç±»ç±»å‹ã€‚"""
                if self.end2end:
                    return self.forward(x)["one2many"]
                return self.forward(x)[0] if isinstance(m, (Segment, Pose, OBB)) else self.forward(x)

            m.stride = torch.tensor([s / x.shape[-2] for x in _forward(torch.zeros(1, ch, s, s))])  # å‰å‘ä¼ æ’­
            self.stride = m.stride
            m.bias_init()  # åªæ‰§è¡Œä¸€æ¬¡
        else:
            self.stride = torch.Tensor([32])  # é»˜è®¤æ­¥å¹…ï¼Œä¾‹å¦‚å¯¹äºRTDETR

        # åˆå§‹åŒ–æƒé‡å’Œåç½®
        initialize_weights(self)
        if verbose:
            self.info()
            LOGGER.info("")

    def _predict_augment(self, x):
        """å¯¹è¾“å…¥å›¾åƒxè¿›è¡Œå¢å¼ºå¹¶è¿”å›å¢å¼ºåçš„æ¨ç†å’Œè®­ç»ƒè¾“å‡ºã€‚"""
        if getattr(self, "end2end", False) or self.__class__.__name__ != "DetectionModel":
            LOGGER.warning("è­¦å‘Š âš ï¸ æ¨¡å‹ä¸æ”¯æŒ 'augment=True'ï¼Œå›é€€ä¸ºå•å°ºåº¦é¢„æµ‹ã€‚")
            return self._predict_once(x)
        img_size = x.shape[-2:]  # é«˜åº¦ï¼Œå®½åº¦
        s = [1, 0.83, 0.67]  # ç¼©æ”¾å› å­
        f = [None, 3, None]  # ç¿»è½¬ï¼ˆ2-ä¸Šä¸‹ç¿»è½¬ï¼Œ3-å·¦å³ç¿»è½¬ï¼‰
        y = []  # è¾“å‡º
        for si, fi in zip(s, f):
            xi = scale_img(x.flip(fi) if fi else x, si, gs=int(self.stride.max()))
            yi = super().predict(xi)[0]  # å‰å‘ä¼ æ’­
            yi = self._descale_pred(yi, fi, si, img_size)
            y.append(yi)
        y = self._clip_augmented(y)  # è£å‰ªå¢å¼ºåçš„å°¾éƒ¨
        return torch.cat(y, -1), None  # å¢å¼ºåçš„æ¨ç†ï¼Œè®­ç»ƒ

    @staticmethod
    def _descale_pred(p, flips, scale, img_size, dim=1):
        """å¯¹å¢å¼ºæ¨ç†åçš„é¢„æµ‹ç»“æœè¿›è¡Œåç¼©æ”¾æ“ä½œã€‚"""
        p[:, :4] /= scale  # åç¼©æ”¾
        x, y, wh, cls = p.split((1, 1, 2, p.shape[dim] - 4), dim)
        if flips == 2:
            y = img_size[0] - y  # åå‘ä¸Šä¸‹ç¿»è½¬
        elif flips == 3:
            x = img_size[1] - x  # åå‘å·¦å³ç¿»è½¬
        return torch.cat((x, y, wh, cls), dim)

    def _clip_augmented(self, y):
        """è£å‰ªYOLOå¢å¼ºæ¨ç†çš„å°¾éƒ¨éƒ¨åˆ†ã€‚"""
        nl = self.model[-1].nl  # æ£€æµ‹å±‚çš„æ•°é‡ï¼ˆP3-P5ï¼‰
        g = sum(4**x for x in range(nl))  # ç½‘æ ¼ç‚¹
        e = 1  # æ’é™¤å±‚æ•°
        i = (y[0].shape[-1] // g) * sum(4**x for x in range(e))  # ç´¢å¼•
        y[0] = y[0][..., :-i]  # å¤§å°ºåº¦
        i = (y[-1].shape[-1] // g) * sum(4 ** (nl - 1 - x) for x in range(e))  # ç´¢å¼•
        y[-1] = y[-1][..., i:]  # å°å°ºåº¦
        return y

    def init_criterion(self):
        """åˆå§‹åŒ–DetectionModelçš„æŸå¤±å‡½æ•°ã€‚"""
        return E2EDetectLoss(self) if getattr(self, "end2end", False) else v8DetectionLoss(self)


class OBBModel(DetectionModel):
    """YOLOv8å®šå‘è¾¹ç•Œæ¡†ï¼ˆOBBï¼‰æ¨¡å‹ã€‚"""

    def __init__(self, cfg="yolov8n-obb.yaml", ch=3, nc=None, verbose=True):
        """åˆå§‹åŒ–YOLOv8 OBBæ¨¡å‹ï¼Œä½¿ç”¨ç»™å®šçš„é…ç½®å’Œå‚æ•°ã€‚"""
        super().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)

    def init_criterion(self):
        """åˆå§‹åŒ–æ¨¡å‹çš„æŸå¤±å‡½æ•°ã€‚"""
        return v8OBBLoss(self)


class SegmentationModel(DetectionModel):
    """YOLOv8 åˆ†å‰²æ¨¡å‹ã€‚"""

    def __init__(self, cfg="yolov8n-seg.yaml", ch=3, nc=None, verbose=True):
        """åˆå§‹åŒ– YOLOv8 åˆ†å‰²æ¨¡å‹ï¼Œä½¿ç”¨ç»™å®šçš„é…ç½®å’Œå‚æ•°ã€‚"""
        super().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)

    def init_criterion(self):
        """åˆå§‹åŒ–åˆ†å‰²æ¨¡å‹çš„æŸå¤±å‡½æ•°ã€‚"""
        return v8SegmentationLoss(self)


class PoseModel(DetectionModel):
    """YOLOv8 å§¿æ€æ¨¡å‹ã€‚"""

    def __init__(self, cfg="yolov8n-pose.yaml", ch=3, nc=None, data_kpt_shape=(None, None), verbose=True):
        """åˆå§‹åŒ– YOLOv8 å§¿æ€æ¨¡å‹ã€‚"""
        if not isinstance(cfg, dict):
            cfg = yaml_model_load(cfg)  # åŠ è½½æ¨¡å‹çš„ YAML é…ç½®
        if any(data_kpt_shape) and list(data_kpt_shape) != list(cfg["kpt_shape"]):
            LOGGER.info(f"å°† model.yaml ä¸­çš„ kpt_shape={cfg['kpt_shape']} æ›¿æ¢ä¸º kpt_shape={data_kpt_shape}")
            cfg["kpt_shape"] = data_kpt_shape
        super().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)

    def init_criterion(self):
        """åˆå§‹åŒ–å§¿æ€æ¨¡å‹çš„æŸå¤±å‡½æ•°ã€‚"""
        return v8PoseLoss(self)


class ClassificationModel(BaseModel):
    """YOLOv8 åˆ†ç±»æ¨¡å‹ã€‚"""

    def __init__(self, cfg="yolov8n-cls.yaml", ch=3, nc=None, verbose=True):
        """åˆå§‹åŒ–åˆ†ç±»æ¨¡å‹ï¼Œä¼ å…¥ YAML é…ç½®ã€é€šé“æ•°ã€ç±»åˆ«æ•°å’Œæ˜¯å¦æ‰“å°è¯¦ç»†ä¿¡æ¯çš„æ ‡å¿—ã€‚"""
        super().__init__()
        self._from_yaml(cfg, ch, nc, verbose)

    def _from_yaml(self, cfg, ch, nc, verbose):
        """è®¾ç½® YOLOv8 æ¨¡å‹çš„é…ç½®å¹¶å®šä¹‰æ¨¡å‹æ¶æ„ã€‚"""
        self.yaml = cfg if isinstance(cfg, dict) else yaml_model_load(cfg)  # è·å–é…ç½®å­—å…¸

        # å®šä¹‰æ¨¡å‹
        ch = self.yaml["ch"] = self.yaml.get("ch", ch)  # è¾“å…¥é€šé“æ•°
        if nc and nc != self.yaml["nc"]:
            LOGGER.info(f"å°† model.yaml ä¸­çš„ nc={self.yaml['nc']} æ›¿æ¢ä¸º nc={nc}")
            self.yaml["nc"] = nc  # è¦†ç›– YAML é…ç½®ä¸­çš„å€¼
        elif not nc and not self.yaml.get("nc", None):
            raise ValueError("æœªæŒ‡å®š ncã€‚å¿…é¡»åœ¨ model.yaml æˆ–å‡½æ•°å‚æ•°ä¸­æŒ‡å®š ncã€‚")
        self.model, self.save = parse_model(deepcopy(self.yaml), ch=ch, verbose=verbose)  # è·å–æ¨¡å‹å’Œä¿å­˜åˆ—è¡¨
        self.stride = torch.Tensor([1])  # æ²¡æœ‰æ­¥å¹…é™åˆ¶
        self.names = {i: f"{i}" for i in range(self.yaml["nc"])}  # é»˜è®¤çš„ç±»åˆ«åç§°å­—å…¸
        self.info()

    @staticmethod
    def reshape_outputs(model, nc):
        """æ ¹æ®éœ€è¦æ›´æ–° TorchVision åˆ†ç±»æ¨¡å‹çš„ç±»åˆ«æ•° 'n'ã€‚"""
        name, m = list((model.model if hasattr(model, "model") else model).named_children())[-1]  # è·å–æœ€åä¸€ä¸ªæ¨¡å—
        if isinstance(m, Classify):  # YOLO åˆ†ç±»å¤´
            if m.linear.out_features != nc:
                m.linear = nn.Linear(m.linear.in_features, nc)
        elif isinstance(m, nn.Linear):  # ResNet, EfficientNet
            if m.out_features != nc:
                setattr(model, name, nn.Linear(m.in_features, nc))
        elif isinstance(m, nn.Sequential):
            types = [type(x) for x in m]
            if nn.Linear in types:
                i = len(types) - 1 - types[::-1].index(nn.Linear)  # è·å–æœ€åä¸€ä¸ª nn.Linear çš„ç´¢å¼•
                if m[i].out_features != nc:
                    m[i] = nn.Linear(m[i].in_features, nc)
            elif nn.Conv2d in types:
                i = len(types) - 1 - types[::-1].index(nn.Conv2d)  # è·å–æœ€åä¸€ä¸ª nn.Conv2d çš„ç´¢å¼•
                if m[i].out_channels != nc:
                    m[i] = nn.Conv2d(m[i].in_channels, nc, m[i].kernel_size, m[i].stride, bias=m[i].bias is not None)

    def init_criterion(self):
        """åˆå§‹åŒ–åˆ†ç±»æ¨¡å‹çš„æŸå¤±å‡½æ•°ã€‚"""
        return v8ClassificationLoss()


class RTDETRDetectionModel(DetectionModel):
    """
    RTDETRï¼ˆåŸºäºå˜å‹å™¨çš„å®æ—¶ç›®æ ‡æ£€æµ‹å’Œè·Ÿè¸ªï¼‰æ£€æµ‹æ¨¡å‹ç±»ã€‚

    æ­¤ç±»è´Ÿè´£æ„å»º RTDETR æ¶æ„ï¼Œå®šä¹‰æŸå¤±å‡½æ•°ï¼Œå¹¶æ”¯æŒè®­ç»ƒå’Œæ¨ç†è¿‡ç¨‹ã€‚RTDETR æ˜¯ä¸€ä¸ªç›®æ ‡æ£€æµ‹å’Œè·Ÿè¸ªæ¨¡å‹ï¼Œç»§æ‰¿è‡ª DetectionModel åŸºç±»ã€‚

    å±æ€§ï¼š
        cfg (str)ï¼šé…ç½®æ–‡ä»¶è·¯å¾„æˆ–é¢„è®¾å­—ç¬¦ä¸²ã€‚é»˜è®¤ä¸º 'rtdetr-l.yaml'ã€‚
        ch (int)ï¼šè¾“å…¥é€šé“æ•°ã€‚é»˜è®¤ä¸º 3ï¼ˆRGBï¼‰ã€‚
        nc (int, å¯é€‰)ï¼šç›®æ ‡æ£€æµ‹çš„ç±»åˆ«æ•°ã€‚é»˜è®¤ä¸º Noneã€‚
        verbose (bool)ï¼šæŒ‡å®šæ˜¯å¦åœ¨åˆå§‹åŒ–æ—¶æ˜¾ç¤ºè¯¦ç»†ä¿¡æ¯ã€‚é»˜è®¤ä¸º Trueã€‚

    æ–¹æ³•ï¼š
        init_criterionï¼šåˆå§‹åŒ–ç”¨äºæŸå¤±è®¡ç®—çš„æ ‡å‡†ã€‚
        lossï¼šè®¡ç®—å¹¶è¿”å›è®­ç»ƒè¿‡ç¨‹ä¸­çš„æŸå¤±ã€‚
        predictï¼šæ‰§è¡Œå‰å‘ä¼ æ’­å¹¶è¿”å›è¾“å‡ºã€‚
    """

    def __init__(self, cfg="rtdetr-l.yaml", ch=3, nc=None, verbose=True):
        """
        åˆå§‹åŒ– RTDETRDetectionModelã€‚

        å‚æ•°ï¼š
            cfg (str)ï¼šé…ç½®æ–‡ä»¶åç§°æˆ–è·¯å¾„ã€‚
            ch (int)ï¼šè¾“å…¥é€šé“æ•°ã€‚
            nc (int, å¯é€‰)ï¼šç±»åˆ«æ•°ã€‚é»˜è®¤ä¸º Noneã€‚
            verbose (bool, å¯é€‰)ï¼šåœ¨åˆå§‹åŒ–æ—¶æ‰“å°é™„åŠ ä¿¡æ¯ã€‚é»˜è®¤ä¸º Trueã€‚
        """
        super().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)

    def init_criterion(self):
        """åˆå§‹åŒ– RTDETRDetectionModel çš„æŸå¤±å‡½æ•°ã€‚"""
        from ultralytics.models.utils.loss import RTDETRDetectionLoss

        return RTDETRDetectionLoss(nc=self.nc, use_vfl=True)

    def loss(self, batch, preds=None):
        """
        è®¡ç®—ç»™å®šæ•°æ®æ‰¹æ¬¡çš„æŸå¤±ã€‚

        å‚æ•°ï¼š
            batch (dict): åŒ…å«å›¾åƒå’Œæ ‡ç­¾æ•°æ®çš„å­—å…¸ã€‚
            preds (torch.Tensor, optional): é¢„è®¡ç®—çš„æ¨¡å‹é¢„æµ‹ç»“æœã€‚é»˜è®¤ä¸º Noneã€‚

        è¿”å›ï¼š
            (tuple): ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«æ€»æŸå¤±å’Œä¸»æŸå¤±çš„ä¸‰ä¸ªéƒ¨åˆ†ï¼ˆåœ¨å¼ é‡ä¸­ï¼‰ã€‚
        """
        if not hasattr(self, "criterion"):
            self.criterion = self.init_criterion()

        img = batch["img"]
        # æ³¨ï¼šå°† gt_bbox å’Œ gt_labels é¢„å¤„ç†ä¸ºåˆ—è¡¨ã€‚
        bs = len(img)
        batch_idx = batch["batch_idx"]
        gt_groups = [(batch_idx == i).sum().item() for i in range(bs)]
        targets = {
            "cls": batch["cls"].to(img.device, dtype=torch.long).view(-1),
            "bboxes": batch["bboxes"].to(device=img.device),
            "batch_idx": batch_idx.to(img.device, dtype=torch.long).view(-1),
            "gt_groups": gt_groups,
        }

        preds = self.predict(img, batch=targets) if preds is None else preds
        dec_bboxes, dec_scores, enc_bboxes, enc_scores, dn_meta = preds if self.training else preds[1]
        if dn_meta is None:
            dn_bboxes, dn_scores = None, None
        else:
            dn_bboxes, dec_bboxes = torch.split(dec_bboxes, dn_meta["dn_num_split"], dim=2)
            dn_scores, dec_scores = torch.split(dec_scores, dn_meta["dn_num_split"], dim=2)

        dec_bboxes = torch.cat([enc_bboxes.unsqueeze(0), dec_bboxes])  # (7, bs, 300, 4)
        dec_scores = torch.cat([enc_scores.unsqueeze(0), dec_scores])

        loss = self.criterion(
            (dec_bboxes, dec_scores), targets, dn_bboxes=dn_bboxes, dn_scores=dn_scores, dn_meta=dn_meta
        )
        # æ³¨ï¼šRTDETR ä¸­æœ‰å¤§çº¦ 12 ä¸ªæŸå¤±ï¼Œåå‘ä¼ æ’­æ—¶ä½¿ç”¨æ‰€æœ‰æŸå¤±ï¼Œä½†ä»…æ˜¾ç¤ºä¸»è¦çš„ä¸‰ä¸ªæŸå¤±ã€‚
        return sum(loss.values()), torch.as_tensor(
            [loss[k].detach() for k in ["loss_giou", "loss_class", "loss_bbox"]], device=img.device
        )

    def predict(self, x, profile=False, visualize=False, batch=None, augment=False, embed=None):
        """
        åœ¨æ¨¡å‹ä¸­æ‰§è¡Œå‰å‘ä¼ æ’­ã€‚

        å‚æ•°ï¼š
            x (torch.Tensor): è¾“å…¥å¼ é‡ã€‚
            profile (bool, optional): å¦‚æœä¸º Trueï¼Œåˆ™å¯¹æ¯å±‚è®¡ç®—æ—¶é—´è¿›è¡Œæ€§èƒ½åˆ†æã€‚é»˜è®¤ä¸º Falseã€‚
            visualize (bool, optional): å¦‚æœä¸º Trueï¼Œåˆ™ä¿å­˜ç‰¹å¾å›¾ä»¥ä¾›å¯è§†åŒ–ã€‚é»˜è®¤ä¸º Falseã€‚
            batch (dict, optional): ç”¨äºè¯„ä¼°çš„åœ°é¢çœŸå®æ•°æ®ã€‚é»˜è®¤ä¸º Noneã€‚
            augment (bool, optional): å¦‚æœä¸º Trueï¼Œåˆ™åœ¨æ¨ç†æœŸé—´æ‰§è¡Œæ•°æ®å¢å¼ºã€‚é»˜è®¤ä¸º Falseã€‚
            embed (list, optional): ä¸€ä¸ªç‰¹å¾å‘é‡/åµŒå…¥çš„åˆ—è¡¨ï¼Œç”¨äºè¿”å›ã€‚

        è¿”å›ï¼š
            (torch.Tensor): æ¨¡å‹çš„è¾“å‡ºå¼ é‡ã€‚
        """
        y, dt, embeddings = [], [], []  # è¾“å‡º
        for m in self.model[:-1]:  # é™¤äº†å¤´éƒ¨éƒ¨åˆ†
            if m.f != -1:  # å¦‚æœä¸æ˜¯æ¥è‡ªå‰ä¸€å±‚
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # æ¥è‡ªæ›´æ—©çš„å±‚
            if profile:
                self._profile_one_layer(m, x, dt)
            x = m(x)  # æ‰§è¡Œ
            y.append(x if m.i in self.save else None)  # ä¿å­˜è¾“å‡º
            if visualize:
                feature_visualization(x, m.type, m.i, save_dir=visualize)
            if embed and m.i in embed:
                embeddings.append(nn.functional.adaptive_avg_pool2d(x, (1, 1)).squeeze(-1).squeeze(-1))  # å±•å¹³
                if m.i == max(embed):
                    return torch.unbind(torch.cat(embeddings, 1), dim=0)
        head = self.model[-1]
        x = head([y[j] for j in head.f], batch)  # å¤´éƒ¨æ¨ç†
        return x


class WorldModel(DetectionModel):
    """YOLOv8 ä¸–ç•Œæ¨¡å‹ã€‚"""

    def __init__(self, cfg="yolov8s-world.yaml", ch=3, nc=None, verbose=True):
        """ä½¿ç”¨ç»™å®šçš„é…ç½®å’Œå‚æ•°åˆå§‹åŒ– YOLOv8 ä¸–ç•Œæ¨¡å‹ã€‚"""
        self.txt_feats = torch.randn(1, nc or 80, 512)  # ç‰¹å¾å ä½ç¬¦
        self.clip_model = None  # CLIP æ¨¡å‹å ä½ç¬¦
        super().__init__(cfg=cfg, ch=ch, nc=nc, verbose=verbose)

    def set_classes(self, text, batch=80, cache_clip_model=True):
        """æå‰è®¾ç½®ç±»ï¼Œä»¥ä¾¿æ¨¡å‹å¯ä»¥è¿›è¡Œç¦»çº¿æ¨ç†è€Œä¸éœ€è¦ CLIP æ¨¡å‹ã€‚"""
        try:
            import clip
        except ImportError:
            check_requirements("git+https://github.com/ultralytics/CLIP.git")
            import clip

        if (
            not getattr(self, "clip_model", None) and cache_clip_model
        ):  # ä¸ºäº†å…¼å®¹æ²¡æœ‰ clip_model å±æ€§çš„æ¨¡å‹
            self.clip_model = clip.load("ViT-B/32")[0]
        model = self.clip_model if cache_clip_model else clip.load("ViT-B/32")[0]
        device = next(model.parameters()).device
        text_token = clip.tokenize(text).to(device)
        txt_feats = [model.encode_text(token).detach() for token in text_token.split(batch)]
        txt_feats = txt_feats[0] if len(txt_feats) == 1 else torch.cat(txt_feats, dim=0)
        txt_feats = txt_feats / txt_feats.norm(p=2, dim=-1, keepdim=True)
        self.txt_feats = txt_feats.reshape(-1, len(text), txt_feats.shape[-1])
        self.model[-1].nc = len(text)

    def predict(self, x, profile=False, visualize=False, txt_feats=None, augment=False, embed=None):
        """
        åœ¨æ¨¡å‹ä¸­æ‰§è¡Œå‰å‘ä¼ æ’­ã€‚

        å‚æ•°ï¼š
            x (torch.Tensor): è¾“å…¥å¼ é‡ã€‚
            profile (bool, optional): å¦‚æœä¸º Trueï¼Œåˆ™å¯¹æ¯å±‚è®¡ç®—æ—¶é—´è¿›è¡Œæ€§èƒ½åˆ†æã€‚é»˜è®¤ä¸º Falseã€‚
            visualize (bool, optional): å¦‚æœä¸º Trueï¼Œåˆ™ä¿å­˜ç‰¹å¾å›¾ä»¥ä¾›å¯è§†åŒ–ã€‚é»˜è®¤ä¸º Falseã€‚
            txt_feats (torch.Tensor): æ–‡æœ¬ç‰¹å¾ï¼Œå¦‚æœæä¾›åˆ™ä½¿ç”¨å®ƒã€‚é»˜è®¤ä¸º Noneã€‚
            augment (bool, optional): å¦‚æœä¸º Trueï¼Œåˆ™åœ¨æ¨ç†æœŸé—´æ‰§è¡Œæ•°æ®å¢å¼ºã€‚é»˜è®¤ä¸º Falseã€‚
            embed (list, optional): ä¸€ä¸ªç‰¹å¾å‘é‡/åµŒå…¥çš„åˆ—è¡¨ï¼Œç”¨äºè¿”å›ã€‚

        è¿”å›ï¼š
            (torch.Tensor): æ¨¡å‹çš„è¾“å‡ºå¼ é‡ã€‚
        """
        txt_feats = (self.txt_feats if txt_feats is None else txt_feats).to(device=x.device, dtype=x.dtype)
        if len(txt_feats) != len(x):
            txt_feats = txt_feats.repeat(len(x), 1, 1)
        ori_txt_feats = txt_feats.clone()
        y, dt, embeddings = [], [], []  # è¾“å‡º
        for m in self.model:  # é™¤äº†å¤´éƒ¨éƒ¨åˆ†
            if m.f != -1:  # å¦‚æœä¸æ˜¯æ¥è‡ªå‰ä¸€å±‚
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]  # æ¥è‡ªæ›´æ—©çš„å±‚
            if profile:
                self._profile_one_layer(m, x, dt)
            if isinstance(m, C2fAttn):
                x = m(x, txt_feats)
            elif isinstance(m, WorldDetect):
                x = m(x, ori_txt_feats)
            elif isinstance(m, ImagePoolingAttn):
                txt_feats = m(x, txt_feats)
            else:
                x = m(x)  # æ‰§è¡Œ

            y.append(x if m.i in self.save else None)  # ä¿å­˜è¾“å‡º
            if visualize:
                feature_visualization(x, m.type, m.i, save_dir=visualize)
            if embed and m.i in embed:
                embeddings.append(nn.functional.adaptive_avg_pool2d(x, (1, 1)).squeeze(-1).squeeze(-1))  # å±•å¹³
                if m.i == max(embed):
                    return torch.unbind(torch.cat(embeddings, 1), dim=0)
        return x

    def loss(self, batch, preds=None):
        """
        è®¡ç®—æŸå¤±ã€‚

        å‚æ•°ï¼š
            batch (dict): ç”¨äºè®¡ç®—æŸå¤±çš„æ•°æ®æ‰¹æ¬¡ã€‚
            preds (torch.Tensor | List[torch.Tensor]): æ¨¡å‹é¢„æµ‹ç»“æœã€‚
        """
        if not hasattr(self, "criterion"):
            self.criterion = self.init_criterion()

        if preds is None:
            preds = self.forward(batch["img"], txt_feats=batch["txt_feats"])
        return self.criterion(preds, batch)


class Ensemble(nn.ModuleList):
    """æ¨¡å‹é›†æˆã€‚"""

    def __init__(self):
        """åˆå§‹åŒ–æ¨¡å‹é›†æˆã€‚"""
        super().__init__()

    def forward(self, x, augment=False, profile=False, visualize=False):
        """ç”Ÿæˆ YOLO ç½‘ç»œçš„æœ€ç»ˆè¾“å‡ºå±‚ã€‚"""
        y = [module(x, augment, profile, visualize)[0] for module in self]
        # y = torch.stack(y).max(0)[0]  # æœ€å¤§å€¼é›†æˆ
        # y = torch.stack(y).mean(0)  # å¹³å‡å€¼é›†æˆ
        y = torch.cat(y, 2)  # NMS é›†æˆï¼Œy çš„å½¢çŠ¶ä¸º (B, HW, C)
        return y, None  # æ¨ç†å’Œè®­ç»ƒè¾“å‡º


# å‡½æ•° ------------------------------------------------------------------------------------------------------------


@contextlib.contextmanager
def temporary_modules(modules=None, attributes=None):
    """
    ç”¨äºä¸´æ—¶æ·»åŠ æˆ–ä¿®æ”¹ Python æ¨¡å—ç¼“å­˜ï¼ˆ`sys.modules`ï¼‰ä¸­çš„æ¨¡å—çš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚

    è¯¥å‡½æ•°å¯ç”¨äºåœ¨è¿è¡Œæ—¶æ›´æ”¹æ¨¡å—è·¯å¾„ã€‚å½“ä½ é‡æ„ä»£ç æ—¶å¾ˆæœ‰ç”¨ï¼Œå¦‚æœä½ å°†æŸä¸ªæ¨¡å—ä»ä¸€ä¸ªä½ç½®ç§»åŠ¨åˆ°å¦ä¸€ä¸ªä½ç½®ï¼Œ
    ä½†ä»å¸Œæœ›æ”¯æŒæ—§çš„å¯¼å…¥è·¯å¾„ä»¥ä¿æŒå‘åå…¼å®¹æ€§ã€‚

    å‚æ•°ï¼š
        modules (dict, optional): ä¸€ä¸ªå­—å…¸ï¼Œå°†æ—§çš„æ¨¡å—è·¯å¾„æ˜ å°„åˆ°æ–°çš„æ¨¡å—è·¯å¾„ã€‚
        attributes (dict, optional): ä¸€ä¸ªå­—å…¸ï¼Œå°†æ—§æ¨¡å—çš„å±æ€§æ˜ å°„åˆ°æ–°çš„æ¨¡å—å±æ€§ã€‚

    ç¤ºä¾‹ï¼š
        ```python
        with temporary_modules({"old.module": "new.module"}, {"old.module.attribute": "new.module.attribute"}):
            import old.module  # ç°åœ¨å°†å¯¼å…¥ new.module
            from old.module import attribute  # ç°åœ¨å°†å¯¼å…¥ new.module.attribute
        ```

    æ³¨æ„ï¼š
        æ›´æ”¹ä»…åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨å†…ç”Ÿæ•ˆï¼Œå¹¶ä¸”åœ¨ä¸Šä¸‹æ–‡ç®¡ç†å™¨é€€å‡ºåä¼šæ’¤é”€ã€‚è¯·æ³¨æ„ï¼Œç›´æ¥æ“ä½œ `sys.modules` å¯èƒ½ä¼šå¯¼è‡´ä¸å¯é¢„æµ‹çš„ç»“æœï¼Œç‰¹åˆ«æ˜¯åœ¨å¤§å‹åº”ç”¨ç¨‹åºæˆ–åº“ä¸­ã€‚åœ¨ä½¿ç”¨æ­¤å‡½æ•°æ—¶è¦å°å¿ƒã€‚
    """
    if modules is None:
        modules = {}
    if attributes is None:
        attributes = {}
    import sys
    from importlib import import_module

    try:
        # è®¾ç½® sys.modules ä¸­çš„æ—§åç§°çš„å±æ€§
        for old, new in attributes.items():
            old_module, old_attr = old.rsplit(".", 1)
            new_module, new_attr = new.rsplit(".", 1)
            setattr(import_module(old_module), old_attr, getattr(import_module(new_module), new_attr))

        # åœ¨ sys.modules ä¸­è®¾ç½®æ—§åç§°çš„æ¨¡å—
        for old, new in modules.items():
            sys.modules[old] = import_module(new)

        yield
    finally:
        # åˆ é™¤ä¸´æ—¶çš„æ¨¡å—è·¯å¾„
        for old in modules:
            if old in sys.modules:
                del sys.modules[old]


class SafeClass:
    """ä¸€ä¸ªå ä½ç±»ï¼Œç”¨äºåœ¨ååºåˆ—åŒ–æ—¶æ›¿æ¢æœªçŸ¥ç±»ã€‚"""

    def __init__(self, *args, **kwargs):
        """åˆå§‹åŒ– SafeClass å®ä¾‹ï¼Œå¿½ç•¥æ‰€æœ‰å‚æ•°ã€‚"""
        pass

    def __call__(self, *args, **kwargs):
        """è¿è¡Œ SafeClass å®ä¾‹ï¼Œå¿½ç•¥æ‰€æœ‰å‚æ•°ã€‚"""
        pass


class SafeUnpickler(pickle.Unpickler):
    """è‡ªå®šä¹‰çš„ Unpicklerï¼Œç”¨äºå°†æœªçŸ¥ç±»æ›¿æ¢ä¸º SafeClassã€‚"""

    def find_class(self, module, name):
        """å°è¯•æŸ¥æ‰¾ç±»ï¼Œå¦‚æœä¸åœ¨å®‰å…¨æ¨¡å—ä¸­åˆ™è¿”å› SafeClassã€‚"""
        safe_modules = (
            "torch",
            "collections",
            "collections.abc",
            "builtins",
            "math",
            "numpy",
            # æ·»åŠ å…¶ä»–è¢«è®¤ä¸ºæ˜¯å®‰å…¨çš„æ¨¡å—
        )
        if module in safe_modules:
            return super().find_class(module, name)
        else:
            return SafeClass


def torch_safe_load(weight, safe_only=False):
    """
    å°è¯•ä½¿ç”¨ torch.load() å‡½æ•°åŠ è½½ PyTorch æ¨¡å‹ã€‚å¦‚æœå¼•å‘ ModuleNotFoundError é”™è¯¯ï¼Œå®ƒä¼šæ•è·è¯¥é”™è¯¯ï¼Œè®°å½•è­¦å‘Šä¿¡æ¯ï¼Œ
    å¹¶å°è¯•é€šè¿‡ check_requirements() å‡½æ•°å®‰è£…ç¼ºå°‘çš„æ¨¡å—ã€‚å®‰è£…å®Œæˆåï¼Œå‡½æ•°ä¼šå†æ¬¡å°è¯•ä½¿ç”¨ torch.load() åŠ è½½æ¨¡å‹ã€‚

    å‚æ•°ï¼š
        weight (str): PyTorch æ¨¡å‹çš„æ–‡ä»¶è·¯å¾„ã€‚
        safe_only (bool): å¦‚æœä¸º Trueï¼Œåœ¨åŠ è½½è¿‡ç¨‹ä¸­æ›¿æ¢æœªçŸ¥çš„ç±»ä¸º SafeClassã€‚

    ç¤ºä¾‹ï¼š
    ```python
    from ultralytics.nn.tasks import torch_safe_load

    ckpt, file = torch_safe_load("path/to/best.pt", safe_only=True)
    ```

    è¿”å›ï¼š
        ckpt (dict): åŠ è½½çš„æ¨¡å‹æ£€æŸ¥ç‚¹ã€‚
        file (str): åŠ è½½çš„æ–‡ä»¶å
    """
    from ultralytics.utils.downloads import attempt_download_asset

    check_suffix(file=weight, suffix=".pt")
    file = attempt_download_asset(weight)  # å¦‚æœæœ¬åœ°ç¼ºå¤±åˆ™åœ¨çº¿æŸ¥æ‰¾
    try:
        with temporary_modules(
            modules={
                "ultralytics.yolo.utils": "ultralytics.utils",
                "ultralytics.yolo.v8": "ultralytics.models.yolo",
                "ultralytics.yolo.data": "ultralytics.data",
            },
            attributes={
                "ultralytics.nn.modules.block.Silence": "torch.nn.Identity",  # YOLOv9e
                "ultralytics.nn.tasks.YOLOv10DetectionModel": "ultralytics.nn.tasks.DetectionModel",  # YOLOv10
                "ultralytics.utils.loss.v10DetectLoss": "ultralytics.utils.loss.E2EDetectLoss",  # YOLOv10
            },
        ):
            if safe_only:
                # é€šè¿‡è‡ªå®šä¹‰çš„ pickle æ¨¡å—åŠ è½½
                safe_pickle = types.ModuleType("safe_pickle")
                safe_pickle.Unpickler = SafeUnpickler
                safe_pickle.load = lambda file_obj: SafeUnpickler(file_obj).load()
                with open(file, "rb") as f:
                    ckpt = torch.load(f, pickle_module=safe_pickle)
            else:
                ckpt = torch.load(file, map_location="cpu")

    except ModuleNotFoundError as e:  # e.name æ˜¯ç¼ºå¤±çš„æ¨¡å—å
        if e.name == "models":
            raise TypeError(
                emojis(
                    f"ERROR âŒï¸ {weight} çœ‹èµ·æ¥æ˜¯ä¸€ä¸ª Ultralytics YOLOv5 æ¨¡å‹ï¼Œæœ€åˆæ˜¯åœ¨ "
                    f"https://github.com/ultralytics/yolov5 ä¸Šè®­ç»ƒçš„ã€‚\nè¯¥æ¨¡å‹ä¸ YOLOv8 ä¸å…¼å®¹ã€‚"
                    f"\næ¨èçš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨æœ€æ–°çš„ 'ultralytics' åŒ…é‡æ–°è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œæˆ–è€…ä½¿ç”¨ä¸€ä¸ªå®˜æ–¹ Ultralytics æ¨¡å‹è¿è¡Œå‘½ä»¤ï¼Œä¾‹å¦‚ 'yolo predict model=yolov8n.pt'"
                )
            ) from e
        LOGGER.warning(
            f"è­¦å‘Š âš ï¸ {weight} ä¼¼ä¹éœ€è¦ '{e.name}'ï¼Œä½†è¯¥æ¨¡å—ä¸åœ¨ Ultralytics çš„è¦æ±‚ä¸­ã€‚"
            f"\nAutoInstall å°†ç°åœ¨ä¸º '{e.name}' è¿è¡Œå®‰è£…ï¼Œä½†æ­¤åŠŸèƒ½å°†æ¥ä¼šè¢«ç§»é™¤ã€‚"
            f"\næ¨èçš„è§£å†³æ–¹æ¡ˆæ˜¯ä½¿ç”¨æœ€æ–°çš„ 'ultralytics' åŒ…é‡æ–°è®­ç»ƒä¸€ä¸ªæ¨¡å‹ï¼Œæˆ–è€…ä½¿ç”¨ä¸€ä¸ªå®˜æ–¹ Ultralytics æ¨¡å‹è¿è¡Œå‘½ä»¤ï¼Œä¾‹å¦‚ 'yolo predict model=yolov8n.pt'"
        )
        check_requirements(e.name)  # å®‰è£…ç¼ºå¤±çš„æ¨¡å—
        ckpt = torch.load(file, map_location="cpu")

    if not isinstance(ckpt, dict):
        # æ–‡ä»¶å¯èƒ½æ˜¯ç”¨ torch.save(model, "saved_model.pt") ä¿å­˜çš„ YOLO å®ä¾‹
        LOGGER.warning(
            f"è­¦å‘Š âš ï¸ æ–‡ä»¶ '{weight}' ä¼¼ä¹æ²¡æœ‰æ­£ç¡®ä¿å­˜æˆ–æ ¼å¼ä¸æ­£ç¡®ã€‚"
            f"ä¸ºäº†è·å¾—æœ€ä½³æ•ˆæœï¼Œè¯·ä½¿ç”¨ model.save('filename.pt') æ­£ç¡®ä¿å­˜ YOLO æ¨¡å‹ã€‚"
        )
        ckpt = {"model": ckpt.model}

    return ckpt, file


def attempt_load_weights(weights, device=None, inplace=True, fuse=False):
    """åŠ è½½ä¸€ä¸ªæ¨¡å‹æƒé‡çš„é›†åˆ weights=[a,b,c] æˆ–å•ä¸ªæ¨¡å‹æƒé‡ weights=[a] æˆ– weights=aã€‚"""
    ensemble = Ensemble()
    for w in weights if isinstance(weights, list) else [weights]:
        ckpt, w = torch_safe_load(w)  # åŠ è½½æ£€æŸ¥ç‚¹
        args = {**DEFAULT_CFG_DICT, **ckpt["train_args"]} if "train_args" in ckpt else None  # åˆå¹¶å‚æ•°
        model = (ckpt.get("ema") or ckpt["model"]).to(device).float()  # FP32 æ¨¡å‹

        # æ¨¡å‹å…¼å®¹æ€§æ›´æ–°
        model.args = args  # å°†å‚æ•°é™„åŠ åˆ°æ¨¡å‹
        model.pt_path = w  # å°† *.pt æ–‡ä»¶è·¯å¾„é™„åŠ åˆ°æ¨¡å‹
        model.task = guess_model_task(model)
        if not hasattr(model, "stride"):
            model.stride = torch.tensor([32.0])

        # æ·»åŠ åˆ°æ¨¡å‹é›†åˆ
        ensemble.append(model.fuse().eval() if fuse and hasattr(model, "fuse") else model.eval())  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    # æ¨¡å—æ›´æ–°
    for m in ensemble.modules():
        if hasattr(m, "inplace"):
            m.inplace = inplace
        elif isinstance(m, nn.Upsample) and not hasattr(m, "recompute_scale_factor"):
            m.recompute_scale_factor = None  # å…¼å®¹ torch 1.11.0

    # è¿”å›æ¨¡å‹
    if len(ensemble) == 1:
        return ensemble[-1]

    # è¿”å›æ¨¡å‹é›†åˆ
    LOGGER.info(f"å·²åˆ›å»ºåŒ…å«ä»¥ä¸‹æƒé‡çš„æ¨¡å‹é›†åˆï¼š{weights}\n")
    for k in "names", "nc", "yaml":
        setattr(ensemble, k, getattr(ensemble[0], k))
    ensemble.stride = ensemble[int(torch.argmax(torch.tensor([m.stride.max() for m in ensemble])))].stride
    assert all(ensemble[0].nc == m.nc for m in ensemble), f"æ¨¡å‹ç±»åˆ«æ•°ä¸åŒ {[m.nc for m in ensemble]}"
    return ensemble


def attempt_load_one_weight(weight, device=None, inplace=True, fuse=False):
    """åŠ è½½å•ä¸ªæ¨¡å‹çš„æƒé‡ã€‚"""
    ckpt, weight = torch_safe_load(weight)  # åŠ è½½æ£€æŸ¥ç‚¹
    args = {**DEFAULT_CFG_DICT, **(ckpt.get("train_args", {}))}  # åˆå¹¶æ¨¡å‹å‚æ•°å’Œé»˜è®¤å‚æ•°ï¼Œä¼˜å…ˆä½¿ç”¨æ¨¡å‹å‚æ•°
    model = (ckpt.get("ema") or ckpt["model"]).to(device).float()  # FP32 æ¨¡å‹

    # æ¨¡å‹å…¼å®¹æ€§æ›´æ–°
    model.args = {k: v for k, v in args.items() if k in DEFAULT_CFG_KEYS}  # å°†å‚æ•°é™„åŠ åˆ°æ¨¡å‹
    model.pt_path = weight  # å°† *.pt æ–‡ä»¶è·¯å¾„é™„åŠ åˆ°æ¨¡å‹
    model.task = guess_model_task(model)
    if not hasattr(model, "stride"):
        model.stride = torch.tensor([32.0])

    model = model.fuse().eval() if fuse and hasattr(model, "fuse") else model.eval()  # å°†æ¨¡å‹è®¾ç½®ä¸ºè¯„ä¼°æ¨¡å¼

    # æ¨¡å—æ›´æ–°
    for m in model.modules():
        if hasattr(m, "inplace"):
            m.inplace = inplace
        elif isinstance(m, nn.Upsample) and not hasattr(m, "recompute_scale_factor"):
            m.recompute_scale_factor = None  # å…¼å®¹ torch 1.11.0

    # è¿”å›æ¨¡å‹å’Œæ£€æŸ¥ç‚¹
    return model, ckpt


def parse_model(d, ch, verbose=True):  # model_dict, input_channels(3)
    """å°† YOLO æ¨¡å‹çš„ yaml å­—å…¸è§£æä¸º PyTorch æ¨¡å‹ã€‚"""
    import ast

    # å‚æ•°
    legacy = True  # å‘åå…¼å®¹ v3/v5/v8/v9 æ¨¡å‹
    max_channels = float("inf")
    nc, act, scales = (d.get(x) for x in ("nc", "activation", "scales"))
    depth, width, kpt_shape = (d.get(x, 1.0) for x in ("depth_multiple", "width_multiple", "kpt_shape"))
    if scales:
        scale = d.get("scale")
        if not scale:
            scale = tuple(scales.keys())[0]
            LOGGER.warning(f"è­¦å‘Š âš ï¸ æ²¡æœ‰ä¼ é€’æ¨¡å‹ç¼©æ”¾å‚æ•°ã€‚å‡è®¾ scale='{scale}'ã€‚")
        depth, width, max_channels = scales[scale]

    if act:
        Conv.default_act = eval(act)  # é‡æ–°å®šä¹‰é»˜è®¤æ¿€æ´»å‡½æ•°ï¼Œä¾‹å¦‚ Conv.default_act = nn.SiLU()
        if verbose:
            LOGGER.info(f"{colorstr('æ¿€æ´»å‡½æ•°:')} {act}")  # æ‰“å°æ¿€æ´»å‡½æ•°

    if verbose:
        LOGGER.info(f"\n{'':>3}{'æ¥æº':>20}{'æ•°é‡':>3}{'å‚æ•°':>10}  {'æ¨¡å—':<45}{'å‚æ•°':<30}")
    ch = [ch]
    layers, save, c2 = [], [], ch[-1]  # å±‚ã€ä¿å­˜åˆ—è¡¨ã€è¾“å‡ºé€šé“
    for i, (f, n, m, args) in enumerate(d["backbone"] + d["head"]):  # æ¥æºï¼Œæ•°é‡ï¼Œæ¨¡å—ï¼Œå‚æ•°
        m = getattr(torch.nn, m[3:]) if "nn." in m else globals()[m]  # è·å–æ¨¡å—
        for j, a in enumerate(args):
            if isinstance(a, str):
                with contextlib.suppress(ValueError):
                    args[j] = locals()[a] if a in locals() else ast.literal_eval(a)
        n = n_ = max(round(n * depth), 1) if n > 1 else n  # æ·±åº¦å¢ç›Š
        if m in {
            Classify,
            Conv,
            ConvTranspose,
            GhostConv,
            Bottleneck,
            GhostBottleneck,
            SPP,
            SPPF,
            C2fPSA,
            C2PSA,
            DWConv,
            Focus,
            BottleneckCSP,
            C1,
            C2,
            C2f,
            C3k2,
            RepNCSPELAN4,
            ELAN1,
            ADown,
            AConv,
            SPPELAN,
            C2fAttn,
            C3,
            C3TR,
            C3Ghost,
            nn.ConvTranspose2d,
            DWConvTranspose2d,
            C3x,
            RepC3,
            PSA,
            SCDown,
            C2fCIB,
            A2C2f,
        }:
            c1, c2 = ch[f], args[0]
            if c2 != nc:  # å¦‚æœ c2 ä¸ç­‰äºç±»åˆ«æ•°ï¼ˆå³ Classify() è¾“å‡ºï¼‰
                c2 = make_divisible(min(c2, max_channels) * width, 8)
            if m is C2fAttn:
                args[1] = make_divisible(min(args[1], max_channels // 2) * width, 8)  # åµŒå…¥é€šé“æ•°
                args[2] = int(
                    max(round(min(args[2], max_channels // 2 // 32)) * width, 1) if args[2] > 1 else args[2]
                )  # å¤´æ•°

            args = [c1, c2, *args[1:]]
            if m in {
                BottleneckCSP,
                C1,
                C2,
                C2f,
                C3k2,
                C2fAttn,
                C3,
                C3TR,
                C3Ghost,
                C3x,
                RepC3,
                C2fPSA,
                C2fCIB,
                C2PSA,
                A2C2f,
            }:
                args.insert(2, n)  # é‡å¤æ¬¡æ•°
                n = 1
            if m is C3k2:  # å¯¹ M/L/X å°ºå¯¸
                legacy = False
                if scale in "mlx":
                    args[3] = True
            if m is A2C2f:  # å¯¹ M/L/X å°ºå¯¸
                legacy = False
                if scale in "mlx":
                    args[3] = True
                if scale in "lx":
                    args.append(True)
        elif m is AIFI:
            args = [ch[f], *args]
        elif m in {HGStem, HGBlock}:
            c1, cm, c2 = ch[f], args[0], args[1]
            args = [c1, cm, c2, *args[2:]]
            if m is HGBlock:
                args.insert(4, n)  # é‡å¤æ¬¡æ•°
                n = 1
        elif m is ResNetLayer:
            c2 = args[1] if args[3] else args[1] * 4
        elif m is nn.BatchNorm2d:
            args = [ch[f]]
        elif m is Concat:
            c2 = sum(ch[x] for x in f)
        elif m in {Detect, WorldDetect, Segment, Pose, OBB, ImagePoolingAttn, v10Detect}:
            args.append([ch[x] for x in f])
            if m is Segment:
                args[2] = make_divisible(min(args[2], max_channels) * width, 8)
            if m in {Detect, Segment, Pose, OBB}:
                m.legacy = legacy
        elif m is RTDETRDecoder:  # ç‰¹æ®Šæƒ…å†µï¼Œé€šé“å‚æ•°å¿…é¡»ä¼ é€’ç»™ç´¢å¼• 1
            args.insert(1, [ch[x] for x in f])
        elif m in {CBLinear, TorchVision, Index}:
            c2 = args[0]
            c1 = ch[f]
            args = [c1, c2, *args[1:]]
        elif m is CBFuse:
            c2 = ch[f[-1]]
        else:
            c2 = ch[f]

        m_ = nn.Sequential(*(m(*args) for _ in range(n))) if n > 1 else m(*args)  # æ¨¡å—
        t = str(m)[8:-2].replace("__main__.", "")  # æ¨¡å—ç±»å‹
        m_.np = sum(x.numel() for x in m_.parameters())  # å‚æ•°æ•°é‡
        m_.i, m_.f, m_.type = i, f, t  # é™„åŠ ç´¢å¼•ï¼Œ'from' ç´¢å¼•ï¼Œç±»å‹
        if verbose:
            LOGGER.info(f"{i:>3}{str(f):>20}{n_:>3}{m_.np:10.0f}  {t:<45}{str(args):<30}")  # æ‰“å°ä¿¡æ¯
        save.extend(x % i for x in ([f] if isinstance(f, int) else f) if x != -1)  # æ·»åŠ åˆ°ä¿å­˜åˆ—è¡¨
        layers.append(m_)
        if i == 0:
            ch = []
        ch.append(c2)
    return nn.Sequential(*layers), sorted(save)


def yaml_model_load(path):
    """ä» YAML æ–‡ä»¶åŠ è½½ YOLOv8 æ¨¡å‹ã€‚"""
    path = Path(path)
    if path.stem in (f"yolov{d}{x}6" for x in "nsmlx" for d in (5, 8)):
        new_stem = re.sub(r"(\d+)([nslmx])6(.+)?$", r"\1\2-p6\3", path.stem)
        LOGGER.warning(f"WARNING âš ï¸ Ultralytics YOLO P6 æ¨¡å‹ç°åœ¨ä½¿ç”¨ -p6 åç¼€ã€‚æ­£åœ¨å°† {path.stem} é‡å‘½åä¸º {new_stem}ã€‚")
        path = path.with_name(new_stem + path.suffix)

    unified_path = re.sub(r"(\d+)([nslmx])(.+)?$", r"\1\3", str(path))  # å³ yolov8x.yaml -> yolov8.yaml
    yaml_file = check_yaml(unified_path, hard=False) or check_yaml(path)
    d = yaml_load(yaml_file)  # æ¨¡å‹å­—å…¸
    d["scale"] = guess_model_scale(path)
    d["yaml_file"] = str(path)
    return d


def guess_model_scale(model_path):
    """
    è¾“å…¥ YOLO æ¨¡å‹çš„ YAML æ–‡ä»¶è·¯å¾„ï¼Œæå–æ¨¡å‹è§„æ¨¡çš„å­—ç¬¦ã€‚è¯¥å‡½æ•°ä½¿ç”¨æ­£åˆ™è¡¨è¾¾å¼åŒ¹é…æ¨¡å‹è§„æ¨¡çš„æ¨¡å¼ï¼Œ
    æ¨¡å‹è§„æ¨¡é€šè¿‡ nã€sã€mã€l æˆ– x è¡¨ç¤ºã€‚å‡½æ•°è¿”å›æ¨¡å‹è§„æ¨¡çš„å­—ç¬¦ä½œä¸ºå­—ç¬¦ä¸²ã€‚

    å‚æ•°ï¼š
        model_path (str | Path): YOLO æ¨¡å‹çš„ YAML æ–‡ä»¶è·¯å¾„ã€‚

    è¿”å›ï¼š
        (str): æ¨¡å‹è§„æ¨¡çš„å­—ç¬¦ï¼Œå¯ä»¥æ˜¯ nã€sã€mã€l æˆ– xã€‚
    """
    try:
        return re.search(r"yolo[v]?\d+([nslmx])", Path(model_path).stem).group(1)  # noqaï¼Œè¿”å› nã€sã€mã€l æˆ– x
    except AttributeError:
        return ""


def guess_model_task(model):
    """
    ä» PyTorch æ¨¡å‹çš„æ¶æ„æˆ–é…ç½®ä¸­æ¨æµ‹æ¨¡å‹çš„ä»»åŠ¡ã€‚

    å‚æ•°ï¼š
        model (nn.Module | dict): PyTorch æ¨¡å‹æˆ– YAML æ ¼å¼çš„æ¨¡å‹é…ç½®ã€‚

    è¿”å›ï¼š
        (str): æ¨¡å‹çš„ä»»åŠ¡ï¼ˆ'detect'ï¼Œ'segment'ï¼Œ'classify'ï¼Œ'pose'ï¼‰ã€‚

    å¼‚å¸¸ï¼š
        SyntaxError: å¦‚æœæ— æ³•ç¡®å®šæ¨¡å‹çš„ä»»åŠ¡ï¼Œåˆ™å¼•å‘è¯¥å¼‚å¸¸ã€‚
    """

    def cfg2task(cfg):
        """ä» YAML å­—å…¸ä¸­æ¨æµ‹ä»»åŠ¡ã€‚"""
        m = cfg["head"][-1][-2].lower()  # è¾“å‡ºæ¨¡å—åç§°
        if m in {"classify", "classifier", "cls", "fc"}:
            return "classify"
        if "detect" in m:
            return "detect"
        if m == "segment":
            return "segment"
        if m == "pose":
            return "pose"
        if m == "obb":
            return "obb"

    # ä»æ¨¡å‹é…ç½®ä¸­æ¨æµ‹ä»»åŠ¡
    if isinstance(model, dict):
        with contextlib.suppress(Exception):
            return cfg2task(model)
    # ä» PyTorch æ¨¡å‹ä¸­æ¨æµ‹ä»»åŠ¡
    if isinstance(model, nn.Module):  # PyTorch æ¨¡å‹
        for x in "model.args", "model.model.args", "model.model.model.args":
            with contextlib.suppress(Exception):
                return eval(x)["task"]
        for x in "model.yaml", "model.model.yaml", "model.model.model.yaml":
            with contextlib.suppress(Exception):
                return cfg2task(eval(x))
        for m in model.modules():
            if isinstance(m, Segment):
                return "segment"
            elif isinstance(m, Classify):
                return "classify"
            elif isinstance(m, Pose):
                return "pose"
            elif isinstance(m, OBB):
                return "obb"
            elif isinstance(m, (Detect, WorldDetect, v10Detect)):
                return "detect"

    # ä»æ¨¡å‹æ–‡ä»¶åæ¨æµ‹ä»»åŠ¡
    if isinstance(model, (str, Path)):
        model = Path(model)
        if "-seg" in model.stem or "segment" in model.parts:
            return "segment"
        elif "-cls" in model.stem or "classify" in model.parts:
            return "classify"
        elif "-pose" in model.stem or "pose" in model.parts:
            return "pose"
        elif "-obb" in model.stem or "obb" in model.parts:
            return "obb"
        elif "detect" in model.parts:
            return "detect"

    # æ— æ³•ä»æ¨¡å‹ä¸­ç¡®å®šä»»åŠ¡
    LOGGER.warning(
        "WARNING âš ï¸ æ— æ³•è‡ªåŠ¨æ¨æµ‹æ¨¡å‹ä»»åŠ¡ï¼Œå‡è®¾ä»»åŠ¡ä¸º 'task=detect'ã€‚"
        "è¯·æ˜¾å¼å®šä¹‰æ¨¡å‹çš„ä»»åŠ¡ï¼Œå¦‚ 'task=detect'ï¼Œ'segment'ï¼Œ'classify'ï¼Œ'pose' æˆ– 'obb'ã€‚"
    )
    return "detect"  # å‡è®¾ä»»åŠ¡ä¸º detect

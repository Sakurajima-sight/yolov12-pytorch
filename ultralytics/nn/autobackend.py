# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import ast
import json
import platform
import zipfile
from collections import OrderedDict, namedtuple
from pathlib import Path

import cv2
import numpy as np
import torch
import torch.nn as nn
from PIL import Image

from ultralytics.utils import ARM64, IS_JETSON, IS_RASPBERRYPI, LINUX, LOGGER, ROOT, yaml_load
from ultralytics.utils.checks import check_requirements, check_suffix, check_version, check_yaml
from ultralytics.utils.downloads import attempt_download_asset, is_url


def check_class_names(names):
    """
    æ£€æŸ¥ç±»åã€‚

    å¦‚æœéœ€è¦ï¼Œå°† imagenet ç±»ä»£ç æ˜ å°„åˆ°å¯è¯»çš„ç±»åã€‚å°†åˆ—è¡¨è½¬æ¢ä¸ºå­—å…¸ã€‚
    """
    if isinstance(names, list):  # å¦‚æœ names æ˜¯ä¸€ä¸ªåˆ—è¡¨
        names = dict(enumerate(names))  # è½¬æ¢ä¸ºå­—å…¸
    if isinstance(names, dict):
        # è½¬æ¢ 1) å­—ç¬¦ä¸²ç±»å‹çš„é”®ä¸ºæ•´æ•°ï¼Œä¾‹å¦‚ '0' è½¬ä¸º 0ï¼Œéå­—ç¬¦ä¸²ç±»å‹çš„å€¼è½¬æ¢ä¸ºå­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ True è½¬ä¸º 'True'
        names = {int(k): str(v) for k, v in names.items()}
        n = len(names)
        if max(names.keys()) >= n:
            raise KeyError(
                f"{n} ç±»æ•°æ®é›†éœ€è¦ç±»ç´¢å¼•åœ¨ 0 åˆ° {n - 1} ä¹‹é—´ï¼Œä½†åœ¨ä½ çš„æ•°æ®é›† YAML æ–‡ä»¶ä¸­å®šä¹‰çš„ç±»ç´¢å¼• "
                f"æ˜¯ {min(names.keys())}-{max(names.keys())}ï¼Œå­˜åœ¨æ— æ•ˆçš„ç±»ç´¢å¼•ã€‚"
            )
        if isinstance(names[0], str) and names[0].startswith("n0"):  # å¦‚æœæ˜¯ imagenet ç±»ä»£ç ï¼Œä¾‹å¦‚ 'n01440764'
            names_map = yaml_load(ROOT / "cfg/datasets/ImageNet.yaml")["map"]  # å¯è¯»çš„ç±»åæ˜ å°„
            names = {k: names_map[v] for k, v in names.items()}
    return names


def default_class_names(data=None):
    """åº”ç”¨é»˜è®¤çš„ç±»ååˆ°è¾“å…¥çš„ YAML æ–‡ä»¶ï¼Œæˆ–è€…è¿”å›æ•°å­—ç±»åã€‚"""
    if data:
        try:
            return yaml_load(check_yaml(data))["names"]
        except Exception:
            pass
    return {i: f"class{i}" for i in range(999)}  # å¦‚æœä¸Šé¢çš„æ“ä½œå¤±è´¥ï¼Œè¿”å›é»˜è®¤çš„ç±»å


class AutoBackend(nn.Module):
    """
    å¤„ç†åŠ¨æ€åç«¯é€‰æ‹©ï¼Œç”¨äºè¿è¡Œ Ultralytics YOLO æ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

    AutoBackend ç±»æ—¨åœ¨ä¸ºå„ç§æ¨ç†å¼•æ“æä¾›ä¸€ä¸ªæŠ½è±¡å±‚ã€‚å®ƒæ”¯æŒå¤šç§æ ¼å¼ï¼Œæ¯ç§æ ¼å¼éƒ½æœ‰ç‰¹å®šçš„å‘½åçº¦å®šï¼Œè¯¦ç»†ä¿¡æ¯å¦‚ä¸‹ï¼š

        æ”¯æŒçš„æ ¼å¼å’Œå‘½åçº¦å®šï¼š
            | æ ¼å¼                   | æ–‡ä»¶åç¼€           |
            |------------------------|--------------------|
            | PyTorch                | *.pt               |
            | TorchScript            | *.torchscript      |
            | ONNX Runtime           | *.onnx             |
            | ONNX OpenCV DNN        | *.onnx (dnn=True)  |
            | OpenVINO               | *openvino_model/   |
            | CoreML                 | *.mlpackage        |
            | TensorRT               | *.engine           |
            | TensorFlow SavedModel  | *_saved_model/     |
            | TensorFlow GraphDef    | *.pb               |
            | TensorFlow Lite        | *.tflite           |
            | TensorFlow Edge TPU    | *_edgetpu.tflite   |
            | PaddlePaddle           | *_paddle_model/    |
            | MNN                    | *.mnn              |
            | NCNN                   | *_ncnn_model/      |

    è¯¥ç±»æ ¹æ®è¾“å…¥çš„æ¨¡å‹æ ¼å¼æä¾›åŠ¨æ€åç«¯åˆ‡æ¢åŠŸèƒ½ï¼Œç®€åŒ–äº†åœ¨å„ç§å¹³å°ä¸Šéƒ¨ç½²æ¨¡å‹çš„å·¥ä½œã€‚
    """

    @torch.no_grad()
    def __init__(
        self,
        weights="yolo11n.pt",
        device=torch.device("cpu"),
        dnn=False,
        data=None,
        fp16=False,
        batch=1,
        fuse=True,
        verbose=True,
    ):
        """
        åˆå§‹åŒ– AutoBackend è¿›è¡Œæ¨ç†ã€‚

        å‚æ•°ï¼š
            weights (str | torch.nn.Module): æ¨¡å‹æƒé‡æ–‡ä»¶çš„è·¯å¾„æˆ–æ¨¡å—å®ä¾‹ã€‚é»˜è®¤ä¸º 'yolo11n.pt'ã€‚
            device (torch.device): æ¨¡å‹è¿è¡Œæ‰€åœ¨çš„è®¾å¤‡ã€‚é»˜è®¤ä¸º CPUã€‚
            dnn (bool): ä½¿ç”¨ OpenCV DNN æ¨¡å—è¿›è¡Œ ONNX æ¨ç†ã€‚é»˜è®¤ä¸º Falseã€‚
            data (str | Path | optional): å¯é€‰çš„é¢å¤–æ•°æ®.yaml æ–‡ä»¶è·¯å¾„ï¼Œå…¶ä¸­åŒ…å«ç±»åã€‚
            fp16 (bool): å¯ç”¨åŠç²¾åº¦æ¨ç†ï¼Œä»…æ”¯æŒç‰¹å®šåç«¯ã€‚é»˜è®¤ä¸º Falseã€‚
            batch (int): å‡è®¾çš„æ¨ç†æ‰¹æ¬¡å¤§å°ã€‚
            fuse (bool): æ˜¯å¦èåˆ Conv2D + BatchNorm å±‚ä»¥ä¼˜åŒ–æ€§èƒ½ã€‚é»˜è®¤ä¸º Trueã€‚
            verbose (bool): å¯ç”¨è¯¦ç»†æ—¥å¿—è®°å½•ã€‚é»˜è®¤ä¸º Trueã€‚
        """
        super().__init__()
        w = str(weights[0] if isinstance(weights, list) else weights)
        nn_module = isinstance(weights, torch.nn.Module)
        (
            pt,
            jit,
            onnx,
            xml,
            engine,
            coreml,
            saved_model,
            pb,
            tflite,
            edgetpu,
            tfjs,
            paddle,
            mnn,
            ncnn,
            imx,
            triton,
        ) = self._model_type(w)
        fp16 &= pt or jit or onnx or xml or engine or nn_module or triton  # FP16
        nhwc = coreml or saved_model or pb or tflite or edgetpu  # BHWC æ ¼å¼ï¼ˆä¸ torch çš„ BCWH æ ¼å¼ä¸åŒï¼‰
        stride = 32  # é»˜è®¤æ­¥å¹…
        model, metadata, task = None, None, None

        # è®¾ç½®è®¾å¤‡
        cuda = torch.cuda.is_available() and device.type != "cpu"  # ä½¿ç”¨ CUDA
        if cuda and not any([nn_module, pt, jit, engine, onnx, paddle]):  # å¦‚æœæ˜¯ GPU æ•°æ®åŠ è½½æ ¼å¼
            device = torch.device("cpu")
            cuda = False

        # å¦‚æœæœ¬åœ°æ²¡æœ‰ï¼Œä¸‹è½½æ¨¡å‹
        if not (pt or triton or nn_module):
            w = attempt_download_asset(w)

        # å†…å­˜ä¸­çš„PyTorchæ¨¡å‹
        if nn_module:
            model = weights.to(device)
            if fuse:
                model = model.fuse(verbose=verbose)
            if hasattr(model, "kpt_shape"):
                kpt_shape = model.kpt_shape  # ä»…å§¿æ€æ£€æµ‹
            stride = max(int(model.stride.max()), 32)  # æ¨¡å‹æ­¥å¹…
            names = model.module.names if hasattr(model, "module") else model.names  # è·å–ç±»åˆ«åç§°
            model.half() if fp16 else model.float()
            self.model = model  # æ˜¾å¼èµ‹å€¼ï¼Œä»¥ä¾¿ä½¿ç”¨to()ã€cpu()ã€cuda()ã€half()ç­‰
            pt = True

        # PyTorchæ¨¡å‹
        elif pt:
            from ultralytics.nn.tasks import attempt_load_weights

            model = attempt_load_weights(
                weights if isinstance(weights, list) else w, device=device, inplace=True, fuse=fuse
            )
            if hasattr(model, "kpt_shape"):
                kpt_shape = model.kpt_shape  # ä»…å§¿æ€æ£€æµ‹
            stride = max(int(model.stride.max()), 32)  # æ¨¡å‹æ­¥å¹…
            names = model.module.names if hasattr(model, "module") else model.names  # è·å–ç±»åˆ«åç§°
            model.half() if fp16 else model.float()
            self.model = model  # æ˜¾å¼èµ‹å€¼ï¼Œä»¥ä¾¿ä½¿ç”¨to()ã€cpu()ã€cuda()ã€half()ç­‰

        # TorchScriptæ¨¡å‹
        elif jit:
            LOGGER.info(f"åŠ è½½ {w} è¿›è¡ŒTorchScriptæ¨ç†...")
            extra_files = {"config.txt": ""}  # æ¨¡å‹å…ƒæ•°æ®
            model = torch.jit.load(w, _extra_files=extra_files, map_location=device)
            model.half() if fp16 else model.float()
            if extra_files["config.txt"]:  # åŠ è½½å…ƒæ•°æ®å­—å…¸
                metadata = json.loads(extra_files["config.txt"], object_hook=lambda x: dict(x.items()))

        # ONNX OpenCV DNN
        elif dnn:
            LOGGER.info(f"åŠ è½½ {w} è¿›è¡ŒONNX OpenCV DNNæ¨ç†...")
            check_requirements("opencv-python>=4.5.4")
            net = cv2.dnn.readNetFromONNX(w)

        # ONNX Runtime å’Œ IMX
        elif onnx or imx:
            LOGGER.info(f"åŠ è½½ {w} è¿›è¡ŒONNX Runtimeæ¨ç†...")
            check_requirements(("onnx", "onnxruntime-gpu" if cuda else "onnxruntime"))
            if IS_RASPBERRYPI or IS_JETSON:
                # è§£å†³'numpy.linalg._umath_linalg'æ²¡æœ‰'_ilp64'å±æ€§çš„é—®é¢˜ï¼Œè¿™å¯¹RPiå’ŒJetsonçš„TF SavedModelæœ‰å½±å“
                check_requirements("numpy==1.23.5")
            import onnxruntime

            providers = ["CPUExecutionProvider"]
            if cuda and "CUDAExecutionProvider" in onnxruntime.get_available_providers():
                providers.insert(0, "CUDAExecutionProvider")
            elif cuda:  # å¦‚æœè¯·æ±‚äº†CUDAä½†ä¸å¯ç”¨ï¼Œåˆ™åªè®°å½•è­¦å‘Š
                LOGGER.warning("è­¦å‘Š âš ï¸ æ— æ³•ä½¿ç”¨CUDAå¯åŠ¨ONNX Runtimeï¼Œæ”¹ä¸ºä½¿ç”¨CPU...")
                device = torch.device("cpu")
                cuda = False
            LOGGER.info(f"ä½¿ç”¨ONNX Runtime {providers[0]}")
            if onnx:
                session = onnxruntime.InferenceSession(w, providers=providers)
            else:
                check_requirements(
                    ["model-compression-toolkit==2.1.1", "sony-custom-layers[torch]==0.2.0", "onnxruntime-extensions"]
                )
                w = next(Path(w).glob("*.onnx"))
                LOGGER.info(f"åŠ è½½ {w} è¿›è¡ŒONNX IMXæ¨ç†...")
                import mct_quantizers as mctq
                from sony_custom_layers.pytorch.object_detection import nms_ort  # type: ignore # noqa

                session = onnxruntime.InferenceSession(
                    w, mctq.get_ort_session_options(), providers=["CPUExecutionProvider"]
                )
                task = "detect"

            output_names = [x.name for x in session.get_outputs()]
            metadata = session.get_modelmeta().custom_metadata_map
            dynamic = isinstance(session.get_outputs()[0].shape[0], str)
            if not dynamic:
                io = session.io_binding()
                bindings = []
                for output in session.get_outputs():
                    y_tensor = torch.empty(output.shape, dtype=torch.float16 if fp16 else torch.float32).to(device)
                    io.bind_output(
                        name=output.name,
                        device_type=device.type,
                        device_id=device.index if cuda else 0,
                        element_type=np.float16 if fp16 else np.float32,
                        shape=tuple(y_tensor.shape),
                        buffer_ptr=y_tensor.data_ptr(),
                    )
                    bindings.append(y_tensor)

        # OpenVINO
        elif xml:
            LOGGER.info(f"åŠ è½½ {w} è¿›è¡ŒOpenVINOæ¨ç†...")
            check_requirements("openvino>=2024.0.0")
            import openvino as ov

            core = ov.Core()
            w = Path(w)
            if not w.is_file():  # å¦‚æœä¸æ˜¯*.xmlæ–‡ä»¶
                w = next(w.glob("*.xml"))  # ä»*_openvino_modelç›®å½•ä¸­è·å–*.xmlæ–‡ä»¶
            ov_model = core.read_model(model=str(w), weights=w.with_suffix(".bin"))
            if ov_model.get_parameters()[0].get_layout().empty:
                ov_model.get_parameters()[0].set_layout(ov.Layout("NCHW"))

            # OpenVINOæ¨ç†æ¨¡å¼ä¸º'LATENCY'ï¼Œ'THROUGHPUT'ï¼ˆä¸æ¨èï¼‰æˆ–'CUMULATIVE_THROUGHPUT'
            inference_mode = "CUMULATIVE_THROUGHPUT" if batch > 1 else "LATENCY"
            LOGGER.info(f"ä½¿ç”¨OpenVINO {inference_mode} æ¨¡å¼è¿›è¡Œæ‰¹æ¬¡={batch}æ¨ç†...")
            ov_compiled_model = core.compile_model(
                ov_model,
                device_name="AUTO",  # AUTOé€‰æ‹©æœ€ä½³å¯ç”¨è®¾å¤‡ï¼Œä¸è¦ä¿®æ”¹
                config={"PERFORMANCE_HINT": inference_mode},
            )
            input_name = ov_compiled_model.input().get_any_name()
            metadata = w.parent / "metadata.yaml"

        # TensorRT
        elif engine:
            LOGGER.info(f"åŠ è½½ {w} è¿›è¡Œ TensorRT æ¨ç†...")
            try:
                import tensorrt as trt  # noqa https://developer.nvidia.com/nvidia-tensorrt-download
            except ImportError:
                if LINUX:
                    check_requirements("tensorrt>7.0.0,!=10.1.0")
                import tensorrt as trt  # noqa
            check_version(trt.__version__, ">=7.0.0", hard=True)
            check_version(trt.__version__, "!=10.1.0", msg="https://github.com/ultralytics/ultralytics/pull/14239")
            if device.type == "cpu":
                device = torch.device("cuda:0")
            Binding = namedtuple("Binding", ("name", "dtype", "shape", "data", "ptr"))
            logger = trt.Logger(trt.Logger.INFO)
            # è¯»å–æ–‡ä»¶
            with open(w, "rb") as f, trt.Runtime(logger) as runtime:
                try:
                    meta_len = int.from_bytes(f.read(4), byteorder="little")  # è¯»å–å…ƒæ•°æ®é•¿åº¦
                    metadata = json.loads(f.read(meta_len).decode("utf-8"))  # è¯»å–å…ƒæ•°æ®
                except UnicodeDecodeError:
                    f.seek(0)  # å¼•æ“æ–‡ä»¶å¯èƒ½ç¼ºå°‘åµŒå…¥çš„Ultralyticså…ƒæ•°æ®
                model = runtime.deserialize_cuda_engine(f.read())  # è¯»å–å¼•æ“

            # æ¨¡å‹ä¸Šä¸‹æ–‡
            try:
                context = model.create_execution_context()
            except Exception as e:  # æ¨¡å‹ä¸ºç©º
                LOGGER.error(f"é”™è¯¯: TensorRT æ¨¡å‹ä¸ {trt.__version__} ç‰ˆæœ¬ä¸å…¼å®¹\n")
                raise e

            bindings = OrderedDict()
            output_names = []
            fp16 = False  # é»˜è®¤æƒ…å†µä¸‹ï¼Œä¸‹é¢ä¼šæ›´æ–°
            dynamic = False
            is_trt10 = not hasattr(model, "num_bindings")
            num = range(model.num_io_tensors) if is_trt10 else range(model.num_bindings)
            for i in num:
                if is_trt10:
                    name = model.get_tensor_name(i)
                    dtype = trt.nptype(model.get_tensor_dtype(name))
                    is_input = model.get_tensor_mode(name) == trt.TensorIOMode.INPUT
                    if is_input:
                        if -1 in tuple(model.get_tensor_shape(name)):
                            dynamic = True
                            context.set_input_shape(name, tuple(model.get_tensor_profile_shape(name, 0)[1]))
                        if dtype == np.float16:
                            fp16 = True
                    else:
                        output_names.append(name)
                    shape = tuple(context.get_tensor_shape(name))
                else:  # TensorRT < 10.0
                    name = model.get_binding_name(i)
                    dtype = trt.nptype(model.get_binding_dtype(i))
                    is_input = model.binding_is_input(i)
                    if model.binding_is_input(i):
                        if -1 in tuple(model.get_binding_shape(i)):  # åŠ¨æ€
                            dynamic = True
                            context.set_binding_shape(i, tuple(model.get_profile_shape(0, i)[1]))
                        if dtype == np.float16:
                            fp16 = True
                    else:
                        output_names.append(name)
                    shape = tuple(context.get_binding_shape(i))
                im = torch.from_numpy(np.empty(shape, dtype=dtype)).to(device)
                bindings[name] = Binding(name, dtype, shape, im, int(im.data_ptr()))
            binding_addrs = OrderedDict((n, d.ptr) for n, d in bindings.items())
            batch_size = bindings["images"].shape[0]  # å¦‚æœæ˜¯åŠ¨æ€çš„ï¼Œè¿™é‡Œæ˜¯æœ€å¤§æ‰¹æ¬¡å¤§å°

        # CoreML
        elif coreml:
            LOGGER.info(f"åŠ è½½ {w} è¿›è¡Œ CoreML æ¨ç†...")
            import coremltools as ct

            model = ct.models.MLModel(w)
            metadata = dict(model.user_defined_metadata)

        # TF SavedModel
        elif saved_model:
            LOGGER.info(f"åŠ è½½ {w} è¿›è¡Œ TensorFlow SavedModel æ¨ç†...")
            import tensorflow as tf

            keras = False  # å‡è®¾æ˜¯TF1çš„saved_model
            model = tf.keras.models.load_model(w) if keras else tf.saved_model.load(w)
            metadata = Path(w) / "metadata.yaml"

        # TF GraphDef
        elif pb:  # https://www.tensorflow.org/guide/migrate#a_graphpb_or_graphpbtxt
            LOGGER.info(f"åŠ è½½ {w} è¿›è¡Œ TensorFlow GraphDef æ¨ç†...")
            import tensorflow as tf

            from ultralytics.engine.exporter import gd_outputs

            def wrap_frozen_graph(gd, inputs, outputs):
                """åŒ…è£…å†»ç»“å›¾ä»¥è¿›è¡Œéƒ¨ç½²ã€‚"""
                x = tf.compat.v1.wrap_function(lambda: tf.compat.v1.import_graph_def(gd, name=""), [])  # åŒ…è£…
                ge = x.graph.as_graph_element
                return x.prune(tf.nest.map_structure(ge, inputs), tf.nest.map_structure(ge, outputs))

            gd = tf.Graph().as_graph_def()  # TF GraphDef
            with open(w, "rb") as f:
                gd.ParseFromString(f.read())
            frozen_func = wrap_frozen_graph(gd, inputs="x:0", outputs=gd_outputs(gd))
            try:  # å°è¯•åœ¨ SavedModel ä¸­æŸ¥æ‰¾å…ƒæ•°æ®å’Œ GraphDef
                metadata = next(Path(w).resolve().parent.rglob(f"{Path(w).stem}_saved_model*/metadata.yaml"))
            except StopIteration:
                pass

        # TFLite æˆ– TFLite Edge TPU
        elif tflite or edgetpu:  # https://www.tensorflow.org/lite/guide/python#install_tensorflow_lite_for_python
            try:  # https://coral.ai/docs/edgetpu/tflite-python/#update-existing-tf-lite-code-for-the-edge-tpu
                from tflite_runtime.interpreter import Interpreter, load_delegate
            except ImportError:
                import tensorflow as tf

                Interpreter, load_delegate = tf.lite.Interpreter, tf.lite.experimental.load_delegate
            if edgetpu:  # TF Edge TPU https://coral.ai/software/#edgetpu-runtime
                device = device[3:] if str(device).startswith("tpu") else ":0"
                LOGGER.info(f"åŠ è½½ {w} åœ¨è®¾å¤‡ {device[1:]} ä¸Šè¿›è¡Œ TensorFlow Lite Edge TPU æ¨ç†...")
                delegate = {"Linux": "libedgetpu.so.1", "Darwin": "libedgetpu.1.dylib", "Windows": "edgetpu.dll"}[platform.system()]
                interpreter = Interpreter(
                    model_path=w,
                    experimental_delegates=[load_delegate(delegate, options={"device": device})],
                )
                device = "cpu"  # å¿…é¡»è¿™æ ·ï¼Œå¦åˆ™ PyTorch ä¼šå°è¯•ä½¿ç”¨é”™è¯¯çš„è®¾å¤‡
            else:  # TFLite
                LOGGER.info(f"åŠ è½½ {w} è¿›è¡Œ TensorFlow Lite æ¨ç†...")
                interpreter = Interpreter(model_path=w)  # åŠ è½½TFLiteæ¨¡å‹
            interpreter.allocate_tensors()  # åˆ†é…å¼ é‡
            input_details = interpreter.get_input_details()  # è¾“å…¥
            output_details = interpreter.get_output_details()  # è¾“å‡º
            # åŠ è½½å…ƒæ•°æ®
            try:
                with zipfile.ZipFile(w, "r") as model:
                    meta_file = model.namelist()[0]
                    metadata = ast.literal_eval(model.read(meta_file).decode("utf-8"))
            except zipfile.BadZipFile:
                pass

        # TF.js
        elif tfjs:
            raise NotImplementedError("YOLOv8 TF.js æ¨ç†å½“å‰ä¸æ”¯æŒã€‚")

        # PaddlePaddle
        elif paddle:
            LOGGER.info(f"åŠ è½½ {w} è¿›è¡Œ PaddlePaddle æ¨ç†...")
            check_requirements("paddlepaddle-gpu" if cuda else "paddlepaddle")
            import paddle.inference as pdi  # noqa

            w = Path(w)
            if not w.is_file():  # å¦‚æœä¸æ˜¯ *.pdmodel æ–‡ä»¶
                w = next(w.rglob("*.pdmodel"))  # ä» *_paddle_model ç›®å½•è·å– *.pdmodel æ–‡ä»¶
            config = pdi.Config(str(w), str(w.with_suffix(".pdiparams")))
            if cuda:
                config.enable_use_gpu(memory_pool_init_size_mb=2048, device_id=0)
            predictor = pdi.create_predictor(config)
            input_handle = predictor.get_input_handle(predictor.get_input_names()[0])
            output_names = predictor.get_output_names()
            metadata = w.parents[1] / "metadata.yaml"

        # MNN
        elif mnn:
            LOGGER.info(f"åŠ è½½ {w} è¿›è¡Œ MNN æ¨ç†...")
            check_requirements("MNN")  # éœ€è¦ MNN
            import os

            import MNN

            config = {"precision": "low", "backend": "CPU", "numThread": (os.cpu_count() + 1) // 2}
            rt = MNN.nn.create_runtime_manager((config,))
            net = MNN.nn.load_module_from_file(w, [], [], runtime_manager=rt, rearrange=True)

            def torch_to_mnn(x):
                return MNN.expr.const(x.data_ptr(), x.shape)

            metadata = json.loads(net.get_info()["bizCode"])

        # NCNN
        elif ncnn:
            LOGGER.info(f"åŠ è½½ {w} è¿›è¡Œ NCNN æ¨ç†...")
            check_requirements("git+https://github.com/Tencent/ncnn.git" if ARM64 else "ncnn")  # éœ€è¦ NCNN
            import ncnn as pyncnn

            net = pyncnn.Net()
            net.opt.use_vulkan_compute = cuda
            w = Path(w)
            if not w.is_file():  # å¦‚æœä¸æ˜¯ *.param æ–‡ä»¶
                w = next(w.glob("*.param"))  # ä» *_ncnn_model ç›®å½•è·å– *.param æ–‡ä»¶
            net.load_param(str(w))
            net.load_model(str(w.with_suffix(".bin")))
            metadata = w.parent / "metadata.yaml"

        # NVIDIA Triton Inference Server
        elif triton:
            check_requirements("tritonclient[all]")
            from ultralytics.utils.triton import TritonRemoteModel

            model = TritonRemoteModel(w)
            metadata = model.metadata

        # å…¶ä»–ä»»ä½•æ ¼å¼ï¼ˆä¸æ”¯æŒçš„æ ¼å¼ï¼‰
        else:
            from ultralytics.engine.exporter import export_formats

            raise TypeError(
                f"æ¨¡å‹='{w}' ä¸æ˜¯ä¸€ä¸ªæ”¯æŒçš„æ¨¡å‹æ ¼å¼ã€‚Ultralytics æ”¯æŒçš„æ ¼å¼ï¼š{export_formats()['Format']}\n"
                f"è¯·å‚é˜… https://docs.ultralytics.com/modes/predict è·å–å¸®åŠ©ã€‚"
            )

        # åŠ è½½å¤–éƒ¨å…ƒæ•°æ® YAML æ–‡ä»¶
        if isinstance(metadata, (str, Path)) and Path(metadata).exists():
            metadata = yaml_load(metadata)
        if metadata and isinstance(metadata, dict):
            for k, v in metadata.items():
                if k in {"stride", "batch"}:
                    metadata[k] = int(v)
                elif k in {"imgsz", "names", "kpt_shape"} and isinstance(v, str):
                    metadata[k] = eval(v)
            stride = metadata["stride"]
            task = metadata["task"]
            batch = metadata["batch"]
            imgsz = metadata["imgsz"]
            names = metadata["names"]
            kpt_shape = metadata.get("kpt_shape")
        elif not (pt or triton or nn_module):
            LOGGER.warning(f"WARNING âš ï¸ æœªæ‰¾åˆ° 'model={weights}' çš„å…ƒæ•°æ®")

        # æ£€æŸ¥ç±»åˆ«åç§°
        if "names" not in locals():  # å¦‚æœæ²¡æœ‰æ‰¾åˆ°ç±»åˆ«åç§°
            names = default_class_names(data)
        names = check_class_names(names)

        # ç¦ç”¨æ¢¯åº¦
        if pt:
            for p in model.parameters():
                p.requires_grad = False

        self.__dict__.update(locals())  # å°†æ‰€æœ‰å˜é‡èµ‹å€¼ç»™ self

    def forward(self, im, augment=False, visualize=False, embed=None):
        """
        å¯¹YOLOv8 MultiBackendæ¨¡å‹è¿›è¡Œæ¨ç†ã€‚

        å‚æ•°:
            im (torch.Tensor): ç”¨äºæ¨ç†çš„å›¾åƒå¼ é‡ã€‚
            augment (bool): æ˜¯å¦åœ¨æ¨ç†è¿‡ç¨‹ä¸­æ‰§è¡Œæ•°æ®å¢å¼ºï¼Œé»˜è®¤ä¸ºFalseã€‚
            visualize (bool): æ˜¯å¦å¯è§†åŒ–è¾“å‡ºé¢„æµ‹ç»“æœï¼Œé»˜è®¤ä¸ºFalseã€‚
            embed (list, optional): ä¸€ä¸ªåŒ…å«ç‰¹å¾å‘é‡/åµŒå…¥çš„åˆ—è¡¨ï¼Œç”¨äºè¿”å›ã€‚

        è¿”å›:
            (tuple): è¿”å›ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«åŸå§‹è¾“å‡ºå¼ é‡ï¼Œä»¥åŠç”¨äºå¯è§†åŒ–çš„å¤„ç†åè¾“å‡ºï¼ˆå¦‚æœvisualize=Trueï¼‰ã€‚
        """
        b, ch, h, w = im.shape  # batch, channel, height, width
        if self.fp16 and im.dtype != torch.float16:
            im = im.half()  # è½¬ä¸ºFP16
        if self.nhwc:
            im = im.permute(0, 2, 3, 1)  # torch BCHW è½¬ä¸º numpy BHWC å½¢çŠ¶(1,320,192,3)

        # PyTorch
        if self.pt or self.nn_module:
            y = self.model(im, augment=augment, visualize=visualize, embed=embed)

        # TorchScript
        elif self.jit:
            y = self.model(im)

        # ONNX OpenCV DNN
        elif self.dnn:
            im = im.cpu().numpy()  # ä»torchè½¬ä¸ºnumpy
            self.net.setInput(im)
            y = self.net.forward()

        # ONNX Runtime
        elif self.onnx or self.imx:
            if self.dynamic:
                im = im.cpu().numpy()  # ä»torchè½¬ä¸ºnumpy
                y = self.session.run(self.output_names, {self.session.get_inputs()[0].name: im})
            else:
                if not self.cuda:
                    im = im.cpu()
                self.io.bind_input(
                    name="images",
                    device_type=im.device.type,
                    device_id=im.device.index if im.device.type == "cuda" else 0,
                    element_type=np.float16 if self.fp16 else np.float32,
                    shape=tuple(im.shape),
                    buffer_ptr=im.data_ptr(),
                )
                self.session.run_with_iobinding(self.io)
                y = self.bindings
            if self.imx:
                # boxes, conf, cls
                y = np.concatenate([y[0], y[1][:, :, None], y[2][:, :, None]], axis=-1)

        # OpenVINO
        elif self.xml:
            im = im.cpu().numpy()  # FP32

            if self.inference_mode in {"THROUGHPUT", "CUMULATIVE_THROUGHPUT"}:  # å¯¹å¤§æ‰¹é‡è¿›è¡Œä¼˜åŒ–
                n = im.shape[0]  # æ‰¹æ¬¡ä¸­çš„å›¾åƒæ•°é‡
                results = [None] * n  # é¢„åˆ†é…åˆ—è¡¨ï¼Œä¸å›¾åƒæ•°é‡åŒ¹é…

                def callback(request, userdata):
                    """ä½¿ç”¨userdataç´¢å¼•å°†ç»“æœæ”¾å…¥é¢„åˆ†é…çš„åˆ—è¡¨ä¸­ã€‚"""
                    results[userdata] = request.results

                # åˆ›å»ºå¼‚æ­¥æ¨ç†é˜Ÿåˆ—ï¼Œè®¾ç½®å›è°ƒå‡½æ•°å¹¶å¼€å§‹æ¯ä¸ªè¾“å…¥å›¾åƒçš„å¼‚æ­¥æ¨ç†
                async_queue = self.ov.runtime.AsyncInferQueue(self.ov_compiled_model)
                async_queue.set_callback(callback)
                for i in range(n):
                    # å¯åŠ¨å¼‚æ­¥æ¨ç†ï¼Œå¹¶ä½¿ç”¨userdata=iæŒ‡å®šç»“æœåˆ—è¡¨ä¸­çš„ä½ç½®
                    async_queue.start_async(inputs={self.input_name: im[i : i + 1]}, userdata=i)  # ä¿æŒå›¾åƒä¸ºBCHWæ ¼å¼
                async_queue.wait_all()  # ç­‰å¾…æ‰€æœ‰æ¨ç†è¯·æ±‚å®Œæˆ
                y = np.concatenate([list(r.values())[0] for r in results])

            else:  # inference_mode = "LATENCY"ï¼Œå¯¹å•ä¸ªæ‰¹æ¬¡è¿›è¡Œæœ€å¿«çš„ç»“æœä¼˜åŒ–
                y = list(self.ov_compiled_model(im).values())

        # TensorRT
        elif self.engine:
            if self.dynamic and im.shape != self.bindings["images"].shape:
                if self.is_trt10:
                    self.context.set_input_shape("images", im.shape)
                    self.bindings["images"] = self.bindings["images"]._replace(shape=im.shape)
                    for name in self.output_names:
                        self.bindings[name].data.resize_(tuple(self.context.get_tensor_shape(name)))
                else:
                    i = self.model.get_binding_index("images")
                    self.context.set_binding_shape(i, im.shape)
                    self.bindings["images"] = self.bindings["images"]._replace(shape=im.shape)
                    for name in self.output_names:
                        i = self.model.get_binding_index(name)
                        self.bindings[name].data.resize_(tuple(self.context.get_binding_shape(i)))

            s = self.bindings["images"].shape
            assert im.shape == s, f"è¾“å…¥å°ºå¯¸ {im.shape} {'>' if self.dynamic else 'ä¸ç­‰äº'} æœ€å¤§æ¨¡å‹å°ºå¯¸ {s}"
            self.binding_addrs["images"] = int(im.data_ptr())
            self.context.execute_v2(list(self.binding_addrs.values()))
            y = [self.bindings[x].data for x in sorted(self.output_names)]

        # CoreML
        elif self.coreml:
            im = im[0].cpu().numpy()
            im_pil = Image.fromarray((im * 255).astype("uint8"))
            # im = im.resize((192, 320), Image.BILINEAR)
            y = self.model.predict({"image": im_pil})  # åæ ‡æ˜¯xywhè§„èŒƒåŒ–çš„
            if "confidence" in y:
                raise TypeError(
                    "Ultralytics åªæ”¯æŒæ¨ç†æœªç®¡é“åŒ–çš„CoreMLæ¨¡å‹ï¼Œè¿™äº›æ¨¡å‹æ˜¯é€šè¿‡'nms=False'å¯¼å‡ºçš„ï¼Œ"
                    f"ä½†'model={w}'åŒ…å«ç”±'nms=True'å¯¼å‡ºçš„NMSç®¡é“ã€‚"
                )
                # TODO: CoreML NMSæ¨ç†å¤„ç†
                # from ultralytics.utils.ops import xywh2xyxy
                # box = xywh2xyxy(y['coordinates'] * [[w, h, w, h]])  # xyxyåƒç´ 
                # conf, cls = y['confidence'].max(1), y['confidence'].argmax(1).astype(np.float32)
                # y = np.concatenate((box, conf.reshape(-1, 1), cls.reshape(-1, 1)), 1)
            y = list(y.values())
            if len(y) == 2 and len(y[1].shape) != 4:  # åˆ†å‰²æ¨¡å‹
                y = list(reversed(y))  # å¯¹äºåˆ†å‰²æ¨¡å‹ï¼ˆé¢„æµ‹ï¼ŒåŸå‹ï¼‰ï¼Œéœ€è¦åè½¬

        # PaddlePaddle
        elif self.paddle:
            im = im.cpu().numpy().astype(np.float32)
            self.input_handle.copy_from_cpu(im)
            self.predictor.run()
            y = [self.predictor.get_output_handle(x).copy_to_cpu() for x in self.output_names]

        # MNN
        elif self.mnn:
            input_var = self.torch_to_mnn(im)
            output_var = self.net.onForward([input_var])
            y = [x.read() for x in output_var]

        # NCNN
        elif self.ncnn:
            mat_in = self.pyncnn.Mat(im[0].cpu().numpy())
            with self.net.create_extractor() as ex:
                ex.input(self.net.input_names()[0], mat_in)
                # è­¦å‘Šï¼š'output_names' è¢«æ’åºï¼Œä½œä¸º https://github.com/pnnx/pnnx/issues/130 çš„ä¸´æ—¶ä¿®å¤
                y = [np.array(ex.extract(x)[1])[None] for x in sorted(self.net.output_names())]

        # NVIDIA Triton æ¨ç†æœåŠ¡å™¨
        elif self.triton:
            im = im.cpu().numpy()  # ä» torch è½¬æ¢åˆ° numpy
            y = self.model(im)

        # TensorFlow (SavedModel, GraphDef, Lite, Edge TPU)
        else:
            im = im.cpu().numpy()
            if self.saved_model:  # SavedModel
                y = self.model(im, training=False) if self.keras else self.model(im)
                if not isinstance(y, list):
                    y = [y]
            elif self.pb:  # GraphDef
                y = self.frozen_func(x=self.tf.constant(im))
            else:  # Lite æˆ– Edge TPU
                details = self.input_details[0]
                is_int = details["dtype"] in {np.int8, np.int16}  # åˆ¤æ–­æ˜¯å¦ä¸º TFLite é‡åŒ– int8 æˆ– int16 æ¨¡å‹
                if is_int:
                    scale, zero_point = details["quantization"]
                    im = (im / scale + zero_point).astype(details["dtype"])  # åç¼©æ”¾
                self.interpreter.set_tensor(details["index"], im)
                self.interpreter.invoke()
                y = []
                for output in self.output_details:
                    x = self.interpreter.get_tensor(output["index"])
                    if is_int:
                        scale, zero_point = output["quantization"]
                        x = (x.astype(np.float32) - zero_point) * scale  # é‡æ–°ç¼©æ”¾
                    if x.ndim == 3:  # å¦‚æœä»»åŠ¡ä¸æ˜¯åˆ†ç±»ï¼Œæ’é™¤æ©ç ï¼ˆndim=4ï¼‰ç­‰
                        # æŒ‰ç…§å›¾åƒå¤§å°åå½’ä¸€åŒ– xywhã€‚å‚è€ƒ https://github.com/ultralytics/ultralytics/pull/1695
                        # xywh åœ¨ TFLite/EdgeTPU ä¸­è¢«å½’ä¸€åŒ–ï¼Œä»¥å‡è½»æ•´æ•°æ¨¡å‹çš„é‡åŒ–è¯¯å·®
                        if x.shape[-1] == 6:  # ç«¯åˆ°ç«¯æ¨¡å‹
                            x[:, :, [0, 2]] *= w
                            x[:, :, [1, 3]] *= h
                        else:
                            x[:, [0, 2]] *= w
                            x[:, [1, 3]] *= h
                            if self.task == "pose":
                                x[:, 5::3] *= w
                                x[:, 6::3] *= h
                    y.append(x)
            # TF æ®µä¿®å¤ï¼šå¯¼å‡ºé¡ºåºä¸ ONNX å¯¼å‡ºç›¸åï¼Œprotos å·²ç»è½¬ç½®
            if len(y) == 2:  # å¤„ç† (det, proto) è¾“å‡ºé¡ºåºåè½¬çš„æƒ…å†µ
                if len(y[1].shape) != 4:
                    y = list(reversed(y))  # åº”è¯¥æ˜¯ y = (1, 116, 8400), (1, 160, 160, 32)
                if y[1].shape[-1] == 6:  # ç«¯åˆ°ç«¯æ¨¡å‹
                    y = [y[1]]
                else:
                    y[1] = np.transpose(y[1], (0, 3, 1, 2))  # åº”è¯¥æ˜¯ y = (1, 116, 8400), (1, 32, 160, 160)
            y = [x if isinstance(x, np.ndarray) else x.numpy() for x in y]

        # for x in y:
        #     print(type(x), len(x)) å¦‚æœæ˜¯ (list, tuple) åˆ™æ‰“å°ç±»å‹å’Œé•¿åº¦ï¼Œå¦åˆ™æ‰“å°ç±»å‹å’Œå½¢çŠ¶  # è°ƒè¯•å½¢çŠ¶
        if isinstance(y, (list, tuple)):
            if len(self.names) == 999 and (self.task == "segment" or len(y) == 2):  # åˆ†å‰²å’Œåç§°æœªå®šä¹‰
                nc = y[0].shape[1] - y[1].shape[1] - 4  # y = (1, 32, 160, 160), (1, 116, 8400)
                self.names = {i: f"class{i}" for i in range(nc)}
            return self.from_numpy(y[0]) if len(y) == 1 else [self.from_numpy(x) for x in y]
        else:
            return self.from_numpy(y)

    def from_numpy(self, x):
        """
        å°† numpy æ•°ç»„è½¬æ¢ä¸ºå¼ é‡ã€‚

        å‚æ•°:
            x (np.ndarray): éœ€è¦è½¬æ¢çš„æ•°ç»„ã€‚

        è¿”å›:
            (torch.Tensor): è½¬æ¢åçš„å¼ é‡
        """
        return torch.tensor(x).to(self.device) if isinstance(x, np.ndarray) else x

    def warmup(self, imgsz=(1, 3, 640, 640)):
        """
        é€šè¿‡è¿è¡Œä¸€æ¬¡å‰å‘ä¼ æ’­æ¥é¢„çƒ­æ¨¡å‹ï¼Œä½¿ç”¨ä¸€ä¸ªè™šæ‹Ÿè¾“å…¥ã€‚

        å‚æ•°:
            imgsz (tuple): è™šæ‹Ÿè¾“å…¥å¼ é‡çš„å½¢çŠ¶ï¼Œæ ¼å¼ä¸º(batch_size, channels, height, width)
        """
        import torchvision  # noqa (åœ¨æ­¤å¯¼å…¥ï¼Œé¿å…è®°å½• torchvision å¯¼å…¥æ—¶é—´)

        warmup_types = self.pt, self.jit, self.onnx, self.engine, self.saved_model, self.pb, self.triton, self.nn_module
        if any(warmup_types) and (self.device.type != "cpu" or self.triton):
            im = torch.empty(*imgsz, dtype=torch.half if self.fp16 else torch.float, device=self.device)  # è¾“å…¥
            for _ in range(2 if self.jit else 1):
                self.forward(im)  # é¢„çƒ­

    @staticmethod
    def _model_type(p="path/to/model.pt"):
        """
        ç»™å®šæ¨¡å‹æ–‡ä»¶çš„è·¯å¾„å¹¶è¿”å›æ¨¡å‹ç±»å‹ã€‚å¯èƒ½çš„ç±»å‹åŒ…æ‹¬ ptã€jitã€onnxã€xmlã€engineã€coremlã€
        saved_modelã€pbã€tfliteã€edgetpuã€tfjsã€ncnn æˆ– paddleã€‚

        å‚æ•°:
            p: æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„ï¼Œé»˜è®¤ä¸º "path/to/model.pt"

        ç¤ºä¾‹:
            >>> model = AutoBackend(weights="path/to/model.onnx")
            >>> model_type = model._model_type()  # è¿”å› "onnx"
        """
        from ultralytics.engine.exporter import export_formats

        sf = export_formats()["Suffix"]  # å¯¼å‡ºåç¼€
        if not is_url(p) and not isinstance(p, str):
            check_suffix(p, sf)  # æ£€æŸ¥åç¼€
        name = Path(p).name
        types = [s in name for s in sf]
        types[5] |= name.endswith(".mlmodel")  # ä¿ç•™å¯¹æ—§ç‰ˆ Apple CoreML *.mlmodel æ ¼å¼çš„æ”¯æŒ
        types[8] &= not types[9]  # tflite &= not edgetpu
        if any(types):
            triton = False
        else:
            from urllib.parse import urlsplit

            url = urlsplit(p)
            triton = bool(url.netloc) and bool(url.path) and url.scheme in {"http", "grpc"}

        return types + [triton]


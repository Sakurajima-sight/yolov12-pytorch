# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""
åŸºå‡†æµ‹è¯•YOLOæ¨¡å‹åœ¨ä¸åŒæ ¼å¼ä¸‹çš„é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚

ç”¨æ³•ï¼š
    from ultralytics.utils.benchmarks import ProfileModels, benchmark
    ProfileModels(['yolov8n.yaml', 'yolov8s.yaml']).profile()
    benchmark(model='yolov8n.pt', imgsz=160)

æ ¼å¼                     | `format=argument`         | æ¨¡å‹
---                      | ---                       | ---
PyTorch                  | -                         | yolov8n.pt
TorchScript              | `torchscript`             | yolov8n.torchscript
ONNX                     | `onnx`                    | yolov8n.onnx
OpenVINO                 | `openvino`                | yolov8n_openvino_model/
TensorRT                 | `engine`                  | yolov8n.engine
CoreML                   | `coreml`                  | yolov8n.mlpackage
TensorFlow SavedModel    | `saved_model`             | yolov8n_saved_model/
TensorFlow GraphDef      | `pb`                      | yolov8n.pb
TensorFlow Lite          | `tflite`                  | yolov8n.tflite
TensorFlow Edge TPU      | `edgetpu`                 | yolov8n_edgetpu.tflite
TensorFlow.js            | `tfjs`                    | yolov8n_web_model/
PaddlePaddle             | `paddle`                  | yolov8n_paddle_model/
MNN                      | `mnn`                     | yolov8n.mnn
NCNN                     | `ncnn`                    | yolov8n_ncnn_model/
"""

import glob
import os
import platform
import re
import shutil
import time
from pathlib import Path

import numpy as np
import torch.cuda
import yaml

from ultralytics import YOLO, YOLOWorld
from ultralytics.cfg import TASK2DATA, TASK2METRIC
from ultralytics.engine.exporter import export_formats
from ultralytics.utils import ARM64, ASSETS, IS_JETSON, IS_RASPBERRYPI, LINUX, LOGGER, MACOS, TQDM, WEIGHTS_DIR
from ultralytics.utils.checks import IS_PYTHON_3_12, check_requirements, check_yolo
from ultralytics.utils.downloads import safe_download
from ultralytics.utils.files import file_size
from ultralytics.utils.torch_utils import get_cpu_info, select_device


def benchmark(
    model=WEIGHTS_DIR / "yolo11n.pt",
    data=None,
    imgsz=160,
    half=False,
    int8=False,
    device="cpu",
    verbose=False,
    eps=1e-3,
):
    """
    åŸºå‡†æµ‹è¯•YOLOæ¨¡å‹åœ¨ä¸åŒæ ¼å¼ä¸‹çš„é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚

    å‚æ•°:
        model (str | Path): æ¨¡å‹æ–‡ä»¶æˆ–ç›®å½•çš„è·¯å¾„ã€‚
        data (str | None): è¦è¯„ä¼°çš„æ•°æ®é›†ï¼Œå¦‚æœæœªä¼ é€’åˆ™ç»§æ‰¿è‡ªTASK2DATAã€‚
        imgsz (int): åŸºå‡†æµ‹è¯•çš„å›¾åƒå¤§å°ã€‚
        half (bool): å¦‚æœä¸ºTrueï¼Œåˆ™ä½¿ç”¨åŠç²¾åº¦æ¨¡å‹ã€‚
        int8 (bool): å¦‚æœä¸ºTrueï¼Œåˆ™ä½¿ç”¨int8ç²¾åº¦æ¨¡å‹ã€‚
        device (str): ç”¨äºè¿è¡ŒåŸºå‡†æµ‹è¯•çš„è®¾å¤‡ï¼Œå¯ä»¥æ˜¯'cpu'æˆ–'cuda'ã€‚
        verbose (bool | float): å¦‚æœä¸ºTrueæˆ–floatï¼Œåˆ™åœ¨ç»™å®šåº¦é‡å€¼æ—¶æ–­è¨€åŸºå‡†æµ‹è¯•é€šè¿‡ã€‚
        eps (float): é˜²æ­¢é™¤ä»¥é›¶çš„epsilonå€¼ã€‚

    è¿”å›:
        (pandas.DataFrame): ä¸€ä¸ªåŒ…å«æ¯ä¸ªæ ¼å¼çš„åŸºå‡†æµ‹è¯•ç»“æœçš„pandas DataFrameï¼Œç»“æœåŒ…æ‹¬æ–‡ä»¶å¤§å°ã€åº¦é‡å€¼å’Œæ¨ç†æ—¶é—´ã€‚

    ç¤ºä¾‹:
        ä½¿ç”¨é»˜è®¤è®¾ç½®åŸºå‡†æµ‹è¯•YOLOæ¨¡å‹ï¼š
        >>> from ultralytics.utils.benchmarks import benchmark
        >>> benchmark(model="yolo11n.pt", imgsz=640)
    """
    import pandas as pd  # åœ¨è¿™é‡Œå¯¼å…¥ä»¥åŠ å¿« 'import ultralytics'

    pd.options.display.max_columns = 10
    pd.options.display.width = 120
    device = select_device(device, verbose=False)
    if isinstance(model, (str, Path)):
        model = YOLO(model)
    is_end2end = getattr(model.model.model[-1], "end2end", False)

    y = []
    t0 = time.time()
    for i, (name, format, suffix, cpu, gpu, _) in enumerate(zip(*export_formats().values())):
        emoji, filename = "âŒ", None  # é»˜è®¤å¯¼å‡º
        try:
            # æ£€æŸ¥
            if i == 7:  # TF GraphDef
                assert model.task != "obb", "TensorFlow GraphDefä¸æ”¯æŒOBBä»»åŠ¡"
            elif i == 9:  # Edge TPU
                assert LINUX and not ARM64, "ä»…æ”¯æŒåœ¨éaarch64 Linuxä¸Šå¯¼å‡ºEdge TPU"
            elif i in {5, 10}:  # CoreML å’Œ TF.js
                assert MACOS or LINUX, "CoreMLå’ŒTF.jså¯¼å‡ºä»…æ”¯æŒåœ¨macOSå’ŒLinuxä¸Š"
                assert not IS_RASPBERRYPI, "Raspberry Piä¸æ”¯æŒCoreMLå’ŒTF.jså¯¼å‡º"
                assert not IS_JETSON, "NVIDIA Jetsonä¸æ”¯æŒCoreMLå’ŒTF.jså¯¼å‡º"
            if i in {5}:  # CoreML
                assert not IS_PYTHON_3_12, "CoreMLä¸æ”¯æŒPython 3.12"
            if i in {6, 7, 8}:  # TF SavedModel, TF GraphDef, å’Œ TFLite
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 TensorFlowå¯¼å‡ºè¿˜ä¸æ”¯æŒonnx2tf"
            if i in {9, 10}:  # TF EdgeTPU å’Œ TF.js
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 TensorFlowå¯¼å‡ºè¿˜ä¸æ”¯æŒonnx2tf"
            if i == 11:  # Paddle
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 Paddleå¯¼å‡ºè¿˜ä¸æ”¯æŒ"
                assert not is_end2end, "End-to-endæ¨¡å‹æš‚ä¸æ”¯æŒPaddlePaddle"
                assert LINUX or MACOS, "Windowsä¸æ”¯æŒPaddleå¯¼å‡º"
            if i == 12:  # MNN
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 MNNå¯¼å‡ºè¿˜ä¸æ”¯æŒ"
            if i == 13:  # NCNN
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 NCNNå¯¼å‡ºè¿˜ä¸æ”¯æŒ"
            if i == 14:  # IMX
                assert not is_end2end
                assert not isinstance(model, YOLOWorld), "YOLOWorldv2 IMXå¯¼å‡ºä¸æ”¯æŒ"
                assert model.task == "detect", "IMXä»…æ”¯æŒæ£€æµ‹ä»»åŠ¡"
                assert "C2f" in model.__str__(), "IMXä»…æ”¯æŒYOLOv8"
            if "cpu" in device.type:
                assert cpu, "CPUä¸Šä¸æ”¯æŒæ¨ç†"
            if "cuda" in device.type:
                assert gpu, "GPUä¸Šä¸æ”¯æŒæ¨ç†"

            # å¯¼å‡º
            if format == "-":
                filename = model.ckpt_path or model.cfg
                exported_model = model  # PyTorchæ ¼å¼
            else:
                filename = model.export(imgsz=imgsz, format=format, half=half, int8=int8, device=device, verbose=False)
                exported_model = YOLO(filename, task=model.task)
                assert suffix in str(filename), "å¯¼å‡ºå¤±è´¥"
            emoji = "â"  # è¡¨ç¤ºå¯¼å‡ºæˆåŠŸ

            # é¢„æµ‹
            assert model.task != "pose" or i != 7, "GraphDef Poseæ¨ç†ä¸æ”¯æŒ"
            assert i not in {9, 10}, "ä¸æ”¯æŒæ¨ç†"  # Edge TPUå’ŒTF.jsä¸æ”¯æŒ
            assert i != 5 or platform.system() == "Darwin", "æ¨ç†ä»…æ”¯æŒåœ¨macOS>=10.13ä¸Š"  # CoreML
            if i in {13}:
                assert not is_end2end, "End-to-end torch.topkæ“ä½œæš‚ä¸æ”¯æŒNCNNé¢„æµ‹"
            exported_model.predict(ASSETS / "bus.jpg", imgsz=imgsz, device=device, half=half)

            # éªŒè¯
            data = data or TASK2DATA[model.task]  # ä»»åŠ¡åˆ°æ•°æ®é›†ï¼Œä¾‹å¦‚ä»»åŠ¡=detectæ—¶ä¸ºcoco8.yaml
            key = TASK2METRIC[model.task]  # ä»»åŠ¡åˆ°åº¦é‡ï¼Œä¾‹å¦‚ä»»åŠ¡=detectæ—¶ä¸ºmetrics/mAP50-95(B)
            results = exported_model.val(
                data=data, batch=1, imgsz=imgsz, plots=False, device=device, half=half, int8=int8, verbose=False
            )
            metric, speed = results.results_dict[key], results.speed["inference"]
            fps = round(1000 / (speed + eps), 2)  # æ¯ç§’å¸§æ•°
            y.append([name, "âœ…", round(file_size(filename), 1), round(metric, 4), round(speed, 2), fps])
        except Exception as e:
            if verbose:
                assert type(e) is AssertionError, f"åŸºå‡†æµ‹è¯•å¤±è´¥ {name}: {e}"
            LOGGER.warning(f"é”™è¯¯ âŒï¸ åŸºå‡†æµ‹è¯•å¤±è´¥ {name}: {e}")
            y.append([name, emoji, round(file_size(filename), 1), None, None, None])  # mAP, t_inference

    # æ‰“å°ç»“æœ
    check_yolo(device=device)  # æ‰“å°ç³»ç»Ÿä¿¡æ¯
    df = pd.DataFrame(y, columns=["æ ¼å¼", "çŠ¶æ€â”", "å¤§å° (MB)", key, "æ¨ç†æ—¶é—´ (ms/å¼ )", "FPS"])

    name = Path(model.ckpt_path).name
    s = f"\nåŸºå‡†æµ‹è¯•å®Œæˆï¼š{name}ï¼Œæ•°æ®é›†ï¼š{data}ï¼Œimgsz={imgsz} ({time.time() - t0:.2f}s)\n{df}\n"
    LOGGER.info(s)
    with open("benchmarks.log", "a", errors="ignore", encoding="utf-8") as f:
        f.write(s)

    if verbose and isinstance(verbose, float):
        metrics = df[key].array  # ç”¨äºä¸åœ°æ¿å€¼æ¯”è¾ƒçš„å€¼
        floor = verbose  # æœ€ä½æ€§èƒ½æ ‡å‡†å€¼ï¼Œä¾‹å¦‚ï¼š= 0.29 mAP å¯¹äº YOLOv5n
        assert all(x > floor for x in metrics if pd.notna(x)), f"åŸºå‡†æµ‹è¯•å¤±è´¥ï¼šæŒ‡æ ‡ < åœ°æ¿å€¼ {floor}"

    return df


class RF100Benchmark:
    """åŸºå‡†æµ‹è¯• YOLO æ¨¡å‹åœ¨ä¸åŒæ ¼å¼ä¸‹çš„é€Ÿåº¦å’Œç²¾åº¦è¡¨ç°ã€‚"""

    def __init__(self):
        """åˆå§‹åŒ– RF100Benchmark ç±»ï¼Œç”¨äºåœ¨ä¸åŒæ ¼å¼ä¸‹åŸºå‡†æµ‹è¯• YOLO æ¨¡å‹çš„æ€§èƒ½ã€‚"""
        self.ds_names = []
        self.ds_cfg_list = []
        self.rf = None
        self.val_metrics = ["ç±»åˆ«", "å›¾åƒ", "ç›®æ ‡", "ç²¾åº¦", "å¬å›ç‡", "mAP50", "mAP95"]

    def set_key(self, api_key):
        """
        è®¾ç½® Roboflow API å¯†é’¥è¿›è¡Œæ•°æ®é›†å¤„ç†ã€‚

        å‚æ•°ï¼š
            api_key (str): API å¯†é’¥ã€‚

        ç¤ºä¾‹ï¼š
            è®¾ç½® Roboflow API å¯†é’¥ä»¥è®¿é—®æ•°æ®é›†ï¼š
            >>> benchmark = RF100Benchmark()
            >>> benchmark.set_key("your_roboflow_api_key")
        """
        check_requirements("roboflow")
        from roboflow import Roboflow

        self.rf = Roboflow(api_key=api_key)

    def parse_dataset(self, ds_link_txt="datasets_links.txt"):
        """
        è§£ææ•°æ®é›†é“¾æ¥å¹¶ä¸‹è½½æ•°æ®é›†ã€‚

        å‚æ•°ï¼š
            ds_link_txt (str): åŒ…å«æ•°æ®é›†é“¾æ¥çš„æ–‡ä»¶è·¯å¾„ã€‚

        ç¤ºä¾‹ï¼š
            >>> benchmark = RF100Benchmark()
            >>> benchmark.set_key("api_key")
            >>> benchmark.parse_dataset("datasets_links.txt")
        """
        (shutil.rmtree("rf-100"), os.mkdir("rf-100")) if os.path.exists("rf-100") else os.mkdir("rf-100")
        os.chdir("rf-100")
        os.mkdir("ultralytics-benchmarks")
        safe_download("https://github.com/ultralytics/assets/releases/download/v0.0.0/datasets_links.txt")

        with open(ds_link_txt) as file:
            for line in file:
                try:
                    _, url, workspace, project, version = re.split("/+", line.strip())
                    self.ds_names.append(project)
                    proj_version = f"{project}-{version}"
                    if not Path(proj_version).exists():
                        self.rf.workspace(workspace).project(project).version(version).download("yolov8")
                    else:
                        print("æ•°æ®é›†å·²ä¸‹è½½ã€‚")
                    self.ds_cfg_list.append(Path.cwd() / proj_version / "data.yaml")
                except Exception:
                    continue

        return self.ds_names, self.ds_cfg_list

    @staticmethod
    def fix_yaml(path):
        """
        ä¿®å¤ç»™å®š YAML æ–‡ä»¶ä¸­çš„è®­ç»ƒå’ŒéªŒè¯è·¯å¾„ã€‚

        å‚æ•°ï¼š
            path (str): è¦ä¿®å¤çš„ YAML æ–‡ä»¶è·¯å¾„ã€‚

        ç¤ºä¾‹ï¼š
            >>> RF100Benchmark.fix_yaml("path/to/data.yaml")
        """
        with open(path) as file:
            yaml_data = yaml.safe_load(file)
        yaml_data["train"] = "train/images"
        yaml_data["val"] = "valid/images"
        with open(path, "w") as file:
            yaml.safe_dump(yaml_data, file)

    def evaluate(self, yaml_path, val_log_file, eval_log_file, list_ind):
        """
        åœ¨éªŒè¯ç»“æœä¸Šè¯„ä¼°æ¨¡å‹æ€§èƒ½ã€‚

        å‚æ•°ï¼š
            yaml_path (str): YAML é…ç½®æ–‡ä»¶è·¯å¾„ã€‚
            val_log_file (str): éªŒè¯æ—¥å¿—æ–‡ä»¶è·¯å¾„ã€‚
            eval_log_file (str): è¯„ä¼°æ—¥å¿—æ–‡ä»¶è·¯å¾„ã€‚
            list_ind (int): å½“å‰æ•°æ®é›†åœ¨åˆ—è¡¨ä¸­çš„ç´¢å¼•ã€‚

        è¿”å›ï¼š
            (float): è¯„ä¼°æ¨¡å‹çš„å¹³å‡ç²¾åº¦ (mAP) å€¼ã€‚

        ç¤ºä¾‹ï¼š
            åœ¨ç‰¹å®šæ•°æ®é›†ä¸Šè¯„ä¼°æ¨¡å‹ï¼š
            >>> benchmark = RF100Benchmark()
            >>> benchmark.evaluate("path/to/data.yaml", "path/to/val_log.txt", "path/to/eval_log.txt", 0)
        """
        skip_symbols = ["ğŸš€", "âš ï¸", "ğŸ’¡", "âŒ"]
        with open(yaml_path) as stream:
            class_names = yaml.safe_load(stream)["names"]
        with open(val_log_file, encoding="utf-8") as f:
            lines = f.readlines()
            eval_lines = []
            for line in lines:
                if any(symbol in line for symbol in skip_symbols):
                    continue
                entries = line.split(" ")
                entries = list(filter(lambda val: val != "", entries))
                entries = [e.strip("\n") for e in entries]
                eval_lines.extend(
                    {
                        "ç±»åˆ«": entries[0],
                        "å›¾åƒ": entries[1],
                        "ç›®æ ‡": entries[2],
                        "ç²¾åº¦": entries[3],
                        "å¬å›ç‡": entries[4],
                        "mAP50": entries[5],
                        "mAP95": entries[6],
                    }
                    for e in entries
                    if e in class_names or (e == "all" and "(AP)" not in entries and "(AR)" not in entries)
                )
        map_val = 0.0
        if len(eval_lines) > 1:
            print("æœ‰å¤šä¸ªå­—å…¸")
            for lst in eval_lines:
                if lst["ç±»åˆ«"] == "all":
                    map_val = lst["mAP50"]
        else:
            print("åªæœ‰ä¸€ä¸ªå­—å…¸ç»“æœ")
            map_val = [res["mAP50"] for res in eval_lines][0]

        with open(eval_log_file, "a") as f:
            f.write(f"{self.ds_names[list_ind]}: {map_val}\n")


class ProfileModels:
    """
    ProfileModelsç±»ï¼Œç”¨äºåœ¨ONNXå’ŒTensorRTä¸Šåˆ†æä¸åŒæ¨¡å‹çš„æ€§èƒ½ã€‚

    è¿™ä¸ªç±»ç”¨äºåˆ†æä¸åŒæ¨¡å‹çš„æ€§èƒ½ï¼Œè¿”å›æ¨¡å‹çš„é€Ÿåº¦å’ŒFLOPsç­‰ç»“æœã€‚

    å±æ€§ï¼š
        paths (List[str]): è¦åˆ†æçš„æ¨¡å‹è·¯å¾„åˆ—è¡¨ã€‚
        num_timed_runs (int): ç”¨äºåˆ†æçš„å®šæ—¶è¿è¡Œæ¬¡æ•°ã€‚
        num_warmup_runs (int): åˆ†æå‰çš„é¢„çƒ­è¿è¡Œæ¬¡æ•°ã€‚
        min_time (float): æœ€å°çš„åˆ†ææ—¶é—´ï¼ˆç§’ï¼‰ã€‚
        imgsz (int): æ¨¡å‹ä½¿ç”¨çš„å›¾åƒå¤§å°ã€‚
        half (bool): æ ‡å¿—ä½ï¼ŒæŒ‡ç¤ºæ˜¯å¦åœ¨TensorRTåˆ†æä¸­ä½¿ç”¨FP16åŠç²¾åº¦ã€‚
        trt (bool): æ ‡å¿—ä½ï¼ŒæŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨TensorRTè¿›è¡Œåˆ†æã€‚
        device (torch.device): ç”¨äºåˆ†æçš„è®¾å¤‡ã€‚

    æ–¹æ³•ï¼š
        profile: åˆ†ææ¨¡å‹å¹¶æ‰“å°ç»“æœã€‚

    ç¤ºä¾‹ï¼š
        åˆ†ææ¨¡å‹å¹¶æ‰“å°ç»“æœ
        >>> from ultralytics.utils.benchmarks import ProfileModels
        >>> profiler = ProfileModels(["yolov8n.yaml", "yolov8s.yaml"], imgsz=640)
        >>> profiler.profile()
    """

    def __init__(
        self,
        paths: list,
        num_timed_runs=100,
        num_warmup_runs=10,
        min_time=60,
        imgsz=640,
        half=True,
        trt=True,
        device=None,
    ):
        """
        åˆå§‹åŒ–ProfileModelsç±»ä»¥åˆ†ææ¨¡å‹ã€‚

        å‚æ•°ï¼š
            paths (List[str]): è¦åˆ†æçš„æ¨¡å‹è·¯å¾„åˆ—è¡¨ã€‚
            num_timed_runs (int): ç”¨äºåˆ†æçš„å®šæ—¶è¿è¡Œæ¬¡æ•°ã€‚
            num_warmup_runs (int): åˆ†æå‰çš„é¢„çƒ­è¿è¡Œæ¬¡æ•°ã€‚
            min_time (float): æœ€å°åˆ†ææ—¶é—´ï¼ˆç§’ï¼‰ã€‚
            imgsz (int): åˆ†ææ—¶ä½¿ç”¨çš„å›¾åƒå¤§å°ã€‚
            half (bool): æ ‡å¿—ä½ï¼ŒæŒ‡ç¤ºæ˜¯å¦åœ¨TensorRTåˆ†æä¸­ä½¿ç”¨FP16åŠç²¾åº¦ã€‚
            trt (bool): æ ‡å¿—ä½ï¼ŒæŒ‡ç¤ºæ˜¯å¦ä½¿ç”¨TensorRTè¿›è¡Œåˆ†æã€‚
            device (torch.device | None): ç”¨äºåˆ†æçš„è®¾å¤‡ã€‚å¦‚æœä¸ºNoneï¼Œåˆ™è‡ªåŠ¨ç¡®å®šè®¾å¤‡ã€‚

        æ³¨æ„ï¼š
            FP16 'half'å‚æ•°é€‰é¡¹å·²ç§»é™¤ï¼Œå› ä¸ºåœ¨CPUä¸Šä½¿ç”¨FP16æ¯”FP32æ…¢ã€‚

        ç¤ºä¾‹ï¼š
            åˆå§‹åŒ–å¹¶åˆ†ææ¨¡å‹
            >>> from ultralytics.utils.benchmarks import ProfileModels
            >>> profiler = ProfileModels(["yolov8n.yaml", "yolov8s.yaml"], imgsz=640)
            >>> profiler.profile()
        """
        self.paths = paths
        self.num_timed_runs = num_timed_runs
        self.num_warmup_runs = num_warmup_runs
        self.min_time = min_time
        self.imgsz = imgsz
        self.half = half
        self.trt = trt  # è¿è¡ŒTensorRTåˆ†æ
        self.device = device or torch.device(0 if torch.cuda.is_available() else "cpu")

    def profile(self):
        """åˆ†æYOLOæ¨¡å‹åœ¨ä¸åŒæ ¼å¼ï¼ˆåŒ…æ‹¬ONNXå’ŒTensorRTï¼‰ä¸Šçš„é€Ÿåº¦å’Œå‡†ç¡®æ€§ã€‚"""
        files = self.get_files()

        if not files:
            print("æœªæ‰¾åˆ°åŒ¹é…çš„*.ptæˆ–*.onnxæ–‡ä»¶ã€‚")
            return

        table_rows = []
        output = []
        for file in files:
            engine_file = file.with_suffix(".engine")
            if file.suffix in {".pt", ".yaml", ".yml"}:
                model = YOLO(str(file))
                model.fuse()  # ä»¥æ­£ç¡®æŠ¥å‘Šå‚æ•°å’ŒGFLOPs
                model_info = model.info()
                if self.trt and self.device.type != "cpu" and not engine_file.is_file():
                    engine_file = model.export(
                        format="engine",
                        half=self.half,
                        imgsz=self.imgsz,
                        device=self.device,
                        verbose=False,
                    )
                onnx_file = model.export(
                    format="onnx",
                    imgsz=self.imgsz,
                    device=self.device,
                    verbose=False,
                )
            elif file.suffix == ".onnx":
                model_info = self.get_onnx_model_info(file)
                onnx_file = file
            else:
                continue

            t_engine = self.profile_tensorrt_model(str(engine_file))
            t_onnx = self.profile_onnx_model(str(onnx_file))
            table_rows.append(self.generate_table_row(file.stem, t_onnx, t_engine, model_info))
            output.append(self.generate_results_dict(file.stem, t_onnx, t_engine, model_info))

        self.print_table(table_rows)
        return output

    def get_files(self):
        """è¿”å›ç”¨æˆ·æŒ‡å®šçš„æ‰€æœ‰ç›¸å…³æ¨¡å‹æ–‡ä»¶çš„è·¯å¾„åˆ—è¡¨ã€‚"""
        files = []
        for path in self.paths:
            path = Path(path)
            if path.is_dir():
                extensions = ["*.pt", "*.onnx", "*.yaml"]
                files.extend([file for ext in extensions for file in glob.glob(str(path / ext))])
            elif path.suffix in {".pt", ".yaml", ".yml"}:  # æ·»åŠ éç°æœ‰æ–‡ä»¶
                files.append(str(path))
            else:
                files.extend(glob.glob(str(path)))

        print(f"æ­£åœ¨åˆ†æ: {sorted(files)}")
        return [Path(file) for file in sorted(files)]

    @staticmethod
    def get_onnx_model_info(onnx_file: str):
        """ä»ONNXæ¨¡å‹æ–‡ä»¶ä¸­æå–å…ƒæ•°æ®ï¼ŒåŒ…æ‹¬å‚æ•°ã€GFLOPså’Œè¾“å…¥å½¢çŠ¶ã€‚"""
        return 0.0, 0.0, 0.0, 0.0  # è¿”å›ï¼ˆå±‚æ•°ã€å‚æ•°æ•°ã€æ¢¯åº¦æ•°ã€FLOPsï¼‰

    @staticmethod
    def iterative_sigma_clipping(data, sigma=2, max_iters=3):
        """å¯¹æ•°æ®åº”ç”¨è¿­ä»£sigmaè£å‰ªï¼Œä»¥æ ¹æ®æŒ‡å®šçš„sigmaå’Œè¿­ä»£æ¬¡æ•°å»é™¤å¼‚å¸¸å€¼ã€‚"""
        data = np.array(data)
        for _ in range(max_iters):
            mean, std = np.mean(data), np.std(data)
            clipped_data = data[(data > mean - sigma * std) & (data < mean + sigma * std)]
            if len(clipped_data) == len(data):
                break
            data = clipped_data
        return data

    def profile_tensorrt_model(self, engine_file: str, eps: float = 1e-3):
        """ä½¿ç”¨TensorRTå¯¹YOLOæ¨¡å‹æ€§èƒ½è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæµ‹é‡å¹³å‡è¿è¡Œæ—¶é—´å’Œæ ‡å‡†åå·®ã€‚"""
        if not self.trt or not Path(engine_file).is_file():
            return 0.0, 0.0

        # æ¨¡å‹å’Œè¾“å…¥æ•°æ®
        model = YOLO(engine_file)
        input_data = np.zeros((self.imgsz, self.imgsz, 3), dtype=np.uint8)  # å¯¹äºåˆ†ç±»ä»»åŠ¡ä½¿ç”¨uint8

        # çƒ­èº«è¿è¡Œ
        elapsed = 0.0
        for _ in range(3):
            start_time = time.time()
            for _ in range(self.num_warmup_runs):
                model(input_data, imgsz=self.imgsz, verbose=False)
            elapsed = time.time() - start_time

        # è®¡ç®—è¿è¡Œæ¬¡æ•°ï¼Œå–æœ€å°æ—¶é—´æˆ–å®šæ—¶è¿è¡Œæ¬¡æ•°ä¸­çš„è¾ƒå¤§å€¼
        num_runs = max(round(self.min_time / (elapsed + eps) * self.num_warmup_runs), self.num_timed_runs * 50)

        # å®šæ—¶è¿è¡Œ
        run_times = []
        for _ in TQDM(range(num_runs), desc=engine_file):
            results = model(input_data, imgsz=self.imgsz, verbose=False)
            run_times.append(results[0].speed["inference"])  # è½¬æ¢ä¸ºæ¯«ç§’

        run_times = self.iterative_sigma_clipping(np.array(run_times), sigma=2, max_iters=3)  # sigmaä¿®å‰ª
        return np.mean(run_times), np.std(run_times)

    def profile_onnx_model(self, onnx_file: str, eps: float = 1e-3):
        """å¯¹ONNXæ¨¡å‹è¿›è¡ŒåŸºå‡†æµ‹è¯•ï¼Œæµ‹é‡å¤šæ¬¡è¿è¡Œçš„å¹³å‡æ¨ç†æ—¶é—´å’Œæ ‡å‡†åå·®ã€‚"""
        check_requirements("onnxruntime")
        import onnxruntime as ort

        # ä½¿ç”¨'TensorrtExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider'ä¹‹ä¸€çš„ä¼šè¯
        sess_options = ort.SessionOptions()
        sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL
        sess_options.intra_op_num_threads = 8  # é™åˆ¶çº¿ç¨‹æ•°
        sess = ort.InferenceSession(onnx_file, sess_options, providers=["CPUExecutionProvider"])

        input_tensor = sess.get_inputs()[0]
        input_type = input_tensor.type
        dynamic = not all(isinstance(dim, int) and dim >= 0 for dim in input_tensor.shape)  # åŠ¨æ€è¾“å…¥å½¢çŠ¶
        input_shape = (1, 3, self.imgsz, self.imgsz) if dynamic else input_tensor.shape

        # å°†ONNXæ•°æ®ç±»å‹æ˜ å°„åˆ°numpyæ•°æ®ç±»å‹
        if "float16" in input_type:
            input_dtype = np.float16
        elif "float" in input_type:
            input_dtype = np.float32
        elif "double" in input_type:
            input_dtype = np.float64
        elif "int64" in input_type:
            input_dtype = np.int64
        elif "int32" in input_type:
            input_dtype = np.int32
        else:
            raise ValueError(f"ä¸æ”¯æŒçš„ONNXæ•°æ®ç±»å‹ {input_type}")

        input_data = np.random.rand(*input_shape).astype(input_dtype)
        input_name = input_tensor.name
        output_name = sess.get_outputs()[0].name

        # çƒ­èº«è¿è¡Œ
        elapsed = 0.0
        for _ in range(3):
            start_time = time.time()
            for _ in range(self.num_warmup_runs):
                sess.run([output_name], {input_name: input_data})
            elapsed = time.time() - start_time

        # è®¡ç®—è¿è¡Œæ¬¡æ•°ï¼Œå–æœ€å°æ—¶é—´æˆ–å®šæ—¶è¿è¡Œæ¬¡æ•°ä¸­çš„è¾ƒå¤§å€¼
        num_runs = max(round(self.min_time / (elapsed + eps) * self.num_warmup_runs), self.num_timed_runs)

        # å®šæ—¶è¿è¡Œ
        run_times = []
        for _ in TQDM(range(num_runs), desc=onnx_file):
            start_time = time.time()
            sess.run([output_name], {input_name: input_data})
            run_times.append((time.time() - start_time) * 1000)  # è½¬æ¢ä¸ºæ¯«ç§’

        run_times = self.iterative_sigma_clipping(np.array(run_times), sigma=2, max_iters=5)  # sigmaä¿®å‰ª
        return np.mean(run_times), np.std(run_times)

    def generate_table_row(self, model_name, t_onnx, t_engine, model_info):
        """ç”ŸæˆåŒ…å«æ¨¡å‹æ€§èƒ½æŒ‡æ ‡ï¼ˆåŒ…æ‹¬æ¨ç†æ—¶é—´å’Œæ¨¡å‹ç»†èŠ‚ï¼‰çš„è¡¨æ ¼è¡Œå­—ç¬¦ä¸²ã€‚"""
        layers, params, gradients, flops = model_info
        return (
            f"| {model_name:18s} | {self.imgsz} | - | {t_onnx[0]:.1f}Â±{t_onnx[1]:.1f} ms | {t_engine[0]:.1f}Â±"
            f"{t_engine[1]:.1f} ms | {params / 1e6:.1f} | {flops:.1f} |"
        )

    @staticmethod
    def generate_results_dict(model_name, t_onnx, t_engine, model_info):
        """ç”ŸæˆåŒ…å«æ¨¡å‹åç§°ã€å‚æ•°ã€GFLOPså’Œé€Ÿåº¦æŒ‡æ ‡çš„æ€§èƒ½ç»“æœå­—å…¸ã€‚"""
        layers, params, gradients, flops = model_info
        return {
            "model/name": model_name,
            "model/parameters": params,
            "model/GFLOPs": round(flops, 3),
            "model/speed_ONNX(ms)": round(t_onnx[0], 3),
            "model/speed_TensorRT(ms)": round(t_engine[0], 3),
        }

    @staticmethod
    def print_table(table_rows):
        """æ‰“å°æ ¼å¼åŒ–çš„æ¨¡å‹åŸºå‡†æµ‹è¯•ç»“æœè¡¨æ ¼ï¼ŒåŒ…æ‹¬é€Ÿåº¦å’Œå‡†ç¡®æ€§æŒ‡æ ‡ã€‚"""
        gpu = torch.cuda.get_device_name(0) if torch.cuda.is_available() else "GPU"
        headers = [
            "æ¨¡å‹",
            "å°ºå¯¸<br><sup>(åƒç´ )",
            "mAP<sup>val<br>50-95",
            f"é€Ÿåº¦<br><sup>CPU ({get_cpu_info()}) ONNX<br>(ms)",
            f"é€Ÿåº¦<br><sup>{gpu} TensorRT<br>(ms)",
            "å‚æ•°<br><sup>(ç™¾ä¸‡)",
            "FLOPs<br><sup>(åäº¿)",
        ]
        header = "|" + "|".join(f" {h} " for h in headers) + "|"
        separator = "|" + "|".join("-" * (len(h) + 2) for h in headers) + "|"

        print(f"\n\n{header}")
        print(separator)
        for row in table_rows:
            print(row)

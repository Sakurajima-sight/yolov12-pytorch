# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""ç”¨äºä¼°ç®—æœ€ä½³YOLOæ‰¹æ¬¡å¤§å°ï¼Œä»¥ä½¿ç”¨PyTorchä¸­å¯ç”¨CUDAå†…å­˜çš„ä¸€ä¸ªéƒ¨åˆ†ã€‚"""

import os
from copy import deepcopy

import numpy as np
import torch

from ultralytics.utils import DEFAULT_CFG, LOGGER, colorstr
from ultralytics.utils.torch_utils import autocast, profile


def check_train_batch_size(model, imgsz=640, amp=True, batch=-1, max_num_obj=1):
    """
    ä½¿ç”¨autobatch()å‡½æ•°è®¡ç®—æœ€ä½³YOLOè®­ç»ƒæ‰¹æ¬¡å¤§å°ã€‚

    å‚æ•°ï¼š
        model (torch.nn.Module): ç”¨äºæ£€æŸ¥æ‰¹æ¬¡å¤§å°çš„YOLOæ¨¡å‹ã€‚
        imgsz (int, å¯é€‰): ç”¨äºè®­ç»ƒçš„å›¾åƒå¤§å°ã€‚
        amp (bool, å¯é€‰): å¦‚æœä¸ºTrueï¼Œä½¿ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ã€‚
        batch (float, å¯é€‰): è¦ä½¿ç”¨çš„GPUå†…å­˜çš„æ¯”ä¾‹ã€‚å¦‚æœä¸º-1ï¼Œåˆ™ä½¿ç”¨é»˜è®¤å€¼ã€‚
        max_num_obj (int, å¯é€‰): æ•°æ®é›†ä¸­çš„æœ€å¤§å¯¹è±¡æ•°é‡ã€‚

    è¿”å›ï¼š
        (int): ä½¿ç”¨autobatch()å‡½æ•°è®¡ç®—å‡ºçš„æœ€ä½³æ‰¹æ¬¡å¤§å°ã€‚

    æ³¨æ„ï¼š
        å¦‚æœ0.0 < batch < 1.0ï¼Œè¡¨ç¤ºè¦ä½¿ç”¨çš„GPUå†…å­˜çš„æ¯”ä¾‹ã€‚
        å¦åˆ™ï¼Œä½¿ç”¨é»˜è®¤æ¯”ä¾‹0.6ã€‚
    """
    with autocast(enabled=amp):
        return autobatch(
            deepcopy(model).train(), imgsz, fraction=batch if 0.0 < batch < 1.0 else 0.6, max_num_obj=max_num_obj
        )


def autobatch(model, imgsz=640, fraction=0.60, batch_size=DEFAULT_CFG.batch, max_num_obj=1):
    """
    è‡ªåŠ¨ä¼°ç®—æœ€ä½³YOLOæ‰¹æ¬¡å¤§å°ï¼Œä»¥ä½¿ç”¨å¯ç”¨CUDAå†…å­˜çš„ä¸€ä¸ªéƒ¨åˆ†ã€‚

    å‚æ•°ï¼š
        model (torch.nn.module): ç”¨äºè®¡ç®—æ‰¹æ¬¡å¤§å°çš„YOLOæ¨¡å‹ã€‚
        imgsz (int, å¯é€‰): ç”¨ä½œYOLOæ¨¡å‹è¾“å…¥çš„å›¾åƒå¤§å°ã€‚é»˜è®¤ä¸º640ã€‚
        fraction (float, å¯é€‰): è¦ä½¿ç”¨çš„å¯ç”¨CUDAå†…å­˜çš„æ¯”ä¾‹ã€‚é»˜è®¤ä¸º0.60ã€‚
        batch_size (int, å¯é€‰): å¦‚æœæ£€æµ‹åˆ°é”™è¯¯ï¼Œä½¿ç”¨çš„é»˜è®¤æ‰¹æ¬¡å¤§å°ã€‚é»˜è®¤ä¸º16ã€‚
        max_num_obj (int, å¯é€‰): æ•°æ®é›†ä¸­çš„æœ€å¤§å¯¹è±¡æ•°é‡ã€‚

    è¿”å›ï¼š
        (int): æœ€ä½³æ‰¹æ¬¡å¤§å°ã€‚
    """
    # æ£€æŸ¥è®¾å¤‡
    prefix = colorstr("AutoBatch: ")
    LOGGER.info(f"{prefix}è®¡ç®—imgsz={imgsz}çš„æœ€ä½³æ‰¹æ¬¡å¤§å°ï¼Œä½¿ç”¨{fraction * 100}%çš„CUDAå†…å­˜ã€‚")
    device = next(model.parameters()).device  # è·å–æ¨¡å‹è®¾å¤‡
    if device.type in {"cpu", "mps"}:
        LOGGER.info(f"{prefix} âš ï¸ ä»…é€‚ç”¨äºCUDAè®¾å¤‡ï¼Œä½¿ç”¨é»˜è®¤æ‰¹æ¬¡å¤§å°{batch_size}")
        return batch_size
    if torch.backends.cudnn.benchmark:
        LOGGER.info(f"{prefix} âš ï¸ éœ€è¦torch.backends.cudnn.benchmark=Falseï¼Œä½¿ç”¨é»˜è®¤æ‰¹æ¬¡å¤§å°{batch_size}")
        return batch_size

    # æ£€æŸ¥CUDAå†…å­˜
    gb = 1 << 30  # å­—èŠ‚è½¬GiB (1024 ** 3)
    d = f"CUDA:{os.getenv('CUDA_VISIBLE_DEVICES', '0').strip()[0]}"  # 'CUDA:0'
    properties = torch.cuda.get_device_properties(device)  # è®¾å¤‡å±æ€§
    t = properties.total_memory / gb  # GiB æ€»å†…å­˜
    r = torch.cuda.memory_reserved(device) / gb  # GiB å·²ä¿ç•™å†…å­˜
    a = torch.cuda.memory_allocated(device) / gb  # GiB å·²åˆ†é…å†…å­˜
    f = t - (r + a)  # GiB å‰©ä½™å†…å­˜
    LOGGER.info(f"{prefix}{d} ({properties.name}) {t:.2f}G æ€»è®¡, {r:.2f}G ä¿ç•™, {a:.2f}G å·²åˆ†é…, {f:.2f}G å‰©ä½™")

    # åˆ†ææ‰¹æ¬¡å¤§å°
    batch_sizes = [1, 2, 4, 8, 16] if t < 16 else [1, 2, 4, 8, 16, 32, 64]
    try:
        img = [torch.empty(b, 3, imgsz, imgsz) for b in batch_sizes]
        results = profile(img, model, n=1, device=device, max_num_obj=max_num_obj)

        # æ‹Ÿåˆè§£å†³æ–¹æ¡ˆ
        xy = [
            [x, y[2]]
            for i, (x, y) in enumerate(zip(batch_sizes, results))
            if y  # æœ‰æ•ˆç»“æœ
            and isinstance(y[2], (int, float))  # æ•°å­—ç±»å‹
            and 0 < y[2] < t  # åœ¨GPUé™åˆ¶èŒƒå›´å†…
            and (i == 0 or not results[i - 1] or y[2] > results[i - 1][2])  # ç¬¬ä¸€ä¸ªå…ƒç´ æˆ–å†…å­˜é€’å¢
        ]
        fit_x, fit_y = zip(*xy) if xy else ([], [])
        p = np.polyfit(np.log(fit_x), np.log(fit_y), deg=1)  # å¯¹æ•°ç©ºé—´ä¸­çš„ä¸€æ¬¡å¤šé¡¹å¼æ‹Ÿåˆ
        b = int(round(np.exp((np.log(f * fraction) - p[1]) / p[0])))  # yæˆªè·ï¼ˆæœ€ä½³æ‰¹æ¬¡å¤§å°ï¼‰
        if None in results:  # ä¸€äº›å¤§å°å¤±è´¥
            i = results.index(None)  # ç¬¬ä¸€ä¸ªå¤±è´¥çš„ç´¢å¼•
            if b >= batch_sizes[i]:  # æˆªè·é«˜äºå¤±è´¥ç‚¹
                b = batch_sizes[max(i - 1, 0)]  # é€‰æ‹©ä¹‹å‰çš„å®‰å…¨ç‚¹
        if b < 1 or b > 1024:  # bè¶…å‡ºå®‰å…¨èŒƒå›´
            LOGGER.info(f"{prefix}è­¦å‘Š âš ï¸ æ‰¹æ¬¡={b}è¶…å‡ºå®‰å…¨èŒƒå›´ï¼Œä½¿ç”¨é»˜è®¤æ‰¹æ¬¡å¤§å°{batch_size}ã€‚")
            b = batch_size

        fraction = (np.exp(np.polyval(p, np.log(b))) + r + a) / t  # é¢„æµ‹çš„æ¯”ä¾‹
        LOGGER.info(f"{prefix}ä½¿ç”¨æ‰¹æ¬¡å¤§å°{b}ï¼ŒCUDAè®¾å¤‡{d}å†…å­˜ä½¿ç”¨{t * fraction:.2f}G/{t:.2f}G ({fraction * 100:.0f}%) âœ…")
        return b
    except Exception as e:
        LOGGER.warning(f"{prefix}è­¦å‘Š âš ï¸ æ£€æµ‹åˆ°é”™è¯¯: {e}, ä½¿ç”¨é»˜è®¤æ‰¹æ¬¡å¤§å°{batch_size}ã€‚")
        return batch_size
    finally:
        torch.cuda.empty_cache()

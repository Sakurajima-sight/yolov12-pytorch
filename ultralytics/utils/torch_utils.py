# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import gc
import math
import os
import random
import time
from contextlib import contextmanager
from copy import deepcopy
from datetime import datetime
from pathlib import Path
from typing import Union

import numpy as np
import thop
import torch
import torch.distributed as dist
import torch.nn as nn
import torch.nn.functional as F

from ultralytics.utils import (
    DEFAULT_CFG_DICT,
    DEFAULT_CFG_KEYS,
    LOGGER,
    NUM_THREADS,
    PYTHON_VERSION,
    TORCHVISION_VERSION,
    WINDOWS,
    __version__,
    colorstr,
)
from ultralytics.utils.checks import check_version

# ç‰ˆæœ¬æ£€æŸ¥ï¼ˆé»˜è®¤æ£€æŸ¥æ˜¯å¦å¤§äºç­‰äºæœ€å°ç‰ˆæœ¬ï¼‰
TORCH_1_9 = check_version(torch.__version__, "1.9.0")
TORCH_1_13 = check_version(torch.__version__, "1.13.0")
TORCH_2_0 = check_version(torch.__version__, "2.0.0")
TORCH_2_4 = check_version(torch.__version__, "2.4.0")
TORCHVISION_0_10 = check_version(TORCHVISION_VERSION, "0.10.0")
TORCHVISION_0_11 = check_version(TORCHVISION_VERSION, "0.11.0")
TORCHVISION_0_13 = check_version(TORCHVISION_VERSION, "0.13.0")
TORCHVISION_0_18 = check_version(TORCHVISION_VERSION, "0.18.0")
if WINDOWS and check_version(torch.__version__, "==2.4.0"):  # åœ¨ Windows ä¸Šæ‹’ç»ä½¿ç”¨ç‰ˆæœ¬ 2.4.0
    LOGGER.warning(
        "è­¦å‘Š âš ï¸ Windows ä¸Šçš„ torch==2.4.0 å­˜åœ¨å·²çŸ¥é—®é¢˜ï¼Œå»ºè®®å‡çº§åˆ° torch>=2.4.1 ä»¥è§£å†³ "
        "https://github.com/ultralytics/ultralytics/issues/15049"
    )


@contextmanager
def torch_distributed_zero_first(local_rank: int):
    """ç¡®ä¿åˆ†å¸ƒå¼è®­ç»ƒä¸­çš„æ‰€æœ‰è¿›ç¨‹ç­‰å¾…æœ¬åœ°ä¸»èŠ‚ç‚¹ï¼ˆrank 0ï¼‰å…ˆå®Œæˆä»»åŠ¡ã€‚"""
    initialized = dist.is_available() and dist.is_initialized()

    if initialized and local_rank not in {-1, 0}:
        dist.barrier(device_ids=[local_rank])
    yield
    if initialized and local_rank == 0:
        dist.barrier(device_ids=[local_rank])


def smart_inference_mode():
    """å¦‚æœ torch>=1.9.0 åˆ™åº”ç”¨ torch.inference_mode() è£…é¥°å™¨ï¼Œå¦åˆ™åº”ç”¨ torch.no_grad() è£…é¥°å™¨ã€‚"""

    def decorate(fn):
        """æ ¹æ® PyTorch ç‰ˆæœ¬åº”ç”¨é€‚å½“çš„æ¨ç†æ¨¡å¼è£…é¥°å™¨ã€‚"""
        if TORCH_1_9 and torch.is_inference_mode_enabled():
            return fn  # å·²åœ¨ inference_mode ä¸­ï¼Œç›´æ¥è¿”å›å‡½æ•°
        else:
            return (torch.inference_mode if TORCH_1_9 else torch.no_grad)()(fn)

    return decorate


def autocast(enabled: bool, device: str = "cuda"):
    """
    è·å–é€‚åˆ PyTorch ç‰ˆæœ¬å’Œ AMP è®¾ç½®çš„ autocast ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚

    è¯¥å‡½æ•°è¿”å›ä¸€ä¸ªé€‚åˆè‡ªåŠ¨æ··åˆç²¾åº¦ï¼ˆAMPï¼‰è®­ç»ƒçš„ä¸Šä¸‹æ–‡ç®¡ç†å™¨ï¼Œå…¼å®¹è¾ƒæ—§å’Œè¾ƒæ–°çš„ PyTorch ç‰ˆæœ¬ã€‚å®ƒå¤„ç† PyTorch ç‰ˆæœ¬ä¹‹é—´ autocast API çš„å·®å¼‚ã€‚

    å‚æ•°ï¼š
        enabled (bool): æ˜¯å¦å¯ç”¨è‡ªåŠ¨æ··åˆç²¾åº¦ã€‚
        device (str, å¯é€‰): ç”¨äºè‡ªåŠ¨æ··åˆç²¾åº¦çš„è®¾å¤‡ã€‚é»˜è®¤ä¸º 'cuda'ã€‚

    è¿”å›ï¼š
        (torch.amp.autocast): é€‚å½“çš„ autocast ä¸Šä¸‹æ–‡ç®¡ç†å™¨ã€‚

    æ³¨æ„ï¼š
        - å¯¹äº PyTorch ç‰ˆæœ¬ 1.13 åŠæ›´é«˜ç‰ˆæœ¬ï¼Œä½¿ç”¨ `torch.amp.autocast`ã€‚
        - å¯¹äºè¾ƒæ—§ç‰ˆæœ¬ï¼Œä½¿ç”¨ `torch.cuda.autocast`ã€‚

    ç¤ºä¾‹ï¼š
        ```python
        with autocast(amp=True):
            # åœ¨æ­¤å¤„è¿›è¡Œæ··åˆç²¾åº¦æ“ä½œ
            pass
        ```
    """
    if TORCH_1_13:
        return torch.amp.autocast(device, enabled=enabled)
    else:
        return torch.cuda.amp.autocast(enabled)


def get_cpu_info():
    """è¿”å›ç³»ç»Ÿçš„ CPU ä¿¡æ¯å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ 'Apple M2'ã€‚"""
    from ultralytics.utils import PERSISTENT_CACHE  # é¿å…å¾ªç¯å¯¼å…¥é”™è¯¯

    if "cpu_info" not in PERSISTENT_CACHE:
        try:
            import cpuinfo  # pip install py-cpuinfo

            k = "brand_raw", "hardware_raw", "arch_string_raw"  # æŒ‰ä¼˜å…ˆçº§æ’åºçš„é”®
            info = cpuinfo.get_cpu_info()  # ä¿¡æ¯å­—å…¸
            string = info.get(k[0] if k[0] in info else k[1] if k[1] in info else k[2], "unknown")
            PERSISTENT_CACHE["cpu_info"] = string.replace("(R)", "").replace("CPU ", "").replace("@ ", "")
        except Exception:
            pass
    return PERSISTENT_CACHE.get("cpu_info", "unknown")


def get_gpu_info(index):
    """è¿”å›ç³»ç»Ÿçš„ GPU ä¿¡æ¯å­—ç¬¦ä¸²ï¼Œä¾‹å¦‚ 'Tesla T4, 15102MiB'ã€‚"""
    properties = torch.cuda.get_device_properties(index)
    return f"{properties.name}, {properties.total_memory / (1 << 20):.0f}MiB"


def select_device(device="", batch=0, newline=False, verbose=True):
    """
    æ ¹æ®æä¾›çš„å‚æ•°é€‰æ‹©é€‚å½“çš„PyTorchè®¾å¤‡ã€‚

    è¯¥å‡½æ•°æ¥å—ä¸€ä¸ªæŒ‡å®šè®¾å¤‡çš„å­—ç¬¦ä¸²æˆ–torch.deviceå¯¹è±¡ï¼Œå¹¶è¿”å›ä¸€ä¸ªè¡¨ç¤ºæ‰€é€‰è®¾å¤‡çš„torch.deviceå¯¹è±¡ã€‚
    è¯¥å‡½æ•°è¿˜ä¼šéªŒè¯å¯ç”¨è®¾å¤‡çš„æ•°é‡ï¼Œå¦‚æœè¯·æ±‚çš„è®¾å¤‡ä¸å¯ç”¨ï¼Œåˆ™ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚

    å‚æ•°:
        device (str | torch.device, å¯é€‰): è®¾å¤‡å­—ç¬¦ä¸²æˆ–torch.deviceå¯¹è±¡ã€‚
            é€‰é¡¹åŒ…æ‹¬ 'None', 'cpu', æˆ– 'cuda', æˆ– '0' æˆ– '0,1,2,3'ã€‚é»˜è®¤ä¸ºç©ºå­—ç¬¦ä¸²ï¼Œè‡ªåŠ¨é€‰æ‹©
            ç¬¬ä¸€ä¸ªå¯ç”¨çš„GPUï¼Œå¦‚æœæ²¡æœ‰GPUåˆ™é€‰æ‹©CPUã€‚
        batch (int, å¯é€‰): æ¨¡å‹ä½¿ç”¨çš„æ‰¹æ¬¡å¤§å°ã€‚é»˜è®¤ä¸º0ã€‚
        newline (bool, å¯é€‰): å¦‚æœä¸ºTrueï¼Œåœ¨æ—¥å¿—å­—ç¬¦ä¸²æœ«å°¾æ·»åŠ æ¢è¡Œç¬¦ã€‚é»˜è®¤ä¸ºFalseã€‚
        verbose (bool, å¯é€‰): å¦‚æœä¸ºTrueï¼Œåˆ™è®°å½•è®¾å¤‡ä¿¡æ¯ã€‚é»˜è®¤ä¸ºTrueã€‚

    è¿”å›:
        (torch.device): é€‰æ‹©çš„è®¾å¤‡ã€‚

    å¼‚å¸¸:
        ValueError: å¦‚æœæŒ‡å®šçš„è®¾å¤‡ä¸å¯ç”¨ï¼Œæˆ–è€…åœ¨ä½¿ç”¨å¤šä¸ªGPUæ—¶æ‰¹æ¬¡å¤§å°ä¸æ˜¯è®¾å¤‡æ•°é‡çš„å€æ•°ã€‚

    ç¤ºä¾‹:
        >>> select_device("cuda:0")
        device(type='cuda', index=0)

        >>> select_device("cpu")
        device(type='cpu')

    æ³¨æ„:
        è®¾ç½® 'CUDA_VISIBLE_DEVICES' ç¯å¢ƒå˜é‡æ¥æŒ‡å®šä½¿ç”¨å“ªäº›GPUã€‚
    """
    if isinstance(device, torch.device) or str(device).startswith("tpu"):
        return device

    s = f"Ultralytics {__version__} ğŸš€ Python-{PYTHON_VERSION} torch-{torch.__version__} "
    device = str(device).lower()
    for remove in "cuda:", "none", "(", ")", "[", "]", "'", " ":
        device = device.replace(remove, "")  # è½¬ä¸ºå­—ç¬¦ä¸²ï¼Œ'cuda:0' -> '0' å’Œ '(0, 1)' -> '0,1'
    cpu = device == "cpu"
    mps = device in {"mps", "mps:0"}  # Apple Metal Performance Shaders (MPS)
    if cpu or mps:
        os.environ["CUDA_VISIBLE_DEVICES"] = "-1"  # å¼ºåˆ¶ torch.cuda.is_available() = False
    elif device:  # è¯·æ±‚éCPUè®¾å¤‡
        if device == "cuda":
            device = "0"
        if "," in device:
            device = ",".join([x for x in device.split(",") if x])  # åˆ é™¤å¤šä½™çš„é€—å·ï¼Œæ¯”å¦‚ "0,,1" -> "0,1"
        visible = os.environ.get("CUDA_VISIBLE_DEVICES", None)
        os.environ["CUDA_VISIBLE_DEVICES"] = device  # è®¾ç½®ç¯å¢ƒå˜é‡ - å¿…é¡»åœ¨éªŒè¯æ˜¯å¦å¯ç”¨ä¹‹å‰
        if not (torch.cuda.is_available() and torch.cuda.device_count() >= len(device.split(","))):
            LOGGER.info(s)
            install = (
                "å¦‚æœtorchæ²¡æœ‰çœ‹åˆ°CUDAè®¾å¤‡ï¼Œè¯·è®¿é—® https://pytorch.org/get-started/locally/ è·å–æœ€æ–°çš„torchå®‰è£…è¯´æ˜ã€‚\n"
                if torch.cuda.device_count() == 0
                else ""
            )
            raise ValueError(
                f"è¯·æ±‚çš„CUDA 'device={device}' æ— æ•ˆã€‚"
                f" ä½¿ç”¨ 'device=cpu' æˆ–è€…æä¾›æœ‰æ•ˆçš„CUDAè®¾å¤‡ï¼Œä¾‹å¦‚ 'device=0' æˆ– 'device=0,1,2,3' è¿›è¡Œå¤šGPUè®­ç»ƒã€‚\n"
                f"\ntorch.cuda.is_available(): {torch.cuda.is_available()}"
                f"\ntorch.cuda.device_count(): {torch.cuda.device_count()}"
                f"\nos.environ['CUDA_VISIBLE_DEVICES']: {visible}\n"
                f"{install}"
            )

    if not cpu and not mps and torch.cuda.is_available():  # å¦‚æœGPUå¯ç”¨ï¼Œä¼˜å…ˆé€‰æ‹©GPU
        devices = device.split(",") if device else "0"  # ä¾‹å¦‚ "0,1" -> ["0", "1"]
        n = len(devices)  # è®¾å¤‡æ•°é‡
        if n > 1:  # å¤šGPU
            if batch < 1:
                raise ValueError(
                    "Multi-GPUè®­ç»ƒä¸æ”¯æŒæ‰¹æ¬¡å°äº1ï¼Œè¯·æŒ‡å®šä¸€ä¸ªæœ‰æ•ˆçš„æ‰¹æ¬¡å¤§å°ï¼Œä¾‹å¦‚ batch=16ã€‚"
                )
            if batch >= 0 and batch % n != 0:  # æ£€æŸ¥æ‰¹æ¬¡å¤§å°æ˜¯å¦èƒ½è¢«è®¾å¤‡æ•°é‡æ•´é™¤
                raise ValueError(
                    f"'batch={batch}' å¿…é¡»æ˜¯GPUæ•°é‡ {n} çš„å€æ•°ã€‚è¯·å°è¯• 'batch={batch // n * n}' æˆ– "
                    f"'batch={batch // n * n + n}'ï¼Œè¿™ä¸¤ä¸ªæ‰¹æ¬¡å¤§å°èƒ½è¢« {n} æ•´é™¤ã€‚"
                )
        space = " " * (len(s) + 1)
        for i, d in enumerate(devices):
            s += f"{'' if i == 0 else space}CUDA:{d} ({get_gpu_info(i)})\n"  # å­—èŠ‚åˆ°MBçš„è½¬æ¢
        arg = "cuda:0"
    elif mps and TORCH_2_0 and torch.backends.mps.is_available():
        # å¦‚æœMPSå¯ç”¨ï¼Œä¼˜å…ˆä½¿ç”¨MPS
        s += f"MPS ({get_cpu_info()})\n"
        arg = "mps"
    else:  # å¦åˆ™é€€å›åˆ°CPU
        s += f"CPU ({get_cpu_info()})\n"
        arg = "cpu"

    if arg in {"cpu", "mps"}:
        torch.set_num_threads(NUM_THREADS)  # ä¸ºCPUè®­ç»ƒé‡ç½®OMP_NUM_THREADS
    if verbose:
        LOGGER.info(s if newline else s.rstrip())
    return torch.device(arg)


def time_sync():
    """PyTorchç²¾ç¡®æ—¶é—´ã€‚"""
    if torch.cuda.is_available():
        torch.cuda.synchronize()
    return time.time()


def fuse_conv_and_bn(conv, bn):
    """èåˆConv2d()å’ŒBatchNorm2d()å±‚ https://tehnokv.com/posts/fusing-batchnorm-and-conv/ã€‚"""
    fusedconv = (
        nn.Conv2d(
            conv.in_channels,
            conv.out_channels,
            kernel_size=conv.kernel_size,
            stride=conv.stride,
            padding=conv.padding,
            dilation=conv.dilation,
            groups=conv.groups,
            bias=True,
        )
        .requires_grad_(False)
        .to(conv.weight.device)
    )

    # å‡†å¤‡å·ç§¯æ»¤æ³¢å™¨
    w_conv = conv.weight.view(conv.out_channels, -1)
    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))
    fusedconv.weight.copy_(torch.mm(w_bn, w_conv).view(fusedconv.weight.shape))

    # å‡†å¤‡ç©ºé—´åç½®
    b_conv = torch.zeros(conv.weight.shape[0], device=conv.weight.device) if conv.bias is None else conv.bias
    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))
    fusedconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)

    return fusedconv


def fuse_deconv_and_bn(deconv, bn):
    """èåˆ ConvTranspose2d() å’Œ BatchNorm2d() å±‚ã€‚"""
    fuseddconv = (
        nn.ConvTranspose2d(
            deconv.in_channels,
            deconv.out_channels,
            kernel_size=deconv.kernel_size,
            stride=deconv.stride,
            padding=deconv.padding,
            output_padding=deconv.output_padding,
            dilation=deconv.dilation,
            groups=deconv.groups,
            bias=True,
        )
        .requires_grad_(False)
        .to(deconv.weight.device)
    )

    # å‡†å¤‡å·ç§¯æ»¤æ³¢å™¨
    w_deconv = deconv.weight.view(deconv.out_channels, -1)
    w_bn = torch.diag(bn.weight.div(torch.sqrt(bn.eps + bn.running_var)))
    fuseddconv.weight.copy_(torch.mm(w_bn, w_deconv).view(fuseddconv.weight.shape))

    # å‡†å¤‡ç©ºé—´åç½®
    b_conv = torch.zeros(deconv.weight.shape[1], device=deconv.weight.device) if deconv.bias is None else deconv.bias
    b_bn = bn.bias - bn.weight.mul(bn.running_mean).div(torch.sqrt(bn.running_var + bn.eps))
    fuseddconv.bias.copy_(torch.mm(w_bn, b_conv.reshape(-1, 1)).reshape(-1) + b_bn)

    return fuseddconv


def model_info(model, detailed=False, verbose=True, imgsz=640):
    """é€å±‚æ‰“å°å’Œè¿”å›è¯¦ç»†çš„æ¨¡å‹ä¿¡æ¯ã€‚"""
    if not verbose:
        return
    n_p = get_num_params(model)  # å‚æ•°æ€»æ•°
    n_g = get_num_gradients(model)  # æ¢¯åº¦æ€»æ•°
    n_l = len(list(model.modules()))  # å±‚æ•°
    if detailed:
        LOGGER.info(f"{'layer':>5}{'name':>40}{'gradient':>10}{'parameters':>12}{'shape':>20}{'mu':>10}{'sigma':>10}")
        for i, (name, p) in enumerate(model.named_parameters()):
            name = name.replace("module_list.", "")
            LOGGER.info(
                f"{i:>5g}{name:>40s}{p.requires_grad!r:>10}{p.numel():>12g}{str(list(p.shape)):>20s}"
                f"{p.mean():>10.3g}{p.std():>10.3g}{str(p.dtype):>15s}"
            )

    flops = get_flops(model, imgsz)  # imgsz å¯èƒ½æ˜¯ int æˆ– listï¼Œä¾‹å¦‚ imgsz=640 æˆ– imgsz=[640, 320]
    fused = " (fused)" if getattr(model, "is_fused", lambda: False)() else ""
    fs = f", {flops:.1f} GFLOPs" if flops else ""
    yaml_file = getattr(model, "yaml_file", "") or getattr(model, "yaml", {}).get("yaml_file", "")
    model_name = Path(yaml_file).stem.replace("yolo", "YOLO") or "Model"
    LOGGER.info(f"{model_name} æ‘˜è¦{fused}: {n_l:,} å±‚, {n_p:,} å‚æ•°, {n_g:,} æ¢¯åº¦{fs}")
    return n_l, n_p, n_g, flops


def get_num_params(model):
    """è¿”å› YOLO æ¨¡å‹ä¸­çš„å‚æ•°æ€»æ•°ã€‚"""
    return sum(x.numel() for x in model.parameters())


def get_num_gradients(model):
    """è¿”å› YOLO æ¨¡å‹ä¸­å…·æœ‰æ¢¯åº¦çš„å‚æ•°æ€»æ•°ã€‚"""
    return sum(x.numel() for x in model.parameters() if x.requires_grad)


def model_info_for_loggers(trainer):
    """
    è¿”å›åŒ…å«æœ‰ç”¨æ¨¡å‹ä¿¡æ¯çš„å­—å…¸ï¼Œç”¨äºæ—¥å¿—è®°å½•ã€‚

    ç¤ºä¾‹ï¼š
        YOLOv8n æ—¥å¿—ä¿¡æ¯
        ```python
        results = {
            "model/parameters": 3151904,
            "model/GFLOPs": 8.746,
            "model/speed_ONNX(ms)": 41.244,
            "model/speed_TensorRT(ms)": 3.211,
            "model/speed_PyTorch(ms)": 18.755,
        }
        ```
    """
    if trainer.args.profile:  # ä¸º ONNX å’Œ TensorRT è®¡æ—¶
        from ultralytics.utils.benchmarks import ProfileModels

        results = ProfileModels([trainer.last], device=trainer.device).profile()[0]
        results.pop("model/name")
    else:  # åªè¿”å›æœ€è¿‘éªŒè¯çš„ PyTorch æ—¶é—´
        results = {
            "model/parameters": get_num_params(trainer.model),
            "model/GFLOPs": round(get_flops(trainer.model), 3),
        }
    results["model/speed_PyTorch(ms)"] = round(trainer.validator.speed["inference"], 3)
    return results


def get_flops(model, imgsz=640):
    """è¿”å› YOLO æ¨¡å‹çš„ FLOPsï¼ˆæµ®ç‚¹è¿ç®—æ¬¡æ•°ï¼‰ã€‚"""
    try:
        model = de_parallel(model)
        p = next(model.parameters())
        if not isinstance(imgsz, list):
            imgsz = [imgsz, imgsz]  # å¦‚æœæ˜¯ int/floatï¼Œåˆ™æ‰©å±•ä¸º list
        try:
            # ä½¿ç”¨æ­¥å¹…å¤§å°æ¥è®¡ç®—è¾“å…¥å¼ é‡
            stride = max(int(model.stride.max()), 32) if hasattr(model, "stride") else 32  # æœ€å¤§æ­¥å¹…
            im = torch.empty((1, p.shape[1], stride, stride), device=p.device)  # è¾“å…¥å›¾åƒ BCHW æ ¼å¼
            flops = thop.profile(deepcopy(model), inputs=[im], verbose=False)[0] / 1e9 * 2  # ä½¿ç”¨æ­¥å¹…è®¡ç®— GFLOPs
            return flops * imgsz[0] / stride * imgsz[1] / stride  # ä½¿ç”¨å›¾åƒå¤§å°è®¡ç®— GFLOPs
        except Exception:
            # ä½¿ç”¨å®é™…çš„å›¾åƒå¤§å°æ¥è®¡ç®—è¾“å…¥å¼ é‡ï¼ˆä¾‹å¦‚ RTDETR æ¨¡å‹éœ€è¦æ­¤æ“ä½œï¼‰
            im = torch.empty((1, p.shape[1], *imgsz), device=p.device)  # è¾“å…¥å›¾åƒ BCHW æ ¼å¼
            return thop.profile(deepcopy(model), inputs=[im], verbose=False)[0] / 1e9 * 2  # ä½¿ç”¨å›¾åƒå¤§å°è®¡ç®— GFLOPs
    except Exception:
        return 0.0


def get_flops_with_torch_profiler(model, imgsz=640):
    """è®¡ç®—æ¨¡å‹çš„ FLOPsï¼ˆä½¿ç”¨ torch.profiler ä»£æ›¿ thop åŒ…ï¼Œä½†é€Ÿåº¦æ…¢ 2-10 å€ï¼‰ã€‚"""
    if not TORCH_2_0:  # ä»…åœ¨ torch>=2.0 å®ç°äº† torch.profiler
        return 0.0
    model = de_parallel(model)
    p = next(model.parameters())
    if not isinstance(imgsz, list):
        imgsz = [imgsz, imgsz]  # å¦‚æœæ˜¯ int/floatï¼Œåˆ™æ‰©å±•ä¸º list
    try:
        # ä½¿ç”¨æ­¥å¹…å¤§å°æ¥è®¡ç®—è¾“å…¥å¼ é‡
        stride = (max(int(model.stride.max()), 32) if hasattr(model, "stride") else 32) * 2  # æœ€å¤§æ­¥å¹…
        im = torch.empty((1, p.shape[1], stride, stride), device=p.device)  # è¾“å…¥å›¾åƒ BCHW æ ¼å¼
        with torch.profiler.profile(with_flops=True) as prof:
            model(im)
        flops = sum(x.flops for x in prof.key_averages()) / 1e9
        flops = flops * imgsz[0] / stride * imgsz[1] / stride  # 640x640 GFLOPs
    except Exception:
        # ä½¿ç”¨å®é™…çš„å›¾åƒå¤§å°æ¥è®¡ç®—è¾“å…¥å¼ é‡ï¼ˆä¾‹å¦‚ RTDETR æ¨¡å‹éœ€è¦æ­¤æ“ä½œï¼‰
        im = torch.empty((1, p.shape[1], *imgsz), device=p.device)  # è¾“å…¥å›¾åƒ BCHW æ ¼å¼
        with torch.profiler.profile(with_flops=True) as prof:
            model(im)
        flops = sum(x.flops for x in prof.key_averages()) / 1e9
    return flops


def initialize_weights(model):
    """å°†æ¨¡å‹æƒé‡åˆå§‹åŒ–ä¸ºéšæœºå€¼ã€‚"""
    for m in model.modules():
        t = type(m)
        if t is nn.Conv2d:
            pass  # nn.init.kaiming_normal_(m.weight, mode='fan_out', nonlinearity='relu')
        elif t is nn.BatchNorm2d:
            m.eps = 1e-3
            m.momentum = 0.03
        elif t in {nn.Hardswish, nn.LeakyReLU, nn.ReLU, nn.ReLU6, nn.SiLU}:
            m.inplace = True


def scale_img(img, ratio=1.0, same_shape=False, gs=32):
    """ç¼©æ”¾å’Œå¡«å……å›¾åƒå¼ é‡ï¼Œé€‰æ‹©æ€§åœ°ä¿æŒé•¿å®½æ¯”å¹¶å¡«å……åˆ°gsçš„å€æ•°ã€‚"""
    if ratio == 1.0:
        return img
    h, w = img.shape[2:]
    s = (int(h * ratio), int(w * ratio))  # æ–°çš„å°ºå¯¸
    img = F.interpolate(img, size=s, mode="bilinear", align_corners=False)  # é‡è®¾å¤§å°
    if not same_shape:  # å¡«å……/è£å‰ªå›¾åƒ
        h, w = (math.ceil(x * ratio / gs) * gs for x in (h, w))
    return F.pad(img, [0, w - s[1], 0, h - s[0]], value=0.447)  # value = imagenet mean


def copy_attr(a, b, include=(), exclude=()):
    """ä»å¯¹è±¡'b'å¤åˆ¶å±æ€§åˆ°å¯¹è±¡'a'ï¼Œå¯ä»¥é€‰æ‹©åŒ…å«/æ’é™¤æŸäº›å±æ€§ã€‚"""
    for k, v in b.__dict__.items():
        if (len(include) and k not in include) or k.startswith("_") or k in exclude:
            continue
        else:
            setattr(a, k, v)


def get_latest_opset():
    """è¿”å›è¯¥ç‰ˆæœ¬PyTorchæ”¯æŒçš„ç¬¬äºŒæ–°ONNXæ“ä½œé›†ç‰ˆæœ¬ï¼Œç»è¿‡æˆç†Ÿåº¦è°ƒæ•´ã€‚"""
    if TORCH_1_13:
        # å¦‚æœæ˜¯PyTorch>=1.13ï¼ŒåŠ¨æ€è®¡ç®—æœ€æ–°æ“ä½œé›†å‡å»ä¸€ä¸ªä½¿ç”¨'symbolic_opset'
        return max(int(k[14:]) for k in vars(torch.onnx) if "symbolic_opset" in k) - 1
    # å¦åˆ™ï¼Œå¯¹äºPyTorch<=1.12ï¼Œè¿”å›ç›¸åº”çš„é¢„å®šä¹‰æ“ä½œé›†
    version = torch.onnx.producer_version.rsplit(".", 1)[0]  # å³ '2.3'
    return {"1.12": 15, "1.11": 14, "1.10": 13, "1.9": 12, "1.8": 12}.get(version, 12)


def intersect_dicts(da, db, exclude=()):
    """è¿”å›å…·æœ‰åŒ¹é…å½¢çŠ¶çš„äº¤é›†é”®çš„å­—å…¸ï¼Œæ’é™¤'exclude'é”®ï¼Œä½¿ç”¨daçš„å€¼ã€‚"""
    return {k: v for k, v in da.items() if k in db and all(x not in k for x in exclude) and v.shape == db[k].shape}


def is_parallel(model):
    """å¦‚æœæ¨¡å‹æ˜¯ç±»å‹ä¸ºDPæˆ–DDPï¼Œåˆ™è¿”å›Trueã€‚"""
    return isinstance(model, (nn.parallel.DataParallel, nn.parallel.DistributedDataParallel))


def de_parallel(model):
    """å»å¹¶è¡ŒåŒ–æ¨¡å‹ï¼šå¦‚æœæ¨¡å‹æ˜¯DPæˆ–DDPç±»å‹ï¼Œè¿”å›å•GPUæ¨¡å‹ã€‚"""
    return model.module if is_parallel(model) else model


def one_cycle(y1=0.0, y2=1.0, steps=100):
    """è¿”å›ä¸€ä¸ªä»y1åˆ°y2çš„æ­£å¼¦å¡åº¦çš„lambdaå‡½æ•° https://arxiv.org/pdf/1812.01187.pdfã€‚"""
    return lambda x: max((1 - math.cos(x * math.pi / steps)) / 2, 0) * (y2 - y1) + y1


def init_seeds(seed=0, deterministic=False):
    """åˆå§‹åŒ–éšæœºæ•°ç”Ÿæˆå™¨ï¼ˆRNGï¼‰ç§å­ https://pytorch.org/docs/stable/notes/randomness.htmlã€‚"""
    random.seed(seed)
    np.random.seed(seed)
    torch.manual_seed(seed)
    torch.cuda.manual_seed(seed)
    torch.cuda.manual_seed_all(seed)  # ç”¨äºå¤šGPUï¼Œä¿è¯å¼‚å¸¸å®‰å…¨
    # torch.backends.cudnn.benchmark = True  # AutoBatché—®é¢˜ https://github.com/ultralytics/yolov5/issues/9287
    if deterministic:
        if TORCH_2_0:
            torch.use_deterministic_algorithms(True, warn_only=True)  # å¦‚æœä¸æ”¯æŒç¡®å®šæ€§ç®—æ³•åˆ™è­¦å‘Š
            torch.backends.cudnn.deterministic = True
            os.environ["CUBLAS_WORKSPACE_CONFIG"] = ":4096:8"
            os.environ["PYTHONHASHSEED"] = str(seed)
        else:
            LOGGER.warning("WARNING âš ï¸ å‡çº§åˆ°torch>=2.0.0ä»¥è¿›è¡Œç¡®å®šæ€§è®­ç»ƒã€‚")
    else:
        torch.use_deterministic_algorithms(False)
        torch.backends.cudnn.deterministic = False


class ModelEMA:
    """
    æ›´æ–°çš„æŒ‡æ•°ç§»åŠ¨å¹³å‡ï¼ˆEMAï¼‰æ¥è‡ª https://github.com/rwightman/pytorch-image-modelsã€‚ä¿æŒæ¨¡å‹çŠ¶æ€å­—å…¸ï¼ˆå‚æ•°å’Œç¼“å†²åŒºï¼‰ä¸­çš„æ‰€æœ‰å†…å®¹çš„ç§»åŠ¨å¹³å‡ã€‚

    æœ‰å…³EMAçš„è¯¦ç»†ä¿¡æ¯ï¼Œè¯·å‚è§ https://www.tensorflow.org/api_docs/python/tf/train/ExponentialMovingAverage

    ç¦ç”¨EMAæ—¶ï¼Œè®¾ç½®'enabled'å±æ€§ä¸º'False'ã€‚
    """

    def __init__(self, model, decay=0.9999, tau=2000, updates=0):
        """ä¸º'model'åˆå§‹åŒ–EMAï¼Œä½¿ç”¨ç»™å®šçš„å‚æ•°ã€‚"""
        self.ema = deepcopy(de_parallel(model)).eval()  # FP32 EMA
        self.updates = updates  # EMAæ›´æ–°æ¬¡æ•°
        self.decay = lambda x: decay * (1 - math.exp(-x / tau))  # è¡°å‡æŒ‡æ•°å¡åº¦ï¼ˆå¸®åŠ©æ—©æœŸepochsï¼‰
        for p in self.ema.parameters():
            p.requires_grad_(False)
        self.enabled = True

    def update(self, model):
        """æ›´æ–°EMAå‚æ•°ã€‚"""
        if self.enabled:
            self.updates += 1
            d = self.decay(self.updates)

            msd = de_parallel(model).state_dict()  # æ¨¡å‹çš„state_dict
            for k, v in self.ema.state_dict().items():
                if v.dtype.is_floating_point:  # å¯¹äºFP16å’ŒFP32
                    v *= d
                    v += (1 - d) * msd[k].detach()
                    # assert v.dtype == msd[k].dtype == torch.float32, f'{k}: EMA {v.dtype},  model {msd[k].dtype}'

    def update_attr(self, model, include=(), exclude=("process_group", "reducer")):
        """æ›´æ–°å±æ€§å¹¶ä¿å­˜å‰¥ç¦»åçš„æ¨¡å‹ï¼Œç§»é™¤ä¼˜åŒ–å™¨ã€‚"""
        if self.enabled:
            copy_attr(self.ema, model, include, exclude)


def strip_optimizer(f: Union[str, Path] = "best.pt", s: str = "", updates: dict = None) -> dict:
    """
    ä» 'f' ä¸­å‰¥ç¦»ä¼˜åŒ–å™¨ä»¥å®Œæˆè®­ç»ƒï¼Œé€‰æ‹©æ€§åœ°ä¿å­˜ä¸º 's'ã€‚

    å‚æ•°ï¼š
        f (str): ä»ä¸­å‰¥ç¦»ä¼˜åŒ–å™¨çš„æ¨¡å‹æ–‡ä»¶è·¯å¾„ã€‚é»˜è®¤ä¸º 'best.pt'ã€‚
        s (str): ä¿å­˜å‰¥ç¦»ä¼˜åŒ–å™¨åçš„æ¨¡å‹çš„æ–‡ä»¶è·¯å¾„ã€‚å¦‚æœæœªæä¾›ï¼Œå°†è¦†ç›– 'f'ã€‚
        updates (dict): åœ¨ä¿å­˜ä¹‹å‰å°†æ›´æ–°åº”ç”¨åˆ°æ£€æŸ¥ç‚¹çš„å­—å…¸ã€‚

    è¿”å›ï¼š
        (dict): åˆå¹¶åçš„æ£€æŸ¥ç‚¹å­—å…¸ã€‚

    ç¤ºä¾‹ï¼š
        ```python
        from pathlib import Path
        from ultralytics.utils.torch_utils import strip_optimizer

        for f in Path("path/to/model/checkpoints").rglob("*.pt"):
            strip_optimizer(f)
        ```

    æ³¨æ„ï¼š
        ä½¿ç”¨ `ultralytics.nn.torch_safe_load` æ¥åŠ è½½ç¼ºå¤±çš„æ¨¡å—ï¼Œä¾‹å¦‚ `x = torch_safe_load(f)[0]`
    """
    try:
        x = torch.load(f, map_location=torch.device("cpu"))
        assert isinstance(x, dict), "æ£€æŸ¥ç‚¹ä¸æ˜¯ä¸€ä¸ª Python å­—å…¸"
        assert "model" in x, "'model' åœ¨æ£€æŸ¥ç‚¹ä¸­ç¼ºå¤±"
    except Exception as e:
        LOGGER.warning(f"è­¦å‘Š âš ï¸ è·³è¿‡ {f}ï¼Œä¸æ˜¯ä¸€ä¸ªæœ‰æ•ˆçš„ Ultralytics æ¨¡å‹: {e}")
        return {}

    metadata = {
        "date": datetime.now().isoformat(),
        "version": __version__,
        "license": "AGPL-3.0 License (https://ultralytics.com/license)",
        "docs": "https://docs.ultralytics.com",
    }

    # æ›´æ–°æ¨¡å‹
    if x.get("ema"):
        x["model"] = x["ema"]  # ä½¿ç”¨ EMA æ›¿æ¢æ¨¡å‹
    if hasattr(x["model"], "args"):
        x["model"].args = dict(x["model"].args)  # å°† IterableSimpleNamespace è½¬æ¢ä¸ºå­—å…¸
    if hasattr(x["model"], "criterion"):
        x["model"].criterion = None  # ç§»é™¤æŸå¤±å‡½æ•°
    x["model"].half()  # è½¬ä¸º FP16
    for p in x["model"].parameters():
        p.requires_grad = False

    # æ›´æ–°å…¶ä»–é”®
    args = {**DEFAULT_CFG_DICT, **x.get("train_args", {})}  # åˆå¹¶å‚æ•°
    for k in "optimizer", "best_fitness", "ema", "updates":  # éœ€è¦æ¸…é™¤çš„é”®
        x[k] = None
    x["epoch"] = -1
    x["train_args"] = {k: v for k, v in args.items() if k in DEFAULT_CFG_KEYS}  # ä»…ä¿ç•™é»˜è®¤é”®
    # x['model'].args = x['train_args']

    # ä¿å­˜
    combined = {**metadata, **x, **(updates or {})}
    torch.save(combined, s or f)  # åˆå¹¶å­—å…¸ï¼ˆä¼˜å…ˆä½¿ç”¨å³ä¾§ï¼‰
    mb = os.path.getsize(s or f) / 1e6  # æ–‡ä»¶å¤§å°
    LOGGER.info(f"ä¼˜åŒ–å™¨å·²ä» {f} ä¸­ç§»é™¤,{f' å·²ä¿å­˜ä¸º {s},' if s else ''} {mb:.1f}MB")
    return combined


def convert_optimizer_state_dict_to_fp16(state_dict):
    """
    å°†ç»™å®šä¼˜åŒ–å™¨çš„ state_dict è½¬æ¢ä¸º FP16ï¼Œé‡ç‚¹æ˜¯å°† 'state' é”®ä¸­çš„å¼ é‡è½¬æ¢ä¸º FP16ã€‚

    è¯¥æ–¹æ³•æ—¨åœ¨å‡å°‘å­˜å‚¨å¤§å°ï¼Œè€Œä¸æ”¹å˜ 'param_groups'ï¼Œå› ä¸ºå®ƒä»¬åŒ…å«éå¼ é‡æ•°æ®ã€‚
    """
    for state in state_dict["state"].values():
        for k, v in state.items():
            if k != "step" and isinstance(v, torch.Tensor) and v.dtype is torch.float32:
                state[k] = v.half()

    return state_dict


@contextmanager
def cuda_memory_usage(device=None):
    """
    ç›‘æ§å’Œç®¡ç† CUDA å†…å­˜ä½¿ç”¨æƒ…å†µã€‚

    è¯¥å‡½æ•°æ£€æŸ¥æ˜¯å¦å¯ç”¨ CUDAï¼Œå¦‚æœå¯ç”¨ï¼Œåˆ™æ¸…ç©º CUDA ç¼“å­˜ä»¥é‡Šæ”¾æœªä½¿ç”¨çš„å†…å­˜ã€‚
    ç„¶åå®ƒä¼šè¿”å›ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å«å†…å­˜ä½¿ç”¨ä¿¡æ¯ï¼Œè°ƒç”¨è€…å¯ä»¥æ›´æ–°è¯¥å­—å…¸ã€‚
    æœ€åï¼Œå®ƒä¼šæ›´æ–°å­—å…¸ï¼Œæ˜¾ç¤ºæŒ‡å®šè®¾å¤‡ä¸Š CUDA å·²é¢„ç•™çš„å†…å­˜é‡ã€‚

    å‚æ•°ï¼š
        device (torch.device, å¯é€‰): è¦æŸ¥è¯¢å†…å­˜ä½¿ç”¨æƒ…å†µçš„ CUDA è®¾å¤‡ã€‚é»˜è®¤ä¸º Noneã€‚

    è¿”å›ï¼š
        (dict): åŒ…å«é”® 'memory' çš„å­—å…¸ï¼Œåˆå§‹å€¼ä¸º 0ï¼Œè°ƒç”¨åä¼šæ›´æ–°ä¸ºå·²é¢„ç•™çš„å†…å­˜ã€‚
    """
    cuda_info = dict(memory=0)
    if torch.cuda.is_available():
        torch.cuda.empty_cache()
        try:
            yield cuda_info
        finally:
            cuda_info["memory"] = torch.cuda.memory_reserved(device)
    else:
        yield cuda_info


def profile(input, ops, n=10, device=None, max_num_obj=0):
    """
    Ultralyticsé€Ÿåº¦ã€å†…å­˜å’ŒFLOPsåˆ†æå™¨ã€‚

    ç¤ºä¾‹:
        ```python
        from ultralytics.utils.torch_utils import profile

        input = torch.randn(16, 3, 640, 640)
        m1 = lambda x: x * torch.sigmoid(x)
        m2 = nn.SiLU()
        profile(input, [m1, m2], n=100)  # åœ¨100æ¬¡è¿­ä»£ä¸­è¿›è¡Œæ€§èƒ½åˆ†æ
        ```
    """
    results = []
    if not isinstance(device, torch.device):
        device = select_device(device)
    LOGGER.info(
        f"{'Params':>12s}{'GFLOPs':>12s}{'GPU_mem (GB)':>14s}{'forward (ms)':>14s}{'backward (ms)':>14s}"
        f"{'input':>24s}{'output':>24s}"
    )
    gc.collect()  # å°è¯•é‡Šæ”¾æœªä½¿ç”¨çš„å†…å­˜
    torch.cuda.empty_cache()
    for x in input if isinstance(input, list) else [input]:
        x = x.to(device)
        x.requires_grad = True
        for m in ops if isinstance(ops, list) else [ops]:
            m = m.to(device) if hasattr(m, "to") else m  # è®¾ç½®è®¾å¤‡
            m = m.half() if hasattr(m, "half") and isinstance(x, torch.Tensor) and x.dtype is torch.float16 else m
            tf, tb, t = 0, 0, [0, 0, 0]  # æ­£å‘ä¼ æ’­æ—¶é—´ï¼Œåå‘ä¼ æ’­æ—¶é—´
            try:
                flops = thop.profile(m, inputs=[x], verbose=False)[0] / 1e9 * 2  # GFLOPs
            except Exception:
                flops = 0

            try:
                mem = 0
                for _ in range(n):
                    with cuda_memory_usage(device) as cuda_info:
                        t[0] = time_sync()
                        y = m(x)
                        t[1] = time_sync()
                        try:
                            (sum(yi.sum() for yi in y) if isinstance(y, list) else y).sum().backward()
                            t[2] = time_sync()
                        except Exception:  # æ²¡æœ‰åå‘ä¼ æ’­æ–¹æ³•
                            # print(e)  # ç”¨äºè°ƒè¯•
                            t[2] = float("nan")
                    mem += cuda_info["memory"] / 1e9  # (GB)
                    tf += (t[1] - t[0]) * 1000 / n  # æ¯æ¬¡æ­£å‘ä¼ æ’­æ—¶é—´ (ms)
                    tb += (t[2] - t[1]) * 1000 / n  # æ¯æ¬¡åå‘ä¼ æ’­æ—¶é—´ (ms)
                    if max_num_obj:  # æ¨¡æ‹Ÿè®­ç»ƒæ—¶æ¯å¼ å›¾ç‰‡çš„é¢„æµ‹ç›®æ ‡æ•°ï¼ˆç”¨äºAutoBatchï¼‰
                        with cuda_memory_usage(device) as cuda_info:
                            torch.randn(
                                x.shape[0],
                                max_num_obj,
                                int(sum((x.shape[-1] / s) * (x.shape[-2] / s) for s in m.stride.tolist())),
                                device=device,
                                dtype=torch.float32,
                            )
                        mem += cuda_info["memory"] / 1e9  # (GB)
                s_in, s_out = (tuple(x.shape) if isinstance(x, torch.Tensor) else "list" for x in (x, y))  # è¾“å…¥è¾“å‡ºå½¢çŠ¶
                p = sum(x.numel() for x in m.parameters()) if isinstance(m, nn.Module) else 0  # æ¨¡å‹å‚æ•°æ•°ç›®
                LOGGER.info(f"{p:12}{flops:12.4g}{mem:>14.3f}{tf:14.4g}{tb:14.4g}{str(s_in):>24s}{str(s_out):>24s}")
                results.append([p, flops, mem, tf, tb, s_in, s_out])
            except Exception as e:
                LOGGER.info(e)
                results.append(None)
            finally:
                gc.collect()  # å°è¯•é‡Šæ”¾æœªä½¿ç”¨çš„å†…å­˜
                torch.cuda.empty_cache()
    return results


class EarlyStopping:
    """æ—©åœç±»ï¼Œå½“æŒ‡å®šçš„è½®æ•°å†…æ²¡æœ‰æ”¹å–„æ—¶åœæ­¢è®­ç»ƒã€‚"""

    def __init__(self, patience=50):
        """
        åˆå§‹åŒ–æ—©åœå¯¹è±¡ã€‚

        å‚æ•°:
            patience (int, å¯é€‰): åœ¨è®­ç»ƒçš„é€‚åº”åº¦åœæ­¢æ”¹å–„åç­‰å¾…çš„è½®æ•°ï¼Œç›´åˆ°åœæ­¢è®­ç»ƒã€‚
        """
        self.best_fitness = 0.0  # å³ mAP
        self.best_epoch = 0
        self.patience = patience or float("inf")  # ç­‰å¾…çš„è½®æ•°ï¼Œå¦‚æœé€‚åº”åº¦åœæ­¢æ”¹å–„åˆ™åœæ­¢è®­ç»ƒ
        self.possible_stop = False  # å¯èƒ½åœ¨ä¸‹ä¸€è½®åœæ­¢

    def __call__(self, epoch, fitness):
        """
        æ£€æŸ¥æ˜¯å¦åœæ­¢è®­ç»ƒã€‚

        å‚æ•°:
            epoch (int): å½“å‰è®­ç»ƒè½®æ¬¡
            fitness (float): å½“å‰è½®æ¬¡çš„é€‚åº”åº¦å€¼

        è¿”å›:
            (bool): å¦‚æœè®­ç»ƒåº”è¯¥åœæ­¢ï¼Œåˆ™è¿”å›Trueï¼Œå¦åˆ™è¿”å›False
        """
        if fitness is None:  # æ£€æŸ¥fitnessæ˜¯å¦ä¸ºNoneï¼ˆå½“val=Falseæ—¶ä¼šå‘ç”Ÿï¼‰
            return False

        if fitness >= self.best_fitness:  # >= 0 å…è®¸è®­ç»ƒåˆæœŸé€‚åº”åº¦ä¸ºé›¶çš„æƒ…å†µ
            self.best_epoch = epoch
            self.best_fitness = fitness
        delta = epoch - self.best_epoch  # æ²¡æœ‰æ”¹å–„çš„è½®æ•°
        self.possible_stop = delta >= (self.patience - 1)  # å¦‚æœdeltaå¤§äºç­‰äºpatienceï¼Œåˆ™å¯èƒ½ä¼šåœæ­¢
        stop = delta >= self.patience  # å¦‚æœè¶…å‡ºpatienceåˆ™åœæ­¢è®­ç»ƒ
        if stop:
            prefix = colorstr("EarlyStopping: ")
            LOGGER.info(
                f"{prefix}è®­ç»ƒæå‰åœæ­¢ï¼Œå› ä¸ºåœ¨æœ€å {self.patience} è½®ä¸­æ²¡æœ‰è§‚å¯Ÿåˆ°æ”¹å–„ã€‚"
                f"æœ€ä½³ç»“æœå‡ºç°åœ¨ç¬¬ {self.best_epoch} è½®ï¼Œæœ€ä½³æ¨¡å‹ä¿å­˜ä¸º best.ptã€‚\n"
                f"è¦æ›´æ–° EarlyStopping(patience={self.patience})ï¼Œè¯·ä¼ é€’ä¸€ä¸ªæ–°çš„ patience å€¼ï¼Œ"
                f"ä¾‹å¦‚ `patience=300` æˆ–ä½¿ç”¨ `patience=0` æ¥ç¦ç”¨æ—©åœã€‚"
            )
        return stop


class FXModel(nn.Module):
    """
    ä¸€ä¸ªç”¨äºtorch.fxå…¼å®¹æ€§çš„è‡ªå®šä¹‰æ¨¡å‹ç±»ã€‚

    è¯¥ç±»æ‰©å±•äº†`torch.nn.Module`ï¼Œæ—¨åœ¨ç¡®ä¿ä¸torch.fxå…¼å®¹ï¼Œä»¥è¿›è¡Œè¿½è¸ªå’Œå›¾å½¢æ“ä½œã€‚
    å®ƒå¤åˆ¶äº†ç°æœ‰æ¨¡å‹çš„å±æ€§ï¼Œå¹¶æ˜¾å¼è®¾ç½®æ¨¡å‹å±æ€§ä»¥ç¡®ä¿æ­£ç¡®å¤åˆ¶ã€‚

    å‚æ•°:
        model (torch.nn.Module): è¦åŒ…è£…ä»¥ä¾¿ä¸torch.fxå…¼å®¹çš„åŸå§‹æ¨¡å‹ã€‚
    """

    def __init__(self, model):
        """
        åˆå§‹åŒ–FXModelã€‚

        å‚æ•°:
            model (torch.nn.Module): è¦åŒ…è£…ä»¥ä¾¿ä¸torch.fxå…¼å®¹çš„åŸå§‹æ¨¡å‹ã€‚
        """
        super().__init__()
        copy_attr(self, model)
        # æ˜¾å¼è®¾ç½®`model`ï¼Œå› ä¸º`copy_attr`åœ¨æŸäº›æƒ…å†µä¸‹æ²¡æœ‰å¤åˆ¶å®ƒã€‚
        self.model = model.model

    def forward(self, x):
        """
        é€šè¿‡æ¨¡å‹è¿›è¡Œå‰å‘ä¼ æ’­ã€‚

        è¯¥æ–¹æ³•æ‰§è¡Œæ¨¡å‹çš„å‰å‘ä¼ æ’­ï¼Œå¤„ç†å±‚ä¹‹é—´çš„ä¾èµ–å…³ç³»å¹¶ä¿å­˜ä¸­é—´è¾“å‡ºã€‚

        å‚æ•°:
            x (torch.Tensor): è¾“å…¥åˆ°æ¨¡å‹çš„å¼ é‡ã€‚

        è¿”å›:
            (torch.Tensor): æ¨¡å‹çš„è¾“å‡ºå¼ é‡ã€‚
        """
        y = []  # è¾“å‡ºåˆ—è¡¨
        for m in self.model:
            if m.f != -1:  # å¦‚æœä¸æ˜¯æ¥è‡ªå‰ä¸€å±‚
                # æ¥è‡ªæ—©æœŸå±‚
                x = y[m.f] if isinstance(m.f, int) else [x if j == -1 else y[j] for j in m.f]
            x = m(x)  # æ‰§è¡Œ
            y.append(x)  # ä¿å­˜è¾“å‡º
        return x

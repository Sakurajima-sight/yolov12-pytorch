# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import hashlib
import json
import os
import random
import subprocess
import time
import zipfile
from multiprocessing.pool import ThreadPool
from pathlib import Path
from tarfile import is_tarfile

import cv2
import numpy as np
from PIL import Image, ImageOps

from ultralytics.nn.autobackend import check_class_names
from ultralytics.utils import (
    DATASETS_DIR,
    LOGGER,
    NUM_THREADS,
    ROOT,
    SETTINGS_FILE,
    TQDM,
    clean_url,
    colorstr,
    emojis,
    is_dir_writeable,
    yaml_load,
    yaml_save,
)
from ultralytics.utils.checks import check_file, check_font, is_ascii
from ultralytics.utils.downloads import download, safe_download, unzip_file
from ultralytics.utils.ops import segments2boxes

HELP_URL = "è¯·å‚é˜… https://docs.ultralytics.com/datasets è·å–æ•°æ®é›†æ ¼å¼åŒ–æŒ‡å¯¼ã€‚"
IMG_FORMATS = {"bmp", "dng", "jpeg", "jpg", "mpo", "png", "tif", "tiff", "webp", "pfm", "heic"}  # å›¾åƒåç¼€
VID_FORMATS = {"asf", "avi", "gif", "m4v", "mkv", "mov", "mp4", "mpeg", "mpg", "ts", "wmv", "webm"}  # è§†é¢‘åç¼€
PIN_MEMORY = str(os.getenv("PIN_MEMORY", True)).lower() == "true"  # æ•°æ®åŠ è½½å™¨çš„å…¨å±€ pin_memory è®¾ç½®
FORMATS_HELP_MSG = f"æ”¯æŒçš„æ ¼å¼æœ‰ï¼š\nå›¾åƒ: {IMG_FORMATS}\nè§†é¢‘: {VID_FORMATS}"


def img2label_paths(img_paths):
    """æ ¹æ®å›¾åƒè·¯å¾„å®šä¹‰æ ‡ç­¾è·¯å¾„ã€‚"""
    sa, sb = f"{os.sep}images{os.sep}", f"{os.sep}labels{os.sep}"  # /images/ å’Œ /labels/ å­å­—ç¬¦ä¸²
    return [sb.join(x.rsplit(sa, 1)).rsplit(".", 1)[0] + ".txt" for x in img_paths]


def get_hash(paths):
    """è¿”å›ä¸€ç»„è·¯å¾„ï¼ˆæ–‡ä»¶æˆ–ç›®å½•ï¼‰çš„å•ä¸ªå“ˆå¸Œå€¼ã€‚"""
    size = sum(os.path.getsize(p) for p in paths if os.path.exists(p))  # è®¡ç®—æ–‡ä»¶å¤§å°
    h = hashlib.sha256(str(size).encode())  # å¯¹æ–‡ä»¶å¤§å°è¿›è¡Œå“ˆå¸Œ
    h.update("".join(paths).encode())  # å¯¹è·¯å¾„è¿›è¡Œå“ˆå¸Œ
    return h.hexdigest()  # è¿”å›å“ˆå¸Œå€¼


def exif_size(img: Image.Image):
    """è¿”å›ç»è¿‡ EXIF ä¿®æ­£åçš„ PIL å›¾åƒå¤§å°ã€‚"""
    s = img.size  # (å®½åº¦, é«˜åº¦)
    if img.format == "JPEG":  # ä»…æ”¯æŒ JPEG å›¾åƒ
        try:
            if exif := img.getexif():
                rotation = exif.get(274, None)  # EXIF ä¸­æ—‹è½¬æ ‡ç­¾çš„é”®æ˜¯ 274
                if rotation in {6, 8}:  # æ—‹è½¬ 270 æˆ– 90 åº¦
                    s = s[1], s[0]
        except Exception:
            pass
    return s


def verify_image(args):
    """éªŒè¯å•å¼ å›¾åƒã€‚"""
    (im_file, cls), prefix = args
    # æ•°é‡ï¼ˆå·²æ‰¾åˆ°ï¼ŒæŸåï¼‰ï¼Œæ¶ˆæ¯
    nf, nc, msg = 0, 0, ""
    try:
        im = Image.open(im_file)
        im.verify()  # PILéªŒè¯
        shape = exif_size(im)  # å›¾åƒå°ºå¯¸
        shape = (shape[1], shape[0])  # é«˜å®½
        assert (shape[0] > 9) & (shape[1] > 9), f"å›¾åƒå°ºå¯¸ {shape} å°äº10åƒç´ "
        assert im.format.lower() in IMG_FORMATS, f"æ— æ•ˆçš„å›¾åƒæ ¼å¼ {im.format}ã€‚ {FORMATS_HELP_MSG}"
        if im.format.lower() in {"jpg", "jpeg"}:
            with open(im_file, "rb") as f:
                f.seek(-2, 2)
                if f.read() != b"\xff\xd9":  # æŸåçš„JPEG
                    ImageOps.exif_transpose(Image.open(im_file)).save(im_file, "JPEG", subsampling=0, quality=100)
                    msg = f"{prefix}è­¦å‘Š âš ï¸ {im_file}: æŸåçš„JPEGå·²æ¢å¤å¹¶ä¿å­˜"
        nf = 1
    except Exception as e:
        nc = 1
        msg = f"{prefix}è­¦å‘Š âš ï¸ {im_file}: å¿½ç•¥æŸåçš„å›¾åƒ/æ ‡ç­¾: {e}"
    return (im_file, cls), nf, nc, msg


def verify_image_label(args):
    """éªŒè¯å•å¼ å›¾åƒ-æ ‡ç­¾å¯¹ã€‚"""
    im_file, lb_file, prefix, keypoint, num_cls, nkpt, ndim = args
    # æ•°é‡ï¼ˆç¼ºå¤±ï¼Œå·²æ‰¾åˆ°ï¼Œç©ºï¼ŒæŸåï¼‰ï¼Œæ¶ˆæ¯ï¼Œåˆ†å‰²ï¼Œå…³é”®ç‚¹
    nm, nf, ne, nc, msg, segments, keypoints = 0, 0, 0, 0, "", [], None
    try:
        # éªŒè¯å›¾åƒ
        im = Image.open(im_file)
        im.verify()  # PILéªŒè¯
        shape = exif_size(im)  # å›¾åƒå°ºå¯¸
        shape = (shape[1], shape[0])  # é«˜å®½
        assert (shape[0] > 9) & (shape[1] > 9), f"å›¾åƒå°ºå¯¸ {shape} å°äº10åƒç´ "
        assert im.format.lower() in IMG_FORMATS, f"æ— æ•ˆçš„å›¾åƒæ ¼å¼ {im.format}ã€‚ {FORMATS_HELP_MSG}"
        if im.format.lower() in {"jpg", "jpeg"}:
            with open(im_file, "rb") as f:
                f.seek(-2, 2)
                if f.read() != b"\xff\xd9":  # æŸåçš„JPEG
                    ImageOps.exif_transpose(Image.open(im_file)).save(im_file, "JPEG", subsampling=0, quality=100)
                    msg = f"{prefix}è­¦å‘Š âš ï¸ {im_file}: æŸåçš„JPEGå·²æ¢å¤å¹¶ä¿å­˜"

        # éªŒè¯æ ‡ç­¾
        if os.path.isfile(lb_file):
            nf = 1  # æ ‡ç­¾å·²æ‰¾åˆ°
            with open(lb_file) as f:
                lb = [x.split() for x in f.read().strip().splitlines() if len(x)]
                if any(len(x) > 6 for x in lb) and (not keypoint):  # æ˜¯åˆ†å‰²
                    classes = np.array([x[0] for x in lb], dtype=np.float32)
                    segments = [np.array(x[1:], dtype=np.float32).reshape(-1, 2) for x in lb]  # (ç±»ï¼Œxy1...)
                    lb = np.concatenate((classes.reshape(-1, 1), segments2boxes(segments)), 1)  # (ç±»ï¼Œxywh)
                lb = np.array(lb, dtype=np.float32)
            if nl := len(lb):
                if keypoint:
                    assert lb.shape[1] == (5 + nkpt * ndim), f"æ ‡ç­¾éœ€è¦ {(5 + nkpt * ndim)} åˆ—"
                    points = lb[:, 5:].reshape(-1, ndim)[:, :2]
                else:
                    assert lb.shape[1] == 5, f"æ ‡ç­¾éœ€è¦5åˆ—ï¼Œæ£€æµ‹åˆ° {lb.shape[1]} åˆ—"
                    points = lb[:, 1:]
                assert points.max() <= 1, f"éå½’ä¸€åŒ–æˆ–è¶…å‡ºèŒƒå›´çš„åæ ‡ {points[points > 1]}"
                assert lb.min() >= 0, f"è´Ÿæ ‡ç­¾å€¼ {lb[lb < 0]}"

                # æ‰€æœ‰æ ‡ç­¾
                max_cls = lb[:, 0].max()  # æœ€å¤§æ ‡ç­¾æ•°
                assert max_cls <= num_cls, (
                    f"æ ‡ç­¾ç±»åˆ« {int(max_cls)} è¶…å‡ºæ•°æ®é›†ç±»åˆ«æ•° {num_cls}ã€‚"
                    f"å¯èƒ½çš„ç±»åˆ«æ ‡ç­¾æ˜¯ 0-{num_cls - 1}"
                )
                _, i = np.unique(lb, axis=0, return_index=True)
                if len(i) < nl:  # é‡å¤è¡Œæ£€æŸ¥
                    lb = lb[i]  # ç§»é™¤é‡å¤é¡¹
                    if segments:
                        segments = [segments[x] for x in i]
                    msg = f"{prefix}è­¦å‘Š âš ï¸ {im_file}: {nl - len(i)} ä¸ªé‡å¤æ ‡ç­¾å·²ç§»é™¤"
            else:
                ne = 1  # æ ‡ç­¾ä¸ºç©º
                lb = np.zeros((0, (5 + nkpt * ndim) if keypoint else 5), dtype=np.float32)
        else:
            nm = 1  # æ ‡ç­¾ç¼ºå¤±
            lb = np.zeros((0, (5 + nkpt * ndim) if keypoints else 5), dtype=np.float32)
        if keypoint:
            keypoints = lb[:, 5:].reshape(-1, nkpt, ndim)
            if ndim == 2:
                kpt_mask = np.where((keypoints[..., 0] < 0) | (keypoints[..., 1] < 0), 0.0, 1.0).astype(np.float32)
                keypoints = np.concatenate([keypoints, kpt_mask[..., None]], axis=-1)  # (nl, nkpt, 3)
        lb = lb[:, :5]
        return im_file, lb, shape, segments, keypoints, nm, nf, ne, nc, msg
    except Exception as e:
        nc = 1
        msg = f"{prefix}è­¦å‘Š âš ï¸ {im_file}: å¿½ç•¥æŸåçš„å›¾åƒ/æ ‡ç­¾: {e}"
        return [None, None, None, None, None, nm, nf, ne, nc, msg]


def visualize_image_annotations(image_path, txt_path, label_map):
    """
    åœ¨å›¾åƒä¸Šå¯è§†åŒ–YOLOæ ‡æ³¨ï¼ˆè¾¹ç•Œæ¡†å’Œç±»åˆ«æ ‡ç­¾ï¼‰ã€‚

    è¯¥å‡½æ•°è¯»å–å›¾åƒåŠå…¶å¯¹åº”çš„YOLOæ ¼å¼æ ‡æ³¨æ–‡ä»¶ï¼Œç„¶å
    ç»˜åˆ¶æ£€æµ‹åˆ°çš„ç‰©ä½“çš„è¾¹ç•Œæ¡†ï¼Œå¹¶ç”¨ç›¸åº”çš„ç±»åˆ«åç§°æ ‡æ³¨å®ƒä»¬ã€‚
    è¾¹ç•Œæ¡†çš„é¢œè‰²æ ¹æ®ç±»åˆ«IDåˆ†é…ï¼Œæ–‡æœ¬é¢œè‰²æ ¹æ®èƒŒæ™¯è‰²çš„äº®åº¦åŠ¨æ€è°ƒæ•´ï¼Œ
    ä»¥ç¡®ä¿å¯è¯»æ€§ã€‚

    å‚æ•°ï¼š
        image_path (str): è¦æ ‡æ³¨çš„å›¾åƒæ–‡ä»¶è·¯å¾„ï¼Œå¯ä»¥æ˜¯PILæ”¯æŒçš„æ ¼å¼ï¼ˆä¾‹å¦‚ï¼Œ.jpgï¼Œ.pngï¼‰ã€‚
        txt_path (str): YOLOæ ¼å¼çš„æ ‡æ³¨æ–‡ä»¶è·¯å¾„ï¼Œå…¶ä¸­æ¯ä¸ªç‰©ä½“åº”åŒ…å«ä¸€è¡Œï¼Œæ ¼å¼ä¸ºï¼š
                        - class_id (int): ç±»åˆ«ç´¢å¼•ã€‚
                        - x_center (float): è¾¹ç•Œæ¡†çš„Xä¸­å¿ƒï¼ˆç›¸å¯¹äºå›¾åƒå®½åº¦ï¼‰ã€‚
                        - y_center (float): è¾¹ç•Œæ¡†çš„Yä¸­å¿ƒï¼ˆç›¸å¯¹äºå›¾åƒé«˜åº¦ï¼‰ã€‚
                        - width (float): è¾¹ç•Œæ¡†çš„å®½åº¦ï¼ˆç›¸å¯¹äºå›¾åƒå®½åº¦ï¼‰ã€‚
                        - height (float): è¾¹ç•Œæ¡†çš„é«˜åº¦ï¼ˆç›¸å¯¹äºå›¾åƒé«˜åº¦ï¼‰ã€‚
        label_map (dict): ä¸€ä¸ªå­—å…¸ï¼Œå°†ç±»åˆ«IDï¼ˆæ•´æ•°ï¼‰æ˜ å°„åˆ°ç±»åˆ«æ ‡ç­¾ï¼ˆå­—ç¬¦ä¸²ï¼‰ã€‚

    ç¤ºä¾‹ï¼š
        >>> label_map = {0: "cat", 1: "dog", 2: "bird"}  # åº”åŒ…æ‹¬æ‰€æœ‰æ ‡æ³¨ç±»åˆ«çš„è¯¦ç»†ä¿¡æ¯
        >>> visualize_image_annotations("path/to/image.jpg", "path/to/annotations.txt", label_map)
    """
    import matplotlib.pyplot as plt

    from ultralytics.utils.plotting import colors

    img = np.array(Image.open(image_path))
    img_height, img_width = img.shape[:2]
    annotations = []
    with open(txt_path) as file:
        for line in file:
            class_id, x_center, y_center, width, height = map(float, line.split())
            x = (x_center - width / 2) * img_width
            y = (y_center - height / 2) * img_height
            w = width * img_width
            h = height * img_height
            annotations.append((x, y, w, h, int(class_id)))
    fig, ax = plt.subplots(1)  # ç»˜åˆ¶å›¾åƒå’Œæ ‡æ³¨
    for x, y, w, h, label in annotations:
        color = tuple(c / 255 for c in colors(label, True))  # è·å–å¹¶è§„èŒƒåŒ–RGBé¢œè‰²
        rect = plt.Rectangle((x, y), w, h, linewidth=2, edgecolor=color, facecolor="none")  # åˆ›å»ºçŸ©å½¢æ¡†
        ax.add_patch(rect)
        luminance = 0.2126 * color[0] + 0.7152 * color[1] + 0.0722 * color[2]  # è®¡ç®—äº®åº¦çš„å…¬å¼
        ax.text(x, y - 5, label_map[label], color="white" if luminance < 0.5 else "black", backgroundcolor=color)
    ax.imshow(img)
    plt.show()


def polygon2mask(imgsz, polygons, color=1, downsample_ratio=1):
    """
    å°†ä¸€ç»„å¤šè¾¹å½¢è½¬æ¢ä¸ºæŒ‡å®šå›¾åƒå¤§å°çš„äºŒå€¼æ©ç ã€‚

    å‚æ•°ï¼š
        imgsz (tuple): å›¾åƒçš„å¤§å°ï¼Œæ ¼å¼ä¸ºï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ã€‚
        polygons (list[np.ndarray]): å¤šè¾¹å½¢åˆ—è¡¨ã€‚æ¯ä¸ªå¤šè¾¹å½¢æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [N, M] çš„æ•°ç»„ï¼Œ
                                     å…¶ä¸­ N æ˜¯å¤šè¾¹å½¢çš„æ•°é‡ï¼ŒM æ˜¯ç‚¹çš„æ•°é‡ï¼Œä¸” M % 2 = 0ã€‚
        color (int, å¯é€‰): ç”¨äºå¡«å……å¤šè¾¹å½¢çš„é¢œè‰²å€¼ã€‚é»˜è®¤å€¼ä¸º1ã€‚
        downsample_ratio (int, å¯é€‰): ä¸‹é‡‡æ ·æ¯”ä¾‹å› å­ã€‚é»˜è®¤å€¼ä¸º1ã€‚

    è¿”å›ï¼š
        (np.ndarray): ä¸€ä¸ªäºŒå€¼æ©ç ï¼Œå¤§å°ä¸ºæŒ‡å®šå›¾åƒå°ºå¯¸ï¼Œä¸”å¤šè¾¹å½¢å·²è¢«å¡«å……ã€‚
    """
    mask = np.zeros(imgsz, dtype=np.uint8)
    polygons = np.asarray(polygons, dtype=np.int32)
    polygons = polygons.reshape((polygons.shape[0], -1, 2))
    cv2.fillPoly(mask, polygons, color=color)
    nh, nw = (imgsz[0] // downsample_ratio, imgsz[1] // downsample_ratio)
    # æ³¨æ„ï¼šå…ˆfillPolyç„¶åå†resizeï¼Œç›®çš„æ˜¯ä¿æŒä¸mask-ratio=1æ—¶ç›¸åŒçš„æŸå¤±è®¡ç®—æ–¹æ³•
    return cv2.resize(mask, (nw, nh))


def polygons2masks(imgsz, polygons, color, downsample_ratio=1):
    """
    å°†ä¸€ç»„å¤šè¾¹å½¢è½¬æ¢ä¸ºæŒ‡å®šå›¾åƒå¤§å°çš„äºŒè¿›åˆ¶æ©è†œã€‚

    å‚æ•°ï¼š
        imgsz (tuple): å›¾åƒçš„å¤§å°ï¼Œæ ¼å¼ä¸º (é«˜åº¦, å®½åº¦)ã€‚
        polygons (list[np.ndarray]): å¤šè¾¹å½¢åˆ—è¡¨ã€‚æ¯ä¸ªå¤šè¾¹å½¢æ˜¯ä¸€ä¸ªå½¢çŠ¶ä¸º [N, M] çš„æ•°ç»„ï¼Œå…¶ä¸­
                                     N æ˜¯å¤šè¾¹å½¢çš„æ•°é‡ï¼ŒM æ˜¯ç‚¹çš„æ•°é‡ï¼Œä¸” M å¿…é¡»ä¸ºå¶æ•°ã€‚
        color (int): ç”¨äºå¡«å……å¤šè¾¹å½¢çš„é¢œè‰²å€¼ã€‚
        downsample_ratio (int, å¯é€‰): æ¯ä¸ªæ©è†œçš„ä¸‹é‡‡æ ·å› å­ã€‚é»˜è®¤ä¸º 1ã€‚

    è¿”å›ï¼š
        (np.ndarray): ä¸€ä¸ªäºŒè¿›åˆ¶æ©è†œé›†åˆï¼Œå›¾åƒå¤§å°å·²æŒ‡å®šï¼Œå¹¶ä¸”å¤šè¾¹å½¢å·²å¡«å……ã€‚
    """
    return np.array([polygon2mask(imgsz, [x.reshape(-1)], color, downsample_ratio) for x in polygons])


def polygons2masks_overlap(imgsz, segments, downsample_ratio=1):
    """è¿”å›ä¸€ä¸ª (640, 640) çš„é‡å æ©è†œã€‚"""
    masks = np.zeros(
        (imgsz[0] // downsample_ratio, imgsz[1] // downsample_ratio),
        dtype=np.int32 if len(segments) > 255 else np.uint8,
    )
    areas = []
    ms = []
    for si in range(len(segments)):
        mask = polygon2mask(imgsz, [segments[si].reshape(-1)], downsample_ratio=downsample_ratio, color=1)
        ms.append(mask.astype(masks.dtype))
        areas.append(mask.sum())
    areas = np.asarray(areas)
    index = np.argsort(-areas)
    ms = np.array(ms)[index]
    for i in range(len(segments)):
        mask = ms[i] * (i + 1)
        masks = masks + mask
        masks = np.clip(masks, a_min=0, a_max=i + 1)
    return masks, index


def find_dataset_yaml(path: Path) -> Path:
    """
    æŸ¥æ‰¾å¹¶è¿”å›ä¸ Detectã€Segment æˆ– Pose æ•°æ®é›†å…³è”çš„ YAML æ–‡ä»¶ã€‚

    è¯¥å‡½æ•°é¦–å…ˆåœ¨æä¾›çš„ç›®å½•çš„æ ¹ç›®å½•æŸ¥æ‰¾ YAML æ–‡ä»¶ï¼Œå¦‚æœæœªæ‰¾åˆ°ï¼Œåˆ™è¿›è¡Œé€’å½’æœç´¢ã€‚
    å®ƒä¼˜å…ˆé€‰æ‹©ä¸æä¾›çš„è·¯å¾„å…·æœ‰ç›¸åŒåŸºåçš„ YAML æ–‡ä»¶ã€‚å¦‚æœæœªæ‰¾åˆ° YAML æ–‡ä»¶æˆ–æ‰¾åˆ°å¤šä¸ª YAML æ–‡ä»¶ï¼Œ
    å°†å¼•å‘ AssertionErrorã€‚

    å‚æ•°ï¼š
        path (Path): æŸ¥æ‰¾ YAML æ–‡ä»¶çš„ç›®å½•è·¯å¾„ã€‚

    è¿”å›ï¼š
        (Path): æ‰¾åˆ°çš„ YAML æ–‡ä»¶çš„è·¯å¾„ã€‚
    """
    files = list(path.glob("*.yaml")) or list(path.rglob("*.yaml"))  # é¦–å…ˆå°è¯•æ ¹ç›®å½•ï¼Œç„¶åé€’å½’æŸ¥æ‰¾
    assert files, f"åœ¨ '{path.resolve()}' ä¸­æœªæ‰¾åˆ° YAML æ–‡ä»¶"
    if len(files) > 1:
        files = [f for f in files if f.stem == path.stem]  # ä¼˜å…ˆé€‰æ‹©åŒ¹é…çš„ *.yaml æ–‡ä»¶
    assert len(files) == 1, f"åœ¨ '{path.resolve()}' ä¸­æœŸæœ›æ‰¾åˆ° 1 ä¸ª YAML æ–‡ä»¶ï¼Œä½†æ‰¾åˆ°äº† {len(files)} ä¸ªã€‚\n{files}"
    return files[0]


def check_det_dataset(dataset, autodownload=True):
    """
    å¦‚æœæ•°æ®é›†åœ¨æœ¬åœ°æœªæ‰¾åˆ°ï¼Œåˆ™ä¸‹è½½ã€éªŒè¯å’Œ/æˆ–è§£å‹æ•°æ®é›†ã€‚

    è¯¥å‡½æ•°æ£€æŸ¥æŒ‡å®šæ•°æ®é›†çš„å¯ç”¨æ€§ï¼Œå¦‚æœæœªæ‰¾åˆ°ï¼Œåˆ™æä¾›ä¸‹è½½å’Œè§£å‹æ•°æ®é›†çš„é€‰é¡¹ã€‚
    ç„¶åè¯»å–å¹¶è§£æé™„å¸¦çš„ YAML æ•°æ®ï¼Œç¡®ä¿æ»¡è¶³å…³é”®è¦æ±‚ï¼Œå¹¶è§£æä¸æ•°æ®é›†ç›¸å…³çš„è·¯å¾„ã€‚

    å‚æ•°ï¼š
        dataset (str): æ•°æ®é›†è·¯å¾„æˆ–æ•°æ®é›†æè¿°ç¬¦ï¼ˆå¦‚ YAML æ–‡ä»¶ï¼‰ã€‚
        autodownload (bool, å¯é€‰): å¦‚æœæœªæ‰¾åˆ°æ•°æ®é›†ï¼Œæ˜¯å¦è‡ªåŠ¨ä¸‹è½½ã€‚é»˜è®¤ä¸º Trueã€‚

    è¿”å›ï¼š
        (dict): è§£æåçš„æ•°æ®é›†ä¿¡æ¯å’Œè·¯å¾„ã€‚
    """
    file = check_file(dataset)

    # ä¸‹è½½ï¼ˆå¯é€‰ï¼‰
    extract_dir = ""
    if zipfile.is_zipfile(file) or is_tarfile(file):
        new_dir = safe_download(file, dir=DATASETS_DIR, unzip=True, delete=False)
        file = find_dataset_yaml(DATASETS_DIR / new_dir)
        extract_dir, autodownload = file.parent, False

    # è¯»å– YAML
    data = yaml_load(file, append_filename=True)  # å­—å…¸å½¢å¼

    # æ£€æŸ¥
    for k in "train", "val":
        if k not in data:
            if k != "val" or "validation" not in data:
                raise SyntaxError(
                    emojis(f"{dataset} ç¼ºå°‘ '{k}:' é”® âŒ.\næ‰€æœ‰æ•°æ® YAML æ–‡ä»¶éƒ½éœ€è¦ 'train' å’Œ 'val' é”®ã€‚")
                )
            LOGGER.info("è­¦å‘Š âš ï¸ å°†æ•°æ® YAML æ–‡ä»¶ä¸­çš„ 'validation' é”®é‡å‘½åä¸º 'val' ä»¥åŒ¹é… YOLO æ ¼å¼ã€‚")
            data["val"] = data.pop("validation")  # ç”¨ 'val' é”®æ›¿æ¢ 'validation' é”®
    if "names" not in data and "nc" not in data:
        raise SyntaxError(emojis(f"{dataset} ç¼ºå°‘ 'names' æˆ– 'nc' é”® âŒ.\næ‰€æœ‰æ•°æ® YAML æ–‡ä»¶éƒ½éœ€è¦ 'names' æˆ– 'nc' é”®ã€‚"))
    if "names" in data and "nc" in data and len(data["names"]) != data["nc"]:
        raise SyntaxError(emojis(f"{dataset} 'names' é•¿åº¦ {len(data['names'])} å’Œ 'nc: {data['nc']}' å¿…é¡»åŒ¹é…ã€‚"))
    if "names" not in data:
        data["names"] = [f"class_{i}" for i in range(data["nc"])]
    else:
        data["nc"] = len(data["names"])

    data["names"] = check_class_names(data["names"])

    # è§£æè·¯å¾„
    path = Path(extract_dir or data.get("path") or Path(data.get("yaml_file", "")).parent)  # æ•°æ®é›†æ ¹ç›®å½•
    if not path.is_absolute():
        path = (DATASETS_DIR / path).resolve()

    # è®¾ç½®è·¯å¾„
    data["path"] = path  # ä¸‹è½½è„šæœ¬
    for k in "train", "val", "test", "minival":
        if data.get(k):  # é¢„å…ˆæ·»åŠ è·¯å¾„
            if isinstance(data[k], str):
                x = (path / data[k]).resolve()
                if not x.exists() and data[k].startswith("../"):
                    x = (path / data[k][3:]).resolve()
                data[k] = str(x)
            else:
                data[k] = [str((path / x).resolve()) for x in data[k]]

    # è§£æ YAML
    val, s = (data.get(x) for x in ("val", "download"))
    if val:
        val = [Path(x).resolve() for x in (val if isinstance(val, list) else [val])]  # val è·¯å¾„
        if not all(x.exists() for x in val):
            name = clean_url(dataset)  # å»æ‰ URL èº«ä»½éªŒè¯çš„ dataset åç§°
            m = f"\næ•°æ®é›† '{name}' å›¾åƒæœªæ‰¾åˆ° âš ï¸ï¼Œç¼ºå°‘è·¯å¾„ '{[x for x in val if not x.exists()][0]}'"
            if s and autodownload:
                LOGGER.warning(m)
            else:
                m += f"\næ³¨æ„ï¼šæ•°æ®é›†ä¸‹è½½ç›®å½•ä¸º '{DATASETS_DIR}'ã€‚æ‚¨å¯ä»¥åœ¨ '{SETTINGS_FILE}' ä¸­æ›´æ–°æ­¤è·¯å¾„"
                raise FileNotFoundError(m)
            t = time.time()
            r = None  # æˆåŠŸ
            if s.startswith("http") and s.endswith(".zip"):  # URL
                safe_download(url=s, dir=DATASETS_DIR, delete=True)
            elif s.startswith("bash "):  # bash è„šæœ¬
                LOGGER.info(f"æ­£åœ¨è¿è¡Œ {s} ...")
                r = os.system(s)
            else:  # python è„šæœ¬
                exec(s, {"yaml": data})
            dt = f"({round(time.time() - t, 1)}s)"
            s = f"æˆåŠŸ âœ… {dt}, ä¿å­˜åˆ° {colorstr('bold', DATASETS_DIR)}" if r in {0, None} else f"å¤±è´¥ {dt} âŒ"
            LOGGER.info(f"æ•°æ®é›†ä¸‹è½½ {s}\n")
    check_font("Arial.ttf" if is_ascii(data["names"]) else "Arial.Unicode.ttf")  # ä¸‹è½½å­—ä½“

    return data  # å­—å…¸


def check_cls_dataset(dataset, split=""):
    """
    æ£€æŸ¥åˆ†ç±»æ•°æ®é›†ï¼Œå¦‚ Imagenetã€‚

    è¯¥å‡½æ•°æ¥å—ä¸€ä¸ª `dataset` åç§°ï¼Œå¹¶å°è¯•æ£€ç´¢ç›¸åº”çš„æ•°æ®é›†ä¿¡æ¯ã€‚
    å¦‚æœæ•°æ®é›†åœ¨æœ¬åœ°æœªæ‰¾åˆ°ï¼Œå®ƒä¼šå°è¯•ä»äº’è”ç½‘ä¸‹è½½å¹¶ä¿å­˜åˆ°æœ¬åœ°ã€‚

    å‚æ•°ï¼š
        dataset (str | Path): æ•°æ®é›†çš„åç§°ã€‚
        split (str, å¯é€‰): æ•°æ®é›†çš„æ‹†åˆ†ï¼Œå€¼å¯ä»¥æ˜¯ 'val'ã€'test' æˆ– ''ã€‚é»˜è®¤ä¸º ''ã€‚

    è¿”å›ï¼š
        (dict): ä¸€ä¸ªåŒ…å«ä»¥ä¸‹é”®çš„å­—å…¸ï¼š
            - 'train' (Path): åŒ…å«è®­ç»ƒé›†çš„ç›®å½•è·¯å¾„ã€‚
            - 'val' (Path): åŒ…å«éªŒè¯é›†çš„ç›®å½•è·¯å¾„ã€‚
            - 'test' (Path): åŒ…å«æµ‹è¯•é›†çš„ç›®å½•è·¯å¾„ã€‚
            - 'nc' (int): æ•°æ®é›†ä¸­çš„ç±»åˆ«æ•°ã€‚
            - 'names' (dict): æ•°æ®é›†ä¸­çš„ç±»åˆ«åç§°å­—å…¸ã€‚
    """
    # ä¸‹è½½ï¼ˆå¦‚æœ dataset=https://file.zip è¢«ç›´æ¥ä¼ é€’ï¼‰
    if str(dataset).startswith(("http:/", "https:/")):
        dataset = safe_download(dataset, dir=DATASETS_DIR, unzip=True, delete=False)
    elif Path(dataset).suffix in {".zip", ".tar", ".gz"}:
        file = check_file(dataset)
        dataset = safe_download(file, dir=DATASETS_DIR, unzip=True, delete=False)

    dataset = Path(dataset)
    data_dir = (dataset if dataset.is_dir() else (DATASETS_DIR / dataset)).resolve()
    if not data_dir.is_dir():
        LOGGER.warning(f"\næœªæ‰¾åˆ°æ•°æ®é›† âš ï¸ï¼Œç¼ºå°‘è·¯å¾„ {data_dir}ï¼Œæ­£åœ¨å°è¯•ä¸‹è½½...")
        t = time.time()
        if str(dataset) == "imagenet":
            subprocess.run(f"bash {ROOT / 'data/scripts/get_imagenet.sh'}", shell=True, check=True)
        else:
            url = f"https://github.com/ultralytics/assets/releases/download/v0.0.0/{dataset}.zip"
            download(url, dir=data_dir.parent)
        s = f"æ•°æ®é›†ä¸‹è½½æˆåŠŸ âœ… ({time.time() - t:.1f}s)ï¼Œä¿å­˜è‡³ {colorstr('bold', data_dir)}\n"
        LOGGER.info(s)
    train_set = data_dir / "train"
    val_set = (
        data_dir / "val"
        if (data_dir / "val").exists()
        else data_dir / "validation"
        if (data_dir / "validation").exists()
        else None
    )  # data/test æˆ– data/val
    test_set = data_dir / "test" if (data_dir / "test").exists() else None  # data/val æˆ– data/test
    if split == "val" and not val_set:
        LOGGER.warning("è­¦å‘Š âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›† 'split=val'ï¼Œæ”¹ä¸ºä½¿ç”¨ 'split=test'ã€‚")
    elif split == "test" and not test_set:
        LOGGER.warning("è­¦å‘Š âš ï¸ æœªæ‰¾åˆ°æ•°æ®é›† 'split=test'ï¼Œæ”¹ä¸ºä½¿ç”¨ 'split=val'ã€‚")

    nc = len([x for x in (data_dir / "train").glob("*") if x.is_dir()])  # ç±»åˆ«æ•°é‡
    names = [x.name for x in (data_dir / "train").iterdir() if x.is_dir()]  # ç±»åˆ«åç§°åˆ—è¡¨
    names = dict(enumerate(sorted(names)))

    # æ‰“å°åˆ°æ§åˆ¶å°
    for k, v in {"train": train_set, "val": val_set, "test": test_set}.items():
        prefix = f"{colorstr(f'{k}:')} {v}..."
        if v is None:
            LOGGER.info(prefix)
        else:
            files = [path for path in v.rglob("*.*") if path.suffix[1:].lower() in IMG_FORMATS]
            nf = len(files)  # æ–‡ä»¶æ•°é‡
            nd = len({file.parent for file in files})  # ç›®å½•æ•°é‡
            if nf == 0:
                if k == "train":
                    raise FileNotFoundError(emojis(f"{dataset} '{k}:' æ²¡æœ‰æ‰¾åˆ°è®­ç»ƒå›¾åƒ âŒ "))
                else:
                    LOGGER.warning(f"{prefix} åœ¨ {nd} ç±»ä¸­æ‰¾åˆ° {nf} å¼ å›¾åƒ: è­¦å‘Š âš ï¸ æ²¡æœ‰æ‰¾åˆ°å›¾åƒ")
            elif nd != nc:
                LOGGER.warning(f"{prefix} åœ¨ {nd} ç±»ä¸­æ‰¾åˆ° {nf} å¼ å›¾åƒ: é”™è¯¯ âŒï¸ éœ€è¦ {nc} ç±»ï¼Œè€Œä¸æ˜¯ {nd}")
            else:
                LOGGER.info(f"{prefix} åœ¨ {nd} ç±»ä¸­æ‰¾åˆ° {nf} å¼ å›¾åƒ âœ… ")

    return {"train": train_set, "val": val_set, "test": test_set, "nc": nc, "names": names}


class HUBDatasetStats:
    """
    ç”¨äºç”Ÿæˆ HUB æ•°æ®é›† JSON å’Œ `-hub` æ•°æ®é›†ç›®å½•çš„ç±»ã€‚

    å‚æ•°ï¼š
        path (str): data.yaml æˆ– data.zip çš„è·¯å¾„ï¼ˆdata.zip å†…åŒ…å« data.yamlï¼‰ã€‚é»˜è®¤ä¸º 'coco8.yaml'ã€‚
        task (str): æ•°æ®é›†ä»»åŠ¡ã€‚é€‰é¡¹æœ‰ 'detect'ã€'segment'ã€'pose'ã€'classify'ã€‚é»˜è®¤ä¸º 'detect'ã€‚
        autodownload (bool): å¦‚æœæ•°æ®é›†æœªåœ¨æœ¬åœ°æ‰¾åˆ°ï¼Œæ˜¯å¦å°è¯•ä¸‹è½½æ•°æ®é›†ã€‚é»˜è®¤ä¸º Falseã€‚

    ç¤ºä¾‹ï¼š
        ä» https://github.com/ultralytics/hub/tree/main/example_datasets ä¸‹è½½ *.zip æ–‡ä»¶
            ä¾‹å¦‚ https://github.com/ultralytics/hub/raw/main/example_datasets/coco8.zip ç”¨äº coco8.zipã€‚
        ```python
        from ultralytics.data.utils import HUBDatasetStats

        stats = HUBDatasetStats("path/to/coco8.zip", task="detect")  # detect æ•°æ®é›†
        stats = HUBDatasetStats("path/to/coco8-seg.zip", task="segment")  # segment æ•°æ®é›†
        stats = HUBDatasetStats("path/to/coco8-pose.zip", task="pose")  # pose æ•°æ®é›†
        stats = HUBDatasetStats("path/to/dota8.zip", task="obb")  # OBB æ•°æ®é›†
        stats = HUBDatasetStats("path/to/imagenet10.zip", task="classify")  # åˆ†ç±»æ•°æ®é›†

        stats.get_json(save=True)
        stats.process_images()
        ```
    """

    def __init__(self, path="coco8.yaml", task="detect", autodownload=False):
        """åˆå§‹åŒ–ç±»ã€‚"""
        path = Path(path).resolve()
        LOGGER.info(f"å¼€å§‹è¿›è¡Œ {path} çš„ HUB æ•°æ®é›†æ£€æŸ¥....")

        self.task = task  # detect, segment, pose, classify, obb
        if self.task == "classify":
            unzip_dir = unzip_file(path)
            data = check_cls_dataset(unzip_dir)
            data["path"] = unzip_dir
        else:  # detect, segment, pose, obb
            _, data_dir, yaml_path = self._unzip(Path(path))
            try:
                # è½½å…¥å¸¦æ£€æŸ¥çš„ YAML æ–‡ä»¶
                data = yaml_load(yaml_path)
                data["path"] = ""  # å»é™¤è·¯å¾„ï¼Œå› ä¸º YAML åº”è¯¥ä½äºæ•°æ®é›†æ ¹ç›®å½•
                yaml_save(yaml_path, data)
                data = check_det_dataset(yaml_path, autodownload)  # dict
                data["path"] = data_dir  # YAML è·¯å¾„åº”è®¾ç½®ä¸ºç©ºï¼ˆç›¸å¯¹è·¯å¾„ï¼‰æˆ–çˆ¶è·¯å¾„ï¼ˆç»å¯¹è·¯å¾„ï¼‰
            except Exception as e:
                raise Exception("é”™è¯¯/HUB/dataset_stats/init") from e

        self.hub_dir = Path(f"{data['path']}-hub")
        self.im_dir = self.hub_dir / "images"
        self.stats = {"nc": len(data["names"]), "names": list(data["names"].values())}  # ç»Ÿè®¡å­—å…¸
        self.data = data

    @staticmethod
    def _unzip(path):
        """è§£å‹ data.zip æ–‡ä»¶ã€‚"""
        if not str(path).endswith(".zip"):  # å¦‚æœè·¯å¾„æ˜¯ data.yaml
            return False, None, path
        unzip_dir = unzip_file(path, path=path.parent)
        assert unzip_dir.is_dir(), (
            f"è§£å‹ {path} æ—¶å‡ºé”™ï¼Œæœªæ‰¾åˆ° {unzip_dir}ã€‚path/to/abc.zip å¿…é¡»è§£å‹åˆ° path/to/abc/"
        )
        return True, str(unzip_dir), find_dataset_yaml(unzip_dir)  # è§£å‹åçš„è·¯å¾„ï¼Œæ•°æ®ç›®å½•ï¼Œyaml æ–‡ä»¶è·¯å¾„

    def _hub_ops(self, f):
        """ä¸º HUB é¢„è§ˆä¿å­˜å‹ç¼©åçš„å›¾åƒã€‚"""
        compress_one_image(f, self.im_dir / Path(f).name)  # ä¿å­˜åˆ° dataset-hub

    def get_json(self, save=False, verbose=False):
        """è¿”å›ç”¨äº Ultralytics HUB çš„æ•°æ®é›† JSONã€‚"""

        def _round(labels):
            """æ›´æ–°æ ‡ç­¾ä¸ºæ•´æ•°ç±»å’Œå››ä½å°æ•°çš„æµ®ç‚¹æ•°ã€‚"""
            if self.task == "detect":
                coordinates = labels["bboxes"]
            elif self.task in {"segment", "obb"}:  # åˆ†å‰²å’Œ OBB ä½¿ç”¨ segmentsã€‚OBB segments æ˜¯å½’ä¸€åŒ–çš„ xyxyxyxy
                coordinates = [x.flatten() for x in labels["segments"]]
            elif self.task == "pose":
                n, nk, nd = labels["keypoints"].shape
                coordinates = np.concatenate((labels["bboxes"], labels["keypoints"].reshape(n, nk * nd)), 1)
            else:
                raise ValueError(f"æœªå®šä¹‰çš„æ•°æ®é›†ä»»åŠ¡={self.task}ã€‚")
            zipped = zip(labels["cls"], coordinates)
            return [[int(c[0]), *(round(float(x), 4) for x in points)] for c, points in zipped]

        for split in "train", "val", "test":
            self.stats[split] = None  # é¢„å®šä¹‰
            path = self.data.get(split)

            # æ£€æŸ¥æ‹†åˆ†æ•°æ®é›†
            if path is None:  # å¦‚æœæ²¡æœ‰è¯¥æ‹†åˆ†
                continue
            files = [f for f in Path(path).rglob("*.*") if f.suffix[1:].lower() in IMG_FORMATS]  # è·å–æ‹†åˆ†æ•°æ®é›†ä¸­çš„å›¾åƒæ–‡ä»¶
            if not files:  # æ²¡æœ‰å›¾åƒæ–‡ä»¶
                continue

            # è·å–æ•°æ®é›†ç»Ÿè®¡ä¿¡æ¯
            if self.task == "classify":
                from torchvision.datasets import ImageFolder  # ç”¨äºåŠ é€Ÿ 'import ultralytics'

                dataset = ImageFolder(self.data[split])

                x = np.zeros(len(dataset.classes)).astype(int)
                for im in dataset.imgs:
                    x[im[1]] += 1

                self.stats[split] = {
                    "instance_stats": {"total": len(dataset), "per_class": x.tolist()},
                    "image_stats": {"total": len(dataset), "unlabelled": 0, "per_class": x.tolist()},
                    "labels": [{Path(k).name: v} for k, v in dataset.imgs],
                }
            else:
                from ultralytics.data import YOLODataset

                dataset = YOLODataset(img_path=self.data[split], data=self.data, task=self.task)
                x = np.array(
                    [
                        np.bincount(label["cls"].astype(int).flatten(), minlength=self.data["nc"])
                        for label in TQDM(dataset.labels, total=len(dataset), desc="Statistics")
                    ]
                )  # å½¢çŠ¶(128x80)
                self.stats[split] = {
                    "instance_stats": {"total": int(x.sum()), "per_class": x.sum(0).tolist()},
                    "image_stats": {
                        "total": len(dataset),
                        "unlabelled": int(np.all(x == 0, 1).sum()),
                        "per_class": (x > 0).sum(0).tolist(),
                    },
                    "labels": [{Path(k).name: _round(v)} for k, v in zip(dataset.im_files, dataset.labels)],
                }

        # ä¿å­˜ã€æ‰“å°å¹¶è¿”å›
        if save:
            self.hub_dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»º dataset-hub/
            stats_path = self.hub_dir / "stats.json"
            LOGGER.info(f"ä¿å­˜ {stats_path.resolve()}...")
            with open(stats_path, "w") as f:
                json.dump(self.stats, f)  # ä¿å­˜ stats.json
        if verbose:
            LOGGER.info(json.dumps(self.stats, indent=2, sort_keys=False))
        return self.stats

    def process_images(self):
        """ä¸º Ultralytics HUB å‹ç¼©å›¾åƒã€‚"""
        from ultralytics.data import YOLODataset  # åˆ†ç±»æ•°æ®é›†

        self.im_dir.mkdir(parents=True, exist_ok=True)  # åˆ›å»º dataset-hub/images/
        for split in "train", "val", "test":
            if self.data.get(split) is None:
                continue
            dataset = YOLODataset(img_path=self.data[split], data=self.data)
            with ThreadPool(NUM_THREADS) as pool:
                for _ in TQDM(pool.imap(self._hub_ops, dataset.im_files), total=len(dataset), desc=f"{split} images"):
                    pass
        LOGGER.info(f"å®Œæˆã€‚æ‰€æœ‰å›¾åƒå·²ä¿å­˜åˆ° {self.im_dir}")
        return self.im_dir


def compress_one_image(f, f_new=None, max_dim=1920, quality=50):
    """
    å‹ç¼©å•å¼ å›¾åƒæ–‡ä»¶ï¼Œå‡å°‘æ–‡ä»¶å¤§å°ï¼ŒåŒæ—¶ä¿æŒå…¶å®½é«˜æ¯”å’Œè´¨é‡ï¼Œä½¿ç”¨Python Imaging Library (PIL) æˆ– OpenCVåº“ã€‚
    å¦‚æœè¾“å…¥å›¾åƒå°äºæœ€å¤§å°ºå¯¸ï¼Œåˆ™ä¸ä¼šè°ƒæ•´å¤§å°ã€‚

    å‚æ•°ï¼š
        f (str): è¾“å…¥å›¾åƒæ–‡ä»¶çš„è·¯å¾„ã€‚
        f_new (str, å¯é€‰): è¾“å‡ºå›¾åƒæ–‡ä»¶çš„è·¯å¾„ã€‚å¦‚æœæœªæŒ‡å®šï¼Œåˆ™ä¼šè¦†ç›–è¾“å…¥æ–‡ä»¶ã€‚
        max_dim (int, å¯é€‰): è¾“å‡ºå›¾åƒçš„æœ€å¤§å°ºå¯¸ï¼ˆå®½åº¦æˆ–é«˜åº¦ï¼‰ã€‚é»˜è®¤å€¼ä¸º1920åƒç´ ã€‚
        quality (int, å¯é€‰): å›¾åƒå‹ç¼©è´¨é‡çš„ç™¾åˆ†æ¯”ã€‚é»˜è®¤å€¼ä¸º50%ã€‚

    ç¤ºä¾‹ï¼š
        ```python
        from pathlib import Path
        from ultralytics.data.utils import compress_one_image

        for f in Path("path/to/dataset").rglob("*.jpg"):
            compress_one_image(f)
        ```
    """
    try:  # ä½¿ç”¨PIL
        im = Image.open(f)
        r = max_dim / max(im.height, im.width)  # æ¯”ä¾‹
        if r < 1.0:  # å›¾åƒå¤ªå¤§
            im = im.resize((int(im.width * r), int(im.height * r)))
        im.save(f_new or f, "JPEG", quality=quality, optimize=True)  # ä¿å­˜
    except Exception as e:  # ä½¿ç”¨OpenCV
        LOGGER.info(f"è­¦å‘Š âš ï¸ HUBæ“ä½œPILå¤±è´¥ {f}: {e}")
        im = cv2.imread(f)
        im_height, im_width = im.shape[:2]
        r = max_dim / max(im_height, im_width)  # æ¯”ä¾‹
        if r < 1.0:  # å›¾åƒå¤ªå¤§
            im = cv2.resize(im, (int(im_width * r), int(im_height * r)), interpolation=cv2.INTER_AREA)
        cv2.imwrite(str(f_new or f), im)


def autosplit(path=DATASETS_DIR / "coco8/images", weights=(0.9, 0.1, 0.0), annotated_only=False):
    """
    è‡ªåŠ¨å°†æ•°æ®é›†æ‹†åˆ†ä¸ºè®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†ï¼Œå¹¶å°†ç»“æœä¿å­˜åˆ°autosplit_*.txtæ–‡ä»¶ä¸­ã€‚

    å‚æ•°ï¼š
        path (Path, å¯é€‰): å›¾åƒç›®å½•è·¯å¾„ã€‚é»˜è®¤å€¼ä¸ºDATASETS_DIR / 'coco8/images'ã€‚
        weights (list | tuple, å¯é€‰): è®­ç»ƒé›†ã€éªŒè¯é›†å’Œæµ‹è¯•é›†çš„æ‹†åˆ†æ¯”ä¾‹ã€‚é»˜è®¤å€¼ä¸º(0.9, 0.1, 0.0)ã€‚
        annotated_only (bool, å¯é€‰): å¦‚æœä¸ºTrueï¼Œåˆ™ä»…ä½¿ç”¨å…·æœ‰å…³è”txtæ–‡ä»¶çš„å›¾åƒã€‚é»˜è®¤å€¼ä¸ºFalseã€‚

    ç¤ºä¾‹ï¼š
        ```python
        from ultralytics.data.utils import autosplit

        autosplit()
        ```
    """
    path = Path(path)  # å›¾åƒç›®å½•
    files = sorted(x for x in path.rglob("*.*") if x.suffix[1:].lower() in IMG_FORMATS)  # ä»…å›¾åƒæ–‡ä»¶
    n = len(files)  # æ–‡ä»¶æ•°é‡
    random.seed(0)  # è®¾ç½®éšæœºç§å­ï¼Œä»¥ç¡®ä¿ç»“æœå¯å¤ç°
    indices = random.choices([0, 1, 2], weights=weights, k=n)  # å°†æ¯ä¸ªå›¾åƒåˆ†é…åˆ°ä¸€ä¸ªæ‹†åˆ†

    txt = ["autosplit_train.txt", "autosplit_val.txt", "autosplit_test.txt"]  # 3ä¸ªtxtæ–‡ä»¶
    for x in txt:
        if (path.parent / x).exists():
            (path.parent / x).unlink()  # åˆ é™¤å·²å­˜åœ¨çš„æ–‡ä»¶

    LOGGER.info(f"è‡ªåŠ¨æ‹†åˆ†å›¾åƒæ¥è‡ª {path}" + ", ä»…ä½¿ç”¨ *.txt æ ‡æ³¨çš„å›¾åƒ" * annotated_only)
    for i, img in TQDM(zip(indices, files), total=n):
        if not annotated_only or Path(img2label_paths([str(img)])[0]).exists():  # æ£€æŸ¥æ ‡ç­¾
            with open(path.parent / txt[i], "a") as f:
                f.write(f"./{img.relative_to(path.parent).as_posix()}" + "\n")  # å°†å›¾åƒæ·»åŠ åˆ°txtæ–‡ä»¶


def load_dataset_cache_file(path):
    """ä»è·¯å¾„åŠ è½½Ultralytics *.cacheå­—å…¸ã€‚"""
    import gc

    gc.disable()  # å‡å°‘pickleåŠ è½½æ—¶é—´ https://github.com/ultralytics/ultralytics/pull/1585
    cache = np.load(str(path), allow_pickle=True).item()  # åŠ è½½å­—å…¸
    gc.enable()
    return cache


def save_dataset_cache_file(prefix, path, x, version):
    """å°†Ultralyticsæ•°æ®é›†*.cacheå­—å…¸xä¿å­˜åˆ°è·¯å¾„ã€‚"""
    x["version"] = version  # æ·»åŠ ç¼“å­˜ç‰ˆæœ¬
    if is_dir_writeable(path.parent):
        if path.exists():
            path.unlink()  # å¦‚æœå­˜åœ¨ï¼Œåˆ™åˆ é™¤*.cacheæ–‡ä»¶
        np.save(str(path), x)  # ä¿å­˜ç¼“å­˜ä»¥ä¾¿ä¸‹æ¬¡ä½¿ç”¨
        path.with_suffix(".cache.npy").rename(path)  # åˆ é™¤ .npy åç¼€
        LOGGER.info(f"{prefix}å·²åˆ›å»ºæ–°ç¼“å­˜: {path}")
    else:
        LOGGER.warning(f"{prefix}è­¦å‘Š âš ï¸ ç¼“å­˜ç›®å½• {path.parent} ä¸å¯å†™ï¼Œç¼“å­˜æœªä¿å­˜ã€‚")

# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import torch
import torch.nn as nn
import torch.nn.functional as F
from scipy.optimize import linear_sum_assignment

from ultralytics.utils.metrics import bbox_iou
from ultralytics.utils.ops import xywh2xyxy, xyxy2xywh


class HungarianMatcher(nn.Module):
    """
    å®ç° HungarianMatcher çš„æ¨¡å—ï¼Œè¿™æ˜¯ä¸€ä¸ªå¯å¾®åˆ†æ¨¡å—ï¼Œç”¨äºä»¥ç«¯åˆ°ç«¯çš„æ–¹å¼è§£å†³åŒ¹é…é—®é¢˜ã€‚

    HungarianMatcher ä½¿ç”¨ä¸€ä¸ªæˆæœ¬å‡½æ•°ï¼Œåœ¨é¢„æµ‹æ¡†ä¸çœŸå®æ¡†ä¹‹é—´æ‰§è¡Œæœ€ä¼˜åŒ¹é…ï¼Œè¯¥æˆæœ¬å‡½æ•°è€ƒè™‘äº†åˆ†ç±»åˆ†æ•°ã€
    è¾¹ç•Œæ¡†åæ ‡ï¼Œä»¥åŠå¯é€‰çš„ mask é¢„æµ‹ã€‚

    å±æ€§ï¼š
        cost_gain (dict): å„ç§æˆæœ¬é¡¹çš„æƒé‡ç³»æ•°å­—å…¸ï¼ŒåŒ…æ‹¬ 'class', 'bbox', 'giou', 'mask', å’Œ 'dice'ã€‚
        use_fl (bool): æ˜¯å¦åœ¨è®¡ç®—åˆ†ç±»æˆæœ¬æ—¶ä½¿ç”¨ Focal Lossã€‚
        with_mask (bool): æ¨¡å‹æ˜¯å¦è¿›è¡Œäº†æ©ç é¢„æµ‹ã€‚
        num_sample_points (int): åœ¨è®¡ç®— mask æˆæœ¬æ—¶æ‰€ä½¿ç”¨çš„é‡‡æ ·ç‚¹æ•°é‡ã€‚
        alpha (float): Focal Loss ä¸­çš„ Î± ç³»æ•°ã€‚
        gamma (float): Focal Loss ä¸­çš„ Î³ ç³»æ•°ã€‚

    æ–¹æ³•ï¼š
        forward(pred_bboxes, pred_scores, gt_bboxes, gt_cls, gt_groups, masks=None, gt_mask=None): 
            é’ˆå¯¹ä¸€ä¸ª batch çš„å›¾åƒï¼Œè®¡ç®—é¢„æµ‹ä¸çœŸå®ä¹‹é—´çš„åŒ¹é…å…³ç³»ã€‚
        _cost_mask(bs, num_gts, masks=None, gt_mask=None): 
            å¦‚æœæ¨¡å‹é¢„æµ‹äº†æ©ç ï¼Œåˆ™è®¡ç®— mask æˆæœ¬å’Œ dice æˆæœ¬ã€‚
    """

    def __init__(self, cost_gain=None, use_fl=True, with_mask=False, num_sample_points=12544, alpha=0.25, gamma=2.0):
        """åˆå§‹åŒ– HungarianMatcher æ¨¡å—ï¼Œç”¨äºé¢„æµ‹è¾¹ç•Œæ¡†ä¸çœŸå®æ¡†ä¹‹é—´çš„æœ€ä¼˜åŒ¹é…ã€‚"""
        super().__init__()
        if cost_gain is None:
            cost_gain = {"class": 1, "bbox": 5, "giou": 2, "mask": 1, "dice": 1}
        self.cost_gain = cost_gain
        self.use_fl = use_fl
        self.with_mask = with_mask
        self.num_sample_points = num_sample_points
        self.alpha = alpha
        self.gamma = gamma

    def forward(self, pred_bboxes, pred_scores, gt_bboxes, gt_cls, gt_groups, masks=None, gt_mask=None):
        """
        HungarianMatcher çš„å‰å‘ä¼ æ’­ã€‚è¯¥å‡½æ•°æ ¹æ®é¢„æµ‹å€¼å’ŒçœŸå®æ ‡ç­¾è®¡ç®—åŒ¹é…æˆæœ¬ï¼ˆåŒ…æ‹¬åˆ†ç±»æˆæœ¬ã€
        è¾¹ç•Œæ¡†çš„ L1 è·ç¦»æˆæœ¬ã€GIoU æˆæœ¬ï¼‰ï¼Œå¹¶åŸºäºè¿™äº›æˆæœ¬æ‰¾åˆ°æœ€ä¼˜çš„åŒ¹é…å…³ç³»ã€‚

        å‚æ•°ï¼š
            pred_bboxes (Tensor): é¢„æµ‹è¾¹ç•Œæ¡†ï¼Œå½¢çŠ¶ä¸º [batch_size, num_queries, 4]ã€‚
            pred_scores (Tensor): é¢„æµ‹ç±»åˆ«åˆ†æ•°ï¼Œå½¢çŠ¶ä¸º [batch_size, num_queries, num_classes]ã€‚
            gt_cls (torch.Tensor): çœŸå®æ ‡ç­¾ç±»åˆ«ï¼Œå½¢çŠ¶ä¸º [num_gts, ]ã€‚
            gt_bboxes (torch.Tensor): çœŸå®è¾¹ç•Œæ¡†ï¼Œå½¢çŠ¶ä¸º [num_gts, 4]ã€‚
            gt_groups (List[int]): ä¸€ä¸ªåˆ—è¡¨ï¼Œé•¿åº¦ä¸º batch_sizeï¼ŒåŒ…å«æ¯å¼ å›¾åƒçš„ ground truth æ•°é‡ã€‚
            masks (Tensor, optional): é¢„æµ‹çš„æ©ç å¼ é‡ï¼Œå½¢çŠ¶ä¸º [batch_size, num_queries, height, width]ã€‚
            gt_mask (List[Tensor], optional): æ¯ä¸ªå…ƒç´ æ˜¯ [num_masks, Height, Width] çš„çœŸå®æ©ç å¼ é‡åˆ—è¡¨ã€‚

        è¿”å›ï¼š
            List[Tuple[Tensor, Tensor]]: è¿”å›ä¸€ä¸ªåˆ—è¡¨ï¼Œé•¿åº¦ç­‰äº batch å¤§å°ã€‚æ¯ä¸ªå…ƒç´ æ˜¯ä¸€ä¸ªäºŒå…ƒç»„ (index_i, index_j)ï¼Œè¡¨ç¤ºï¼š
                - index_i æ˜¯é¢„æµ‹æ¡†ä¸­è¢«é€‰ä¸­çš„ç´¢å¼•ï¼ˆæŒ‰é¡ºåºæ’åˆ—ï¼‰
                - index_j æ˜¯å¯¹åº”çš„ ground truth ç´¢å¼•ï¼ˆæŒ‰é¡ºåºæ’åˆ—ï¼‰
                å¯¹äºæ¯ä¸€ä¸ª batch çš„æ ·æœ¬ï¼Œæœ‰ï¼š
                    len(index_i) = len(index_j) = min(num_queries, num_target_boxes)
        """
        bs, nq, nc = pred_scores.shape

        if sum(gt_groups) == 0:
            return [(torch.tensor([], dtype=torch.long), torch.tensor([], dtype=torch.long)) for _ in range(bs)]

        # å±•å¹³ç”¨äºæ‰¹é‡è®¡ç®—æˆæœ¬çŸ©é˜µ
        # [batch_size * num_queries, num_classes]
        pred_scores = pred_scores.detach().view(-1, nc)
        pred_scores = F.sigmoid(pred_scores) if self.use_fl else F.softmax(pred_scores, dim=-1)
        # [batch_size * num_queries, 4]
        pred_bboxes = pred_bboxes.detach().view(-1, 4)

        # è®¡ç®—åˆ†ç±»æˆæœ¬
        pred_scores = pred_scores[:, gt_cls]
        if self.use_fl:
            neg_cost_class = (1 - self.alpha) * (pred_scores**self.gamma) * (-(1 - pred_scores + 1e-8).log())
            pos_cost_class = self.alpha * ((1 - pred_scores) ** self.gamma) * (-(pred_scores + 1e-8).log())
            cost_class = pos_cost_class - neg_cost_class
        else:
            cost_class = -pred_scores

        # è®¡ç®—è¾¹ç•Œæ¡†çš„ L1 è·ç¦»æˆæœ¬
        cost_bbox = (pred_bboxes.unsqueeze(1) - gt_bboxes.unsqueeze(0)).abs().sum(-1)  # (bs*num_queries, num_gt)

        # è®¡ç®—è¾¹ç•Œæ¡†çš„ GIoU æˆæœ¬ï¼Œå½¢çŠ¶ä¸º (bs*num_queries, num_gt)
        cost_giou = 1.0 - bbox_iou(pred_bboxes.unsqueeze(1), gt_bboxes.unsqueeze(0), xywh=True, GIoU=True).squeeze(-1)

        # æœ€ç»ˆçš„æˆæœ¬çŸ©é˜µ
        C = (
            self.cost_gain["class"] * cost_class
            + self.cost_gain["bbox"] * cost_bbox
            + self.cost_gain["giou"] * cost_giou
        )

        # è®¡ç®— mask æˆæœ¬å’Œ dice æˆæœ¬ï¼ˆå¦‚æœä½¿ç”¨äº†æ©ç ï¼‰
        if self.with_mask:
            C += self._cost_mask(bs, gt_groups, masks, gt_mask)

        # å°†æ— æ•ˆæ•°å€¼ï¼ˆNaN å’Œ infï¼‰ç½®ä¸º 0ï¼Œé¿å… ValueError: matrix contains invalid numeric entries
        C[C.isnan() | C.isinf()] = 0.0

        C = C.view(bs, nq, -1).cpu()
        indices = [linear_sum_assignment(c[i]) for i, c in enumerate(C.split(gt_groups, -1))]
        gt_groups = torch.as_tensor([0, *gt_groups[:-1]]).cumsum_(0)  # æŸ¥è¯¢å’Œ gt çš„åç§»ç´¢å¼•
        return [
            (torch.tensor(i, dtype=torch.long), torch.tensor(j, dtype=torch.long) + gt_groups[k])
            for k, (i, j) in enumerate(indices)
        ]

    # æ­¤å‡½æ•°ç”¨äºæœªæ¥æ”¯æŒ RT-DETR Segment æ¨¡å‹
    # def _cost_mask(self, bs, num_gts, masks=None, gt_mask=None):
    #     assert masks is not None and gt_mask is not None, 'ç¡®ä¿è¾“å…¥ä¸­åŒ…å« `mask` å’Œ `gt_mask`'
    #     # æ‰€æœ‰ mask å…±äº«åŒä¸€æ‰¹é‡‡æ ·ç‚¹ä»¥æé«˜åŒ¹é…æ•ˆç‡
    #     sample_points = torch.rand([bs, 1, self.num_sample_points, 2])
    #     sample_points = 2.0 * sample_points - 1.0
    #
    #     out_mask = F.grid_sample(masks.detach(), sample_points, align_corners=False).squeeze(-2)
    #     out_mask = out_mask.flatten(0, 1)
    #
    #     tgt_mask = torch.cat(gt_mask).unsqueeze(1)
    #     sample_points = torch.cat([a.repeat(b, 1, 1, 1) for a, b in zip(sample_points, num_gts) if b > 0])
    #     tgt_mask = F.grid_sample(tgt_mask, sample_points, align_corners=False).squeeze([1, 2])
    #
    #     with torch.amp.autocast("cuda", enabled=False):
    #         # äºŒå€¼äº¤å‰ç†µæˆæœ¬
    #         pos_cost_mask = F.binary_cross_entropy_with_logits(out_mask, torch.ones_like(out_mask), reduction='none')
    #         neg_cost_mask = F.binary_cross_entropy_with_logits(out_mask, torch.zeros_like(out_mask), reduction='none')
    #         cost_mask = torch.matmul(pos_cost_mask, tgt_mask.T) + torch.matmul(neg_cost_mask, 1 - tgt_mask.T)
    #         cost_mask /= self.num_sample_points
    #
    #         # Dice æˆæœ¬
    #         out_mask = F.sigmoid(out_mask)
    #         numerator = 2 * torch.matmul(out_mask, tgt_mask.T)
    #         denominator = out_mask.sum(-1, keepdim=True) + tgt_mask.sum(-1).unsqueeze(0)
    #         cost_dice = 1 - (numerator + 1) / (denominator + 1)
    #
    #         C = self.cost_gain['mask'] * cost_mask + self.cost_gain['dice'] * cost_dice
    #     return C


def get_cdn_group(
    batch, num_classes, num_queries, class_embed, num_dn=100, cls_noise_ratio=0.5, box_noise_scale=1.0, training=False
):
    """
    è·å–å¯¹æ¯”å»å™ªè®­ç»ƒç»„ã€‚æœ¬å‡½æ•°ä»ground truthä¸­åˆ›å»ºå¸¦æ­£è´Ÿæ ·æœ¬çš„å¯¹æ¯”å»å™ªè®­ç»ƒç»„ï¼Œå¯¹ç±»åˆ«æ ‡ç­¾å’Œè¾¹ç•Œæ¡†åæ ‡æ·»åŠ å™ªå£°ï¼Œ
    å¹¶è¿”å›ä¿®æ”¹åçš„æ ‡ç­¾ã€è¾¹ç•Œæ¡†ã€æ³¨æ„åŠ›æ©ç åŠå…ƒä¿¡æ¯ã€‚

    å‚æ•°ï¼š
        batch (dict): åŒ…å«ä»¥ä¸‹å†…å®¹çš„å­—å…¸ï¼š
            - 'gt_cls'ï¼šç±»åˆ«æ ‡ç­¾ (å½¢çŠ¶ä¸º[num_gts,])ï¼›
            - 'gt_bboxes'ï¼šè¾¹ç•Œæ¡†åæ ‡ (å½¢çŠ¶ä¸º[num_gts, 4])ï¼›
            - 'gt_groups'ï¼šä¸€ä¸ªé•¿åº¦ä¸ºbatch sizeçš„åˆ—è¡¨ï¼Œè¡¨ç¤ºæ¯å¼ å›¾åƒä¸­gtçš„æ•°é‡ã€‚
        num_classes (int): ç±»åˆ«æ€»æ•°ã€‚
        num_queries (int): æŸ¥è¯¢çš„æ€»æ•°ã€‚
        class_embed (torch.Tensor): å°†ç±»åˆ«æ ‡ç­¾æ˜ å°„åˆ°åµŒå…¥ç©ºé—´çš„æƒé‡ã€‚
        num_dn (int, optional): å»å™ªæ ·æœ¬æ€»æ•°ã€‚é»˜è®¤å€¼ä¸º100ã€‚
        cls_noise_ratio (float, optional): ç±»åˆ«æ ‡ç­¾çš„å™ªå£°æ¯”ä¾‹ã€‚é»˜è®¤å€¼ä¸º0.5ã€‚
        box_noise_scale (float, optional): è¾¹ç•Œæ¡†åæ ‡çš„å™ªå£°ç¼©æ”¾æ¯”ä¾‹ã€‚é»˜è®¤å€¼ä¸º1.0ã€‚
        training (bool, optional): æ˜¯å¦å¤„äºè®­ç»ƒæ¨¡å¼ã€‚é»˜è®¤å€¼ä¸ºFalseã€‚

    è¿”å›ï¼š
        (Tuple[Optional[Tensor], Optional[Tensor], Optional[Tensor], Optional[Dict]]): 
        è¿”å›å¤„ç†åçš„ç±»åˆ«åµŒå…¥ã€è¾¹ç•Œæ¡†ã€æ³¨æ„åŠ›æ©ç ä»¥åŠå…ƒä¿¡æ¯ã€‚
        è‹¥ä¸åœ¨è®­ç»ƒæ¨¡å¼æˆ–num_dn <= 0ï¼Œåˆ™æ‰€æœ‰è¿”å›é¡¹å‡ä¸ºNoneã€‚
    """
    if (not training) or num_dn <= 0:
        return None, None, None, None

    gt_groups = batch["gt_groups"]  # æ¯å¼ å›¾åƒçš„gtæ•°é‡
    total_num = sum(gt_groups)      # æ‰€æœ‰å›¾åƒä¸­gtæ€»æ•°
    max_nums = max(gt_groups)       # å½“å‰batchä¸­æœ€å¤§gtæ•°
    if max_nums == 0:
        return None, None, None, None

    num_group = num_dn // max_nums  # æ¯ç»„æœ€å¤šåŒ…å«max_numsä¸ªç›®æ ‡
    num_group = 1 if num_group == 0 else num_group  # è‡³å°‘ä¸º1ç»„

    # å°†gtå¡«å……åˆ°batchä¸­æœ€å¤§gtæ•°é‡
    bs = len(gt_groups)
    gt_cls = batch["cls"]  # æ‰€æœ‰gtçš„ç±»åˆ«æ ‡ç­¾ (bs*num, )
    gt_bbox = batch["bboxes"]  # æ‰€æœ‰gtçš„è¾¹ç•Œæ¡†åæ ‡ (bs*num, 4)
    b_idx = batch["batch_idx"]  # æ¯ä¸ªgtæ‰€å±çš„å›¾åƒç´¢å¼•

    # æ¯ç»„åŒ…å«æ­£è´Ÿæ ·æœ¬
    dn_cls = gt_cls.repeat(2 * num_group)        # é‡å¤2å€æ•°é‡ï¼šæ­£è´Ÿæ ·æœ¬
    dn_bbox = gt_bbox.repeat(2 * num_group, 1)   # åŒæ ·é‡å¤è¾¹ç•Œæ¡†
    dn_b_idx = b_idx.repeat(2 * num_group).view(-1)  # æ‰å¹³åŒ–å›¾åƒç´¢å¼•

    # è´Ÿæ ·æœ¬çš„ç´¢å¼•ï¼ˆç¬¬äºŒæ®µä¸ºè´Ÿæ ·æœ¬ï¼‰
    neg_idx = torch.arange(total_num * num_group, dtype=torch.long, device=gt_bbox.device) + num_group * total_num

    # ä¸ºç±»åˆ«æ·»åŠ å™ªå£°
    if cls_noise_ratio > 0:
        # åŠæ•°ç±»åˆ«æ ‡ç­¾åŠ å™ªå£°
        mask = torch.rand(dn_cls.shape) < (cls_noise_ratio * 0.5)
        idx = torch.nonzero(mask).squeeze(-1)
        # ç”¨éšæœºç±»åˆ«æ›¿æ¢
        new_label = torch.randint_like(idx, 0, num_classes, dtype=dn_cls.dtype, device=dn_cls.device)
        dn_cls[idx] = new_label

    # ä¸ºè¾¹ç•Œæ¡†æ·»åŠ å™ªå£°
    if box_noise_scale > 0:
        known_bbox = xywh2xyxy(dn_bbox)

        diff = (dn_bbox[..., 2:] * 0.5).repeat(1, 2) * box_noise_scale  # å™ªå£°æ¯”ä¾‹ï¼ŒåŸºäºå®½é«˜çš„ä¸€åŠ

        rand_sign = torch.randint_like(dn_bbox, 0, 2) * 2.0 - 1.0  # -1 æˆ– 1 çš„ç¬¦å·
        rand_part = torch.rand_like(dn_bbox)                      # éšæœºæ‰°åŠ¨éƒ¨åˆ†
        rand_part[neg_idx] += 1.0                                 # è´Ÿæ ·æœ¬æ‰°åŠ¨åŠ å¤§
        rand_part *= rand_sign                                    # åº”ç”¨ç¬¦å·
        known_bbox += rand_part * diff                            # åº”ç”¨æ‰°åŠ¨
        known_bbox.clip_(min=0.0, max=1.0)                         # é™åˆ¶åœ¨[0, 1]èŒƒå›´å†…
        dn_bbox = xyxy2xywh(known_bbox)
        dn_bbox = torch.logit(dn_bbox, eps=1e-6)                  # åº”ç”¨é€†sigmoidä»¥ç”¨äºåç»­è¾“å…¥ç½‘ç»œ

    # è®¡ç®—æœ€ç»ˆçš„å»å™ªæŸ¥è¯¢æ•°
    num_dn = int(max_nums * 2 * num_group)

    # ç±»åˆ«åµŒå…¥æŸ¥è¡¨
    dn_cls_embed = class_embed[dn_cls]  # æŸ¥è¯¢æ‰€æœ‰ç±»åˆ«åµŒå…¥ (bs*num*2*num_group, 256)

    # åˆå§‹åŒ–å¡«å……
    padding_cls = torch.zeros(bs, num_dn, dn_cls_embed.shape[-1], device=gt_cls.device)
    padding_bbox = torch.zeros(bs, num_dn, 4, device=gt_bbox.device)

    # æ­£æ ·æœ¬çš„ç´¢å¼•æ˜ å°„ï¼Œç”¨äºmetaä¿¡æ¯è®°å½•
    map_indices = torch.cat([torch.tensor(range(num), dtype=torch.long) for num in gt_groups])
    pos_idx = torch.stack([map_indices + max_nums * i for i in range(num_group)], dim=0)

    # å…¨éƒ¨æ ·æœ¬ï¼ˆæ­£è´Ÿï¼‰å¯¹åº”ä½ç½®çš„æ˜ å°„ç´¢å¼•
    map_indices = torch.cat([map_indices + max_nums * i for i in range(2 * num_group)])
    padding_cls[(dn_b_idx, map_indices)] = dn_cls_embed
    padding_bbox[(dn_b_idx, map_indices)] = dn_bbox

    # æ„å»ºæ³¨æ„åŠ›æ©ç 
    tgt_size = num_dn + num_queries
    attn_mask = torch.zeros([tgt_size, tgt_size], dtype=torch.bool)
    # åŒ¹é…æŸ¥è¯¢ä¸èƒ½çœ‹åˆ°é‡å»ºéƒ¨åˆ†
    attn_mask[num_dn:, :num_dn] = True
    # é‡å»ºéƒ¨åˆ†ä¸èƒ½çœ‹åˆ°å½¼æ­¤
    for i in range(num_group):
        if i == 0:
            attn_mask[max_nums * 2 * i : max_nums * 2 * (i + 1), max_nums * 2 * (i + 1) : num_dn] = True
        if i == num_group - 1:
            attn_mask[max_nums * 2 * i : max_nums * 2 * (i + 1), : max_nums * i * 2] = True
        else:
            attn_mask[max_nums * 2 * i : max_nums * 2 * (i + 1), max_nums * 2 * (i + 1) : num_dn] = True
            attn_mask[max_nums * 2 * i : max_nums * 2 * (i + 1), : max_nums * 2 * i] = True

    # æ„å»ºå…ƒä¿¡æ¯
    dn_meta = {
        "dn_pos_idx": [p.reshape(-1) for p in pos_idx.cpu().split(list(gt_groups), dim=1)],
        "dn_num_group": num_group,
        "dn_num_split": [num_dn, num_queries],
    }

    return (
        padding_cls.to(class_embed.device),
        padding_bbox.to(class_embed.device),
        attn_mask.to(class_embed.device),
        dn_meta,
    )

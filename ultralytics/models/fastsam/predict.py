# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

import torch
from PIL import Image

from ultralytics.models.yolo.segment import SegmentationPredictor
from ultralytics.utils import DEFAULT_CFG, checks
from ultralytics.utils.metrics import box_iou
from ultralytics.utils.ops import scale_masks

from .utils import adjust_bboxes_to_image_border


class FastSAMPredictor(SegmentationPredictor):
    """
    FastSAMPredictoræ˜¯ä¸“ä¸ºFast SAMï¼ˆSegment Anything Modelï¼‰åˆ†å‰²é¢„æµ‹ä»»åŠ¡åœ¨Ultralytics YOLOæ¡†æ¶ä¸­è®¾è®¡çš„ã€‚

    è¯¥ç±»æ‰©å±•äº†SegmentationPredictorï¼Œç‰¹åˆ«å®šåˆ¶äº†Fast SAMçš„é¢„æµ‹æµç¨‹ã€‚å®ƒè°ƒæ•´äº†åå¤„ç†æ­¥éª¤ï¼Œä»¥ä¾¿æ•´åˆæ©è†œé¢„æµ‹å’Œéæœ€å¤§æŠ‘åˆ¶ï¼ŒåŒæ—¶ä¼˜åŒ–å•ç±»åˆ†å‰²ã€‚
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """åˆå§‹åŒ–ä¸€ä¸ªFastSAMPredictorï¼Œç”¨äºUltralytics YOLOæ¡†æ¶ä¸­çš„Fast SAMåˆ†å‰²ä»»åŠ¡ã€‚"""
        super().__init__(cfg, overrides, _callbacks)
        self.prompts = {}

    def postprocess(self, preds, img, orig_imgs):
        """å¯¹FastSAMé¢„æµ‹ç»“æœåº”ç”¨è¾¹ç•Œæ¡†åå¤„ç†ã€‚"""
        bboxes = self.prompts.pop("bboxes", None)
        points = self.prompts.pop("points", None)
        labels = self.prompts.pop("labels", None)
        texts = self.prompts.pop("texts", None)
        results = super().postprocess(preds, img, orig_imgs)
        for result in results:
            full_box = torch.tensor(
                [0, 0, result.orig_shape[1], result.orig_shape[0]], device=preds[0].device, dtype=torch.float32
            )
            boxes = adjust_bboxes_to_image_border(result.boxes.xyxy, result.orig_shape)
            idx = torch.nonzero(box_iou(full_box[None], boxes) > 0.9).flatten()
            if idx.numel() != 0:
                result.boxes.xyxy[idx] = full_box

        return self.prompt(results, bboxes=bboxes, points=points, labels=labels, texts=texts)

    def prompt(self, results, bboxes=None, points=None, labels=None, texts=None):
        """
        åŸºäºè¾¹ç•Œæ¡†ã€ç‚¹å’Œæ©è†œçš„æç¤ºè¿›è¡Œå›¾åƒåˆ†å‰²æ¨æ–­çš„å†…éƒ¨å‡½æ•°ã€‚
        åˆ©ç”¨SAMçš„ä¸“ç”¨æ¶æ„è¿›è¡ŒåŸºäºæç¤ºçš„å®æ—¶åˆ†å‰²ã€‚

        å‚æ•°ï¼š
            results (Results | List[Results]): æ¥è‡ªFastSAMæ¨¡å‹çš„åŸå§‹æ¨æ–­ç»“æœï¼Œä¸å«ä»»ä½•æç¤ºã€‚
            bboxes (np.ndarray | List, optional): è¾¹ç•Œæ¡†ï¼Œå½¢çŠ¶ä¸º (N, 4)ï¼ŒXYXYæ ¼å¼ã€‚
            points (np.ndarray | List, optional): æŒ‡ç¤ºç‰©ä½“ä½ç½®çš„ç‚¹ï¼Œå½¢çŠ¶ä¸º (N, 2)ï¼Œå•ä½ä¸ºåƒç´ ã€‚
            labels (np.ndarray | List, optional): ç‚¹æç¤ºçš„æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º (N, )ã€‚1è¡¨ç¤ºå‰æ™¯ï¼Œ0è¡¨ç¤ºèƒŒæ™¯ã€‚
            texts (str | List[str], optional): æ–‡æœ¬æç¤ºï¼ŒåŒ…å«å­—ç¬¦ä¸²å¯¹è±¡çš„åˆ—è¡¨ã€‚

        è¿”å›ï¼š
            (List[Results]): æ ¹æ®æç¤ºç¡®å®šçš„è¾“å‡ºç»“æœã€‚
        """
        if bboxes is None and points is None and texts is None:
            return results
        prompt_results = []
        if not isinstance(results, list):
            results = [results]
        for result in results:
            if len(result) == 0:
                prompt_results.append(result)
                continue
            masks = result.masks.data
            if masks.shape[1:] != result.orig_shape:
                masks = scale_masks(masks[None], result.orig_shape)[0]
            # è¾¹ç•Œæ¡†æç¤º
            idx = torch.zeros(len(result), dtype=torch.bool, device=self.device)
            if bboxes is not None:
                bboxes = torch.as_tensor(bboxes, dtype=torch.int32, device=self.device)
                bboxes = bboxes[None] if bboxes.ndim == 1 else bboxes
                bbox_areas = (bboxes[:, 3] - bboxes[:, 1]) * (bboxes[:, 2] - bboxes[:, 0])
                mask_areas = torch.stack([masks[:, b[1] : b[3], b[0] : b[2]].sum(dim=(1, 2)) for b in bboxes])
                full_mask_areas = torch.sum(masks, dim=(1, 2))

                union = bbox_areas[:, None] + full_mask_areas - mask_areas
                idx[torch.argmax(mask_areas / union, dim=1)] = True
            if points is not None:
                points = torch.as_tensor(points, dtype=torch.int32, device=self.device)
                points = points[None] if points.ndim == 1 else points
                if labels is None:
                    labels = torch.ones(points.shape[0])
                labels = torch.as_tensor(labels, dtype=torch.int32, device=self.device)
                assert len(labels) == len(points), (
                    f"æœŸå¾…`labels`ä¸`point`å¤§å°ç›¸åŒï¼Œä½†åˆ†åˆ«ä¸º {len(labels)} å’Œ {len(points)}"
                )
                point_idx = (
                    torch.ones(len(result), dtype=torch.bool, device=self.device)
                    if labels.sum() == 0  # æ‰€æœ‰è´Ÿç‚¹
                    else torch.zeros(len(result), dtype=torch.bool, device=self.device)
                )
                for point, label in zip(points, labels):
                    point_idx[torch.nonzero(masks[:, point[1], point[0]], as_tuple=True)[0]] = bool(label)
                idx |= point_idx
            if texts is not None:
                if isinstance(texts, str):
                    texts = [texts]
                crop_ims, filter_idx = [], []
                for i, b in enumerate(result.boxes.xyxy.tolist()):
                    x1, y1, x2, y2 = (int(x) for x in b)
                    if masks[i].sum() <= 100:
                        filter_idx.append(i)
                        continue
                    crop_ims.append(Image.fromarray(result.orig_img[y1:y2, x1:x2, ::-1]))
                similarity = self._clip_inference(crop_ims, texts)
                text_idx = torch.argmax(similarity, dim=-1)  # (M, )
                if len(filter_idx):
                    text_idx += (torch.tensor(filter_idx, device=self.device)[None] <= int(text_idx)).sum(0)
                idx[text_idx] = True

            prompt_results.append(result[idx])

        return prompt_results

    def _clip_inference(self, images, texts):
        """
        CLIPæ¨æ–­è¿‡ç¨‹ã€‚

        å‚æ•°ï¼š
            images (List[PIL.Image]): ä¸€ç»„æºå›¾åƒï¼Œæ¯ä¸ªå›¾åƒåº”ä¸ºPIL.Imageç±»å‹ï¼Œä¸”ä½¿ç”¨RGBé€šé“é¡ºåºã€‚
            texts (List[str]): ä¸€ç»„æç¤ºæ–‡æœ¬ï¼Œæ¯ä¸ªæ–‡æœ¬åº”ä¸ºå­—ç¬¦ä¸²å¯¹è±¡ã€‚

        è¿”å›ï¼š
            (torch.Tensor): ç»™å®šå›¾åƒå’Œæ–‡æœ¬ä¹‹é—´çš„ç›¸ä¼¼åº¦ã€‚
        """
        try:
            import clip
        except ImportError:
            checks.check_requirements("git+https://github.com/ultralytics/CLIP.git")
            import clip
        if (not hasattr(self, "clip_model")) or (not hasattr(self, "clip_preprocess")):
            self.clip_model, self.clip_preprocess = clip.load("ViT-B/32", device=self.device)
        images = torch.stack([self.clip_preprocess(image).to(self.device) for image in images])
        tokenized_text = clip.tokenize(texts).to(self.device)
        image_features = self.clip_model.encode_image(images)
        text_features = self.clip_model.encode_text(tokenized_text)
        image_features /= image_features.norm(dim=-1, keepdim=True)  # (N, 512)
        text_features /= text_features.norm(dim=-1, keepdim=True)  # (M, 512)
        return (image_features * text_features[:, None]).sum(-1)  # (M, N)

    def set_prompts(self, prompts):
        """æå‰è®¾ç½®æç¤ºã€‚"""
        self.prompts = prompts

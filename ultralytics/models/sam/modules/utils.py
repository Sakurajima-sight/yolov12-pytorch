# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

from typing import Tuple

import torch
import torch.nn.functional as F


def select_closest_cond_frames(frame_idx, cond_frame_outputs, max_cond_frame_num):
    """
    é€‰æ‹©ç»™å®šå¸§ç´¢å¼•çš„æœ€è¿‘æ¡ä»¶å¸§ã€‚

    å‚æ•°ï¼š
        frame_idx (int): å½“å‰å¸§ç´¢å¼•ã€‚
        cond_frame_outputs (Dict[int, Any]): ä»¥å¸§ç´¢å¼•ä¸ºé”®çš„æ¡ä»¶å¸§è¾“å‡ºå­—å…¸ã€‚
        max_cond_frame_num (int): è¦é€‰æ‹©çš„æœ€å¤§æ¡ä»¶å¸§æ•°é‡ã€‚

    è¿”å›ï¼š
        (Tuple[Dict[int, Any], Dict[int, Any]]): è¿”å›ä¸€ä¸ªåŒ…å«ä¸¤ä¸ªå­—å…¸çš„å…ƒç»„ï¼š
            - selected_outputs: ä»cond_frame_outputsä¸­é€‰æ‹©çš„é¡¹ç›®ã€‚
            - unselected_outputs: æœªé€‰æ‹©çš„cond_frame_outputsä¸­çš„é¡¹ç›®ã€‚

    ç¤ºä¾‹ï¼š
        >>> frame_idx = 5
        >>> cond_frame_outputs = {1: "a", 3: "b", 7: "c", 9: "d"}
        >>> max_cond_frame_num = 2
        >>> selected, unselected = select_closest_cond_frames(frame_idx, cond_frame_outputs, max_cond_frame_num)
        >>> print(selected)
        {3: 'b', 7: 'c'}
        >>> print(unselected)
        {1: 'a', 9: 'd'}
    """
    if max_cond_frame_num == -1 or len(cond_frame_outputs) <= max_cond_frame_num:
        selected_outputs = cond_frame_outputs
        unselected_outputs = {}
    else:
        assert max_cond_frame_num >= 2, "æˆ‘ä»¬åº”è¯¥å…è®¸ä½¿ç”¨2ä¸ªæˆ–æ›´å¤šçš„æ¡ä»¶å¸§"
        selected_outputs = {}

        # é€‰æ‹©`frame_idx`ä¹‹å‰æœ€è¿‘çš„æ¡ä»¶å¸§ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        idx_before = max((t for t in cond_frame_outputs if t < frame_idx), default=None)
        if idx_before is not None:
            selected_outputs[idx_before] = cond_frame_outputs[idx_before]

        # é€‰æ‹©`frame_idx`ä¹‹åæœ€è¿‘çš„æ¡ä»¶å¸§ï¼ˆå¦‚æœæœ‰çš„è¯ï¼‰
        idx_after = min((t for t in cond_frame_outputs if t >= frame_idx), default=None)
        if idx_after is not None:
            selected_outputs[idx_after] = cond_frame_outputs[idx_after]

        # æ·»åŠ å…¶ä»–æ—¶é—´ä¸Šæœ€è¿‘çš„æ¡ä»¶å¸§ï¼Œç›´åˆ°è¾¾åˆ°`max_cond_frame_num`ä¸ªæ¡ä»¶å¸§ã€‚
        num_remain = max_cond_frame_num - len(selected_outputs)
        inds_remain = sorted(
            (t for t in cond_frame_outputs if t not in selected_outputs),
            key=lambda x: abs(x - frame_idx),
        )[:num_remain]
        selected_outputs.update((t, cond_frame_outputs[t]) for t in inds_remain)
        unselected_outputs = {t: v for t, v in cond_frame_outputs.items() if t not in selected_outputs}

    return selected_outputs, unselected_outputs


def get_1d_sine_pe(pos_inds, dim, temperature=10000):
    """ç”Ÿæˆç»™å®šä½ç½®å’Œç»´åº¦çš„1Dæ­£å¼¦ä½ç½®åµŒå…¥ã€‚"""
    pe_dim = dim // 2
    dim_t = torch.arange(pe_dim, dtype=torch.float32, device=pos_inds.device)
    dim_t = temperature ** (2 * (dim_t // 2) / pe_dim)

    pos_embed = pos_inds.unsqueeze(-1) / dim_t
    pos_embed = torch.cat([pos_embed.sin(), pos_embed.cos()], dim=-1)
    return pos_embed


def init_t_xy(end_x: int, end_y: int):
    """åˆå§‹åŒ–æŒ‡å®šç»´åº¦ç½‘æ ¼çš„1Då’Œ2Dåæ ‡å¼ é‡ã€‚"""
    t = torch.arange(end_x * end_y, dtype=torch.float32)
    t_x = (t % end_x).float()
    t_y = torch.div(t, end_x, rounding_mode="floor").float()
    return t_x, t_y


def compute_axial_cis(dim: int, end_x: int, end_y: int, theta: float = 10000.0):
    """è®¡ç®—2Dç©ºé—´ä½ç½®çš„è½´å‘å¤æŒ‡æ•°ä½ç½®ç¼–ç ï¼ˆç”¨äºç½‘æ ¼ï¼‰ã€‚"""
    freqs_x = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))
    freqs_y = 1.0 / (theta ** (torch.arange(0, dim, 4)[: (dim // 4)].float() / dim))

    t_x, t_y = init_t_xy(end_x, end_y)
    freqs_x = torch.outer(t_x, freqs_x)
    freqs_y = torch.outer(t_y, freqs_y)
    freqs_cis_x = torch.polar(torch.ones_like(freqs_x), freqs_x)
    freqs_cis_y = torch.polar(torch.ones_like(freqs_y), freqs_y)
    return torch.cat([freqs_cis_x, freqs_cis_y], dim=-1)


def reshape_for_broadcast(freqs_cis: torch.Tensor, x: torch.Tensor):
    """è°ƒæ•´é¢‘ç‡å¼ é‡çš„å½¢çŠ¶ï¼Œä»¥ä¾¿ä¸è¾“å…¥å¼ é‡è¿›è¡Œå¹¿æ’­ï¼Œç¡®ä¿ç»´åº¦å…¼å®¹ã€‚"""
    ndim = x.ndim
    assert 0 <= 1 < ndim
    assert freqs_cis.shape == (x.shape[-2], x.shape[-1])
    shape = [d if i >= ndim - 2 else 1 for i, d in enumerate(x.shape)]
    return freqs_cis.view(*shape)


def apply_rotary_enc(
    xq: torch.Tensor,
    xk: torch.Tensor,
    freqs_cis: torch.Tensor,
    repeat_freqs_k: bool = False,
):
    """åº”ç”¨æ—‹è½¬ä½ç½®ç¼–ç åˆ°æŸ¥è¯¢å’Œé”®å¼ é‡ï¼Œä½¿ç”¨å¤æ•°é¢‘ç‡æˆåˆ†ã€‚"""
    xq_ = torch.view_as_complex(xq.float().reshape(*xq.shape[:-1], -1, 2))
    xk_ = torch.view_as_complex(xk.float().reshape(*xk.shape[:-1], -1, 2)) if xk.shape[-2] != 0 else None
    freqs_cis = reshape_for_broadcast(freqs_cis, xq_)
    xq_out = torch.view_as_real(xq_ * freqs_cis).flatten(3)
    if xk_ is None:
        # æ²¡æœ‰é”®å¼ é‡ï¼Œå› è€Œä¸éœ€è¦æ—‹è½¬
        return xq_out.type_as(xq).to(xq.device), xk
    # å¦‚æœéœ€è¦ï¼Œé‡å¤é¢‘ç‡ä»¥åŒ¹é…é”®å¼ é‡çš„åºåˆ—é•¿åº¦
    if repeat_freqs_k:
        r = xk_.shape[-2] // xq_.shape[-2]
        freqs_cis = freqs_cis.repeat(*([1] * (freqs_cis.ndim - 2)), r, 1)
    xk_out = torch.view_as_real(xk_ * freqs_cis).flatten(3)
    return xq_out.type_as(xq).to(xq.device), xk_out.type_as(xk).to(xk.device)


def window_partition(x, window_size):
    """
    å°†è¾“å…¥å¼ é‡åˆ†å‰²æˆä¸é‡å çš„çª—å£ï¼Œå¦‚æœéœ€è¦çš„è¯è¿›è¡Œå¡«å……ã€‚

    å‚æ•°ï¼š
        x (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º(B, H, W, C)ã€‚
        window_size (int): æ¯ä¸ªçª—å£çš„å¤§å°ã€‚

    è¿”å›ï¼š
        (Tuple[torch.Tensor, Tuple[int, int]]): è¿”å›ä¸€ä¸ªåŒ…å«ä»¥ä¸‹å†…å®¹çš„å…ƒç»„ï¼š
            - windows (torch.Tensor): åˆ†å‰²åçš„çª—å£ï¼Œå½¢çŠ¶ä¸º(B * num_windows, window_size, window_size, C)ã€‚
            - (Hp, Wp) (Tuple[int, int]): åœ¨åˆ†å‰²ä¹‹å‰çš„å¡«å……é«˜åº¦å’Œå®½åº¦ã€‚

    ç¤ºä¾‹ï¼š
        >>> x = torch.randn(1, 16, 16, 3)
        >>> windows, (Hp, Wp) = window_partition(x, window_size=4)
        >>> print(windows.shape, Hp, Wp)
        torch.Size([16, 4, 4, 3]) 16 16
    """
    B, H, W, C = x.shape

    pad_h = (window_size - H % window_size) % window_size
    pad_w = (window_size - W % window_size) % window_size
    if pad_h > 0 or pad_w > 0:
        x = F.pad(x, (0, 0, 0, pad_w, 0, pad_h))
    Hp, Wp = H + pad_h, W + pad_w

    x = x.view(B, Hp // window_size, window_size, Wp // window_size, window_size, C)
    windows = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(-1, window_size, window_size, C)
    return windows, (Hp, Wp)


def window_unpartition(windows, window_size, pad_hw, hw):
    """
    å°†åˆ†å—çš„åºåˆ—è¿˜åŸä¸ºåŸå§‹åºåˆ—å¹¶å»é™¤å¡«å……ã€‚

    æ­¤å‡½æ•°é€†è½¬çª—å£åŒ–è¿‡ç¨‹ï¼Œé€šè¿‡å»é™¤çª—å£åŒ–è¿‡ç¨‹ä¸­æ·»åŠ çš„å¡«å……ï¼Œé‡å»ºåŸå§‹è¾“å…¥ã€‚

    å‚æ•°ï¼š
        windows (torch.Tensor): çª—å£åŒ–åºåˆ—çš„è¾“å…¥å¼ é‡ï¼Œå½¢çŠ¶ä¸º (B * num_windows, window_size,
            window_size, C)ï¼Œå…¶ä¸­ B æ˜¯æ‰¹é‡å¤§å°ï¼Œnum_windows æ˜¯çª—å£æ•°é‡ï¼Œwindow_size æ˜¯æ¯ä¸ªçª—å£çš„å¤§å°ï¼ŒC æ˜¯é€šé“æ•°ã€‚
        window_size (int): æ¯ä¸ªçª—å£çš„å¤§å°ã€‚
        pad_hw (Tuple[int, int]): è¾“å…¥åœ¨çª—å£åŒ–å‰çš„å¡«å……é«˜åº¦å’Œå®½åº¦ (Hp, Wp)ã€‚
        hw (Tuple[int, int]): è¾“å…¥åœ¨å¡«å……å’Œçª—å£åŒ–ä¹‹å‰çš„åŸå§‹é«˜åº¦å’Œå®½åº¦ (H, W)ã€‚

    è¿”å›ï¼š
        (torch.Tensor): è¿˜åŸåçš„åºåˆ—ï¼Œå½¢çŠ¶ä¸º (B, H, W, C)ï¼Œå…¶ä¸­ B æ˜¯æ‰¹é‡å¤§å°ï¼ŒH å’Œ W æ˜¯åŸå§‹é«˜åº¦å’Œå®½åº¦ï¼ŒC æ˜¯é€šé“æ•°ã€‚

    ç¤ºä¾‹ï¼š
        >>> windows = torch.rand(32, 8, 8, 64)  # 32 ä¸ªå¤§å°ä¸º 8x8ï¼Œé€šé“æ•°ä¸º 64 çš„çª—å£
        >>> pad_hw = (16, 16)  # å¡«å……åçš„é«˜åº¦å’Œå®½åº¦
        >>> hw = (15, 14)  # åŸå§‹çš„é«˜åº¦å’Œå®½åº¦
        >>> x = window_unpartition(windows, window_size=8, pad_hw=pad_hw, hw=hw)
        >>> print(x.shape)
        torch.Size([1, 15, 14, 64])
    """
    Hp, Wp = pad_hw
    H, W = hw
    B = windows.shape[0] // (Hp * Wp // window_size // window_size)
    x = windows.view(B, Hp // window_size, Wp // window_size, window_size, window_size, -1)
    x = x.permute(0, 1, 3, 2, 4, 5).contiguous().view(B, Hp, Wp, -1)

    if Hp > H or Wp > W:
        x = x[:, :H, :W, :].contiguous()
    return x


def get_rel_pos(q_size: int, k_size: int, rel_pos: torch.Tensor) -> torch.Tensor:
    """
    æ ¹æ®æŸ¥è¯¢å’Œé”®çš„å¤§å°æå–ç›¸å¯¹ä½ç½®åµŒå…¥ã€‚

    å‚æ•°ï¼š
        q_size (int): æŸ¥è¯¢çš„å¤§å°ã€‚
        k_size (int): é”®çš„å¤§å°ã€‚
        rel_pos (torch.Tensor): å½¢çŠ¶ä¸º (L, C) çš„ç›¸å¯¹ä½ç½®åµŒå…¥ï¼Œå…¶ä¸­ L æ˜¯æœ€å¤§ç›¸å¯¹è·ç¦»ï¼ŒC æ˜¯åµŒå…¥ç»´åº¦ã€‚

    è¿”å›ï¼š
        (torch.Tensor): æ ¹æ®ç›¸å¯¹ä½ç½®æå–çš„åµŒå…¥ï¼Œå½¢çŠ¶ä¸º (q_size, k_size, C)ã€‚

    ç¤ºä¾‹ï¼š
        >>> q_size, k_size = 8, 16
        >>> rel_pos = torch.randn(31, 64)  # 31 = 2 * max(8, 16) - 1
        >>> extracted_pos = get_rel_pos(q_size, k_size, rel_pos)
        >>> print(extracted_pos.shape)
        torch.Size([8, 16, 64])
    """
    max_rel_dist = int(2 * max(q_size, k_size) - 1)
    # å¦‚æœéœ€è¦ï¼Œæ’å€¼ç›¸å¯¹ä½ç½®ã€‚
    if rel_pos.shape[0] != max_rel_dist:
        # æ’å€¼ç›¸å¯¹ä½ç½®ã€‚
        rel_pos_resized = F.interpolate(
            rel_pos.reshape(1, rel_pos.shape[0], -1).permute(0, 2, 1),
            size=max_rel_dist,
            mode="linear",
        )
        rel_pos_resized = rel_pos_resized.reshape(-1, max_rel_dist).permute(1, 0)
    else:
        rel_pos_resized = rel_pos

    # å¦‚æœæŸ¥è¯¢å’Œé”®çš„å½¢çŠ¶ä¸åŒï¼Œåˆ™ç¼©æ”¾åæ ‡ã€‚
    q_coords = torch.arange(q_size)[:, None] * max(k_size / q_size, 1.0)
    k_coords = torch.arange(k_size)[None, :] * max(q_size / k_size, 1.0)
    relative_coords = (q_coords - k_coords) + (k_size - 1) * max(q_size / k_size, 1.0)

    return rel_pos_resized[relative_coords.long()]


def add_decomposed_rel_pos(
    attn: torch.Tensor,
    q: torch.Tensor,
    rel_pos_h: torch.Tensor,
    rel_pos_w: torch.Tensor,
    q_size: Tuple[int, int],
    k_size: Tuple[int, int],
) -> torch.Tensor:
    """
    å°†åˆ†è§£çš„ç›¸å¯¹ä½ç½®åµŒå…¥æ·»åŠ åˆ°æ³¨æ„åŠ›å›¾ä¸­ã€‚

    æ­¤å‡½æ•°è®¡ç®—å¹¶åº”ç”¨åˆ†è§£çš„ç›¸å¯¹ä½ç½®åµŒå…¥ï¼Œå¢å¼ºæ³¨æ„åŠ›æœºåˆ¶ï¼Œé€šè¿‡ç»“åˆæŸ¥è¯¢å’Œé”®ä½ç½®ä¹‹é—´çš„ç©ºé—´å…³ç³»ã€‚

    å‚æ•°ï¼š
        attn (torch.Tensor): æ³¨æ„åŠ›å›¾ï¼Œå½¢çŠ¶ä¸º (B, q_h * q_w, k_h * k_w)ã€‚
        q (torch.Tensor): æ³¨æ„åŠ›å±‚ä¸­çš„æŸ¥è¯¢å¼ é‡ï¼Œå½¢çŠ¶ä¸º (B, q_h * q_w, C)ã€‚
        rel_pos_h (torch.Tensor): é«˜åº¦è½´çš„ç›¸å¯¹ä½ç½®åµŒå…¥ï¼Œå½¢çŠ¶ä¸º (Lh, C)ã€‚
        rel_pos_w (torch.Tensor): å®½åº¦è½´çš„ç›¸å¯¹ä½ç½®åµŒå…¥ï¼Œå½¢çŠ¶ä¸º (Lw, C)ã€‚
        q_size (Tuple[int, int]): æŸ¥è¯¢ q çš„ç©ºé—´åºåˆ—å¤§å°ï¼Œå½¢çŠ¶ä¸º (q_h, q_w)ã€‚
        k_size (Tuple[int, int]): é”® k çš„ç©ºé—´åºåˆ—å¤§å°ï¼Œå½¢çŠ¶ä¸º (k_h, k_w)ã€‚

    è¿”å›ï¼š
        (torch.Tensor): æ·»åŠ äº†ç›¸å¯¹ä½ç½®åµŒå…¥åçš„æ³¨æ„åŠ›å›¾ï¼Œå½¢çŠ¶ä¸º (B, q_h * q_w, k_h * k_w)ã€‚

    ç¤ºä¾‹ï¼š
        >>> B, C, q_h, q_w, k_h, k_w = 1, 64, 8, 8, 8, 8
        >>> attn = torch.rand(B, q_h * q_w, k_h * k_w)
        >>> q = torch.rand(B, q_h * q_w, C)
        >>> rel_pos_h = torch.rand(2 * max(q_h, k_h) - 1, C)
        >>> rel_pos_w = torch.rand(2 * max(q_w, k_w) - 1, C)
        >>> q_size, k_size = (q_h, q_w), (k_h, k_w)
        >>> updated_attn = add_decomposed_rel_pos(attn, q, rel_pos_h, rel_pos_w, q_size, k_size)
        >>> print(updated_attn.shape)
        torch.Size([1, 64, 64])

    å‚è€ƒæ–‡çŒ®ï¼š
        https://github.com/facebookresearch/mvit/blob/main/mvit/models/attention.py
    """
    q_h, q_w = q_size
    k_h, k_w = k_size
    Rh = get_rel_pos(q_h, k_h, rel_pos_h)
    Rw = get_rel_pos(q_w, k_w, rel_pos_w)

    B, _, dim = q.shape
    r_q = q.reshape(B, q_h, q_w, dim)
    rel_h = torch.einsum("bhwc,hkc->bhwk", r_q, Rh)
    rel_w = torch.einsum("bhwc,wkc->bhwk", r_q, Rw)

    attn = (attn.view(B, q_h, q_w, k_h, k_w) + rel_h[:, :, :, :, None] + rel_w[:, :, :, None, :]).view(
        B, q_h * q_w, k_h * k_w
    )

    return attn

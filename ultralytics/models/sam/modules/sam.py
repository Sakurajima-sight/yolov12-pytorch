# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

# ç‰ˆæƒæ‰€æœ‰ (c) Meta Platforms, Inc. and å…¶å­å…¬å¸ã€‚
# ä¿ç•™æ‰€æœ‰æƒåˆ©ã€‚

# æœ¬æºä»£ç ä¾æ®æ ¹ç›®å½• LICENSE æ–‡ä»¶ä¸­çš„è®¸å¯è¯æˆæƒä½¿ç”¨ã€‚

from typing import List

import torch
import torch.nn.functional as F
from torch import nn
from torch.nn.init import trunc_normal_

from ultralytics.nn.modules import MLP

from .blocks import SAM2TwoWayTransformer
from .decoders import MaskDecoder, SAM2MaskDecoder
from .encoders import ImageEncoderViT, PromptEncoder
from .utils import get_1d_sine_pe, select_closest_cond_frames

# ä¸€ä¸ªç”¨äºå ä½ç¼ºå¤±ç›®æ ‡çš„æå°åˆ†æ•°ï¼ˆè´Ÿæ— ç©·è¿‘ä¼¼å€¼ï¼‰
NO_OBJ_SCORE = -1024.0


class SAMModel(nn.Module):
    """
    Segment Anything Model (SAM) ç”¨äºç›®æ ‡åˆ†å‰²ä»»åŠ¡ã€‚

    è¯¥ç±»ç»“åˆäº†å›¾åƒç¼–ç å™¨ã€æç¤ºç¼–ç å™¨å’Œæ©ç è§£ç å™¨ï¼Œä»å›¾åƒå’Œè¾“å…¥æç¤ºä¸­é¢„æµ‹ç›®æ ‡æ©ç ã€‚

    å±æ€§:
        mask_threshold (float): æ©ç é¢„æµ‹çš„é˜ˆå€¼ã€‚
        image_encoder (ImageEncoderViT): ç”¨äºå°†å›¾åƒç¼–ç ä¸ºåµŒå…¥çš„ä¸»å¹²ç½‘ç»œã€‚
        prompt_encoder (PromptEncoder): ç”¨äºç¼–ç å„ç§ç±»å‹çš„è¾“å…¥æç¤ºã€‚
        mask_decoder (MaskDecoder): ä»å›¾åƒå’Œæç¤ºåµŒå…¥ä¸­é¢„æµ‹ç›®æ ‡æ©ç ã€‚

    æ–¹æ³•:
        __init__: åˆå§‹åŒ– SAMModelï¼ŒåŒ…å«ç¼–ç å™¨ã€è§£ç å™¨å’Œå½’ä¸€åŒ–å‚æ•°ã€‚

    ç¤ºä¾‹:
        >>> image_encoder = ImageEncoderViT(...)
        >>> prompt_encoder = PromptEncoder(...)
        >>> mask_decoder = MaskDecoder(...)
        >>> sam_model = SAMModel(image_encoder, prompt_encoder, mask_decoder)
        >>> # è¿›ä¸€æ­¥çš„ä½¿ç”¨å–å†³äº SAMPredictor ç±»

    å¤‡æ³¨:
        æ‰€æœ‰çš„ forward() æ“ä½œéƒ½åœ¨ SAMPredictor ç±»ä¸­å®ç°ã€‚
    """

    mask_threshold: float = 0.0

    def __init__(
        self,
        image_encoder: ImageEncoderViT,
        prompt_encoder: PromptEncoder,
        mask_decoder: MaskDecoder,
        pixel_mean: List[float] = (123.675, 116.28, 103.53),
        pixel_std: List[float] = (58.395, 57.12, 57.375),
    ) -> None:
        """
        åˆå§‹åŒ– SAMModel ç±»ï¼Œä»å›¾åƒå’Œè¾“å…¥æç¤ºä¸­é¢„æµ‹ç›®æ ‡æ©ç ã€‚

        å‚æ•°:
            image_encoder (ImageEncoderViT): ç”¨äºå°†å›¾åƒç¼–ç ä¸ºå›¾åƒåµŒå…¥çš„ä¸»å¹²ç½‘ç»œã€‚
            prompt_encoder (PromptEncoder): ç¼–ç å„ç§ç±»å‹çš„è¾“å…¥æç¤ºã€‚
            mask_decoder (MaskDecoder): ä»å›¾åƒåµŒå…¥å’Œç¼–ç çš„æç¤ºä¸­é¢„æµ‹æ©ç ã€‚
            pixel_mean (List[float]): ç”¨äºå½’ä¸€åŒ–è¾“å…¥å›¾åƒçš„åƒç´ å‡å€¼ã€‚
            pixel_std (List[float]): ç”¨äºå½’ä¸€åŒ–è¾“å…¥å›¾åƒçš„åƒç´ æ ‡å‡†å·®ã€‚

        ç¤ºä¾‹:
            >>> image_encoder = ImageEncoderViT(...)
            >>> prompt_encoder = PromptEncoder(...)
            >>> mask_decoder = MaskDecoder(...)
            >>> sam_model = SAMModel(image_encoder, prompt_encoder, mask_decoder)
            >>> # è¿›ä¸€æ­¥çš„ä½¿ç”¨å–å†³äº SAMPredictor ç±»

        å¤‡æ³¨:
            æ‰€æœ‰çš„ forward() æ“ä½œéƒ½ç§»åˆ° SAMPredictor ä¸­ã€‚
        """
        super().__init__()
        self.image_encoder = image_encoder
        self.prompt_encoder = prompt_encoder
        self.mask_decoder = mask_decoder
        self.register_buffer("pixel_mean", torch.Tensor(pixel_mean).view(-1, 1, 1), False)
        self.register_buffer("pixel_std", torch.Tensor(pixel_std).view(-1, 1, 1), False)

    def set_imgsz(self, imgsz):
        """
        è®¾ç½®å›¾åƒå°ºå¯¸ï¼Œä½¿æ¨¡å‹èƒ½å¤Ÿå…¼å®¹ä¸åŒçš„å›¾åƒå°ºå¯¸ã€‚

        å‚æ•°:
            imgsz (Tuple[int, int]): è¾“å…¥å›¾åƒçš„å°ºå¯¸ã€‚
        """
        if hasattr(self.image_encoder, "set_imgsz"):
            self.image_encoder.set_imgsz(imgsz)
        self.prompt_encoder.input_image_size = imgsz
        self.prompt_encoder.image_embedding_size = [x // 16 for x in imgsz]  # 16 æ˜¯ ViT æ¨¡å‹çš„å›ºå®šè¡¥ä¸å¤§å°
        self.image_encoder.img_size = imgsz[0]


class SAM2Model(torch.nn.Module):
    """
    SAM2Model ç±»æ˜¯ Segment Anything Model 2ï¼Œå…·æœ‰åŸºäºè®°å¿†çš„è§†é¢‘ç›®æ ‡åˆ†å‰²èƒ½åŠ›ã€‚

    è¯¥ç±»æ‰©å±•äº† SAM çš„åŠŸèƒ½ï¼Œå¤„ç†è§†é¢‘åºåˆ—ï¼Œç»“åˆè®°å¿†æœºåˆ¶ä»¥ä¿è¯æ—¶é—´ä¸€è‡´æ€§ï¼Œå¹¶æœ‰æ•ˆè¿½è¸ªè·¨å¸§çš„ç›®æ ‡ã€‚

    å±æ€§:
        mask_threshold (float): æ©ç é¢„æµ‹çš„é˜ˆå€¼ã€‚
        image_encoder (ImageEncoderViT): ç”¨äºæå–å›¾åƒç‰¹å¾çš„è§†è§‰ç¼–ç å™¨ã€‚
        memory_attention (nn.Module): ç”¨äºå¤„ç†è®°å¿†ç‰¹å¾çš„æ¨¡å—ã€‚
        memory_encoder (nn.Module): ç”¨äºç”Ÿæˆè®°å¿†è¡¨ç¤ºçš„ç¼–ç å™¨ã€‚
        num_maskmem (int): å¯è®¿é—®çš„è®°å¿†å¸§çš„æ•°é‡ã€‚
        image_size (int): è¾“å…¥å›¾åƒçš„å°ºå¯¸ã€‚
        backbone_stride (int): ä¸»å¹²ç½‘ç»œè¾“å‡ºçš„æ­¥å¹…ã€‚
        sam_prompt_embed_dim (int): SAM æç¤ºåµŒå…¥çš„ç»´åº¦ã€‚
        sam_image_embedding_size (int): SAM å›¾åƒåµŒå…¥çš„å°ºå¯¸ã€‚
        sam_prompt_encoder (PromptEncoder): ç”¨äºå¤„ç†è¾“å…¥æç¤ºçš„ç¼–ç å™¨ã€‚
        sam_mask_decoder (SAM2MaskDecoder): ç”¨äºç”Ÿæˆç›®æ ‡æ©ç çš„è§£ç å™¨ã€‚
        obj_ptr_proj (nn.Module): ç”¨äºç›®æ ‡æŒ‡é’ˆçš„æŠ•å½±å±‚ã€‚
        obj_ptr_tpos_proj (nn.Module): ç”¨äºç›®æ ‡æŒ‡é’ˆä¸­æ—¶é—´ä½ç½®ç¼–ç çš„æŠ•å½±ã€‚

    æ–¹æ³•:
        forward_image: é€šè¿‡ç¼–ç å™¨å¤„ç†å›¾åƒæ‰¹æ¬¡ï¼Œæå–å¤šå±‚ç‰¹å¾ã€‚
        track_step: æ‰§è¡Œå•æ­¥è·Ÿè¸ªï¼Œæ›´æ–°ç›®æ ‡æ©ç å’Œè®°å¿†ç‰¹å¾ã€‚

    ç¤ºä¾‹:
        >>> model = SAM2Model(image_encoder, memory_attention, memory_encoder)
        >>> image_batch = torch.rand(1, 3, 512, 512)
        >>> features = model.forward_image(image_batch)
        >>> track_results = model.track_step(0, True, features, None, None, None, {})
    """

    mask_threshold: float = 0.0

    def __init__(
        self,
        image_encoder,
        memory_attention,
        memory_encoder,
        num_maskmem=7,
        image_size=512,
        backbone_stride=16,
        sigmoid_scale_for_mem_enc=1.0,
        sigmoid_bias_for_mem_enc=0.0,
        binarize_mask_from_pts_for_mem_enc=False,
        use_mask_input_as_output_without_sam=False,
        max_cond_frames_in_attn=-1,
        directly_add_no_mem_embed=False,
        use_high_res_features_in_sam=False,
        multimask_output_in_sam=False,
        multimask_min_pt_num=1,
        multimask_max_pt_num=1,
        multimask_output_for_tracking=False,
        use_multimask_token_for_obj_ptr: bool = False,
        iou_prediction_use_sigmoid=False,
        memory_temporal_stride_for_eval=1,
        non_overlap_masks_for_mem_enc=False,
        use_obj_ptrs_in_encoder=False,
        max_obj_ptrs_in_encoder=16,
        add_tpos_enc_to_obj_ptrs=True,
        proj_tpos_enc_in_obj_ptrs=False,
        use_signed_tpos_enc_to_obj_ptrs=False,
        only_obj_ptrs_in_the_past_for_eval=False,
        pred_obj_scores: bool = False,
        pred_obj_scores_mlp: bool = False,
        fixed_no_obj_ptr: bool = False,
        soft_no_obj_ptr: bool = False,
        use_mlp_for_obj_ptr_proj: bool = False,
        no_obj_embed_spatial: bool = False,
        sam_mask_decoder_extra_args=None,
        compile_image_encoder: bool = False,
    ):
        """
        åˆå§‹åŒ– SAM2Modelï¼Œç”¨äºè§†é¢‘ç›®æ ‡åˆ†å‰²å’ŒåŸºäºè®°å¿†çš„è¿½è¸ªã€‚

        å‚æ•°è¯´æ˜:
            image_encoder (nn.Module): ç”¨äºæå–å›¾åƒç‰¹å¾çš„è§†è§‰ç¼–ç å™¨ã€‚
            memory_attention (nn.Module): ç”¨äºå¯¹è®°å¿†ç‰¹å¾è¿›è¡Œæ³¨æ„åŠ›æœºåˆ¶å¤„ç†çš„æ¨¡å—ã€‚
            memory_encoder (nn.Module): ç”¨äºç”Ÿæˆè®°å¿†è¡¨ç¤ºçš„ç¼–ç å™¨ã€‚
            num_maskmem (int): å¯è®¿é—®çš„è®°å¿†å¸§æ•°é‡ï¼Œé»˜è®¤ä¸º7ï¼ˆ1ä¸ªè¾“å…¥å¸§ + 6ä¸ªå…ˆå‰å¸§ï¼‰ã€‚
            image_size (int): è¾“å…¥å›¾åƒçš„å°ºå¯¸ã€‚
            backbone_stride (int): å›¾åƒéª¨å¹²ç½‘ç»œè¾“å‡ºçš„æ­¥å¹…ã€‚
            sigmoid_scale_for_mem_enc (float): ç”¨äºæ©ç sigmoidæ¦‚ç‡çš„ç¼©æ”¾å› å­ã€‚
            sigmoid_bias_for_mem_enc (float): ç”¨äºæ©ç sigmoidæ¦‚ç‡çš„åç½®å› å­ã€‚
            binarize_mask_from_pts_for_mem_enc (bool): æ˜¯å¦åœ¨è¯„ä¼°æ—¶å¯¹äº¤äº’å¸§è¿›è¡Œæ©ç äºŒå€¼åŒ–ã€‚
            use_mask_input_as_output_without_sam (bool): æ˜¯å¦åœ¨æœ‰æ©ç è¾“å…¥çš„å¸§ä¸Šç›´æ¥è¾“å‡ºè¾“å…¥æ©ç ï¼Œè€Œä¸ä½¿ç”¨SAMæç¤ºç¼–ç å™¨å’Œæ©ç è§£ç å™¨ã€‚
            max_cond_frames_in_attn (int): å‚ä¸è®°å¿†æ³¨æ„åŠ›çš„æœ€å¤§æ¡ä»¶å¸§æ•°ï¼Œ-1è¡¨ç¤ºæ— é™åˆ¶ã€‚
            directly_add_no_mem_embed (bool): æ˜¯å¦åœ¨ç¬¬ä¸€å¸§ä¸Šç›´æ¥æ·»åŠ æ²¡æœ‰è®°å¿†çš„åµŒå…¥ã€‚
            use_high_res_features_in_sam (bool): æ˜¯å¦åœ¨SAMæ©ç è§£ç å™¨ä¸­ä½¿ç”¨é«˜åˆ†è¾¨ç‡ç‰¹å¾å›¾ã€‚
            multimask_output_in_sam (bool): æ˜¯å¦ä¸ºåˆå§‹æ¡ä»¶å¸§çš„ç¬¬ä¸€æ¬¡ç‚¹å‡»è¾“å‡ºå¤šä¸ªï¼ˆ3ä¸ªï¼‰æ©ç ã€‚
            multimask_min_pt_num (int): ä½¿ç”¨å¤šæ©ç è¾“å‡ºæ—¶æœ€å°çš„ç‚¹å‡»æ¬¡æ•°ã€‚
            multimask_max_pt_num (int): ä½¿ç”¨å¤šæ©ç è¾“å‡ºæ—¶æœ€å¤§çš„ç‚¹å‡»æ¬¡æ•°ã€‚
            multimask_output_for_tracking (bool): æ˜¯å¦ä½¿ç”¨å¤šæ©ç è¾“å‡ºè¿›è¡Œè¿½è¸ªã€‚
            use_multimask_token_for_obj_ptr (bool): æ˜¯å¦ä½¿ç”¨å¤šæ©ç tokenä½œä¸ºå¯¹è±¡æŒ‡é’ˆã€‚
            iou_prediction_use_sigmoid (bool): æ˜¯å¦ä½¿ç”¨sigmoidé™åˆ¶IoUé¢„æµ‹åœ¨[0, 1]èŒƒå›´å†…ã€‚
            memory_temporal_stride_for_eval (int): åœ¨è¯„ä¼°æœŸé—´è®°å¿†åº“çš„æ—¶é—´æ­¥å¹…ã€‚
            non_overlap_masks_for_mem_enc (bool): åœ¨è¯„ä¼°æ—¶æ˜¯å¦å¯¹è®°å¿†ç¼–ç å™¨ä¸­çš„å¯¹è±¡æ©ç åº”ç”¨ä¸é‡å çº¦æŸã€‚
            use_obj_ptrs_in_encoder (bool): æ˜¯å¦åœ¨ç¼–ç å™¨ä¸­äº¤å‰å…³æ³¨æ¥è‡ªå…¶ä»–å¸§çš„å¯¹è±¡æŒ‡é’ˆã€‚
            max_obj_ptrs_in_encoder (int): åœ¨ç¼–ç å™¨çš„äº¤å‰æ³¨æ„åŠ›ä¸­ï¼Œæ¥è‡ªå…¶ä»–å¸§çš„æœ€å¤§å¯¹è±¡æŒ‡é’ˆæ•°é‡ã€‚
            add_tpos_enc_to_obj_ptrs (bool): æ˜¯å¦åœ¨ç¼–ç å™¨ä¸­ä¸ºå¯¹è±¡æŒ‡é’ˆæ·»åŠ æ—¶é—´ä½ç½®ç¼–ç ã€‚
            proj_tpos_enc_in_obj_ptrs (bool): æ˜¯å¦ä¸ºå¯¹è±¡æŒ‡é’ˆä¸­çš„æ—¶é—´ä½ç½®ç¼–ç æ·»åŠ é¢å¤–çš„çº¿æ€§æŠ•å½±å±‚ã€‚
            use_signed_tpos_enc_to_obj_ptrs (bool): æ˜¯å¦åœ¨å¯¹è±¡æŒ‡é’ˆä¸­çš„æ—¶é—´ä½ç½®ç¼–ç ä¸­ä½¿ç”¨ç¬¦å·è·ç¦»ï¼ˆè€Œä¸æ˜¯æ— ç¬¦å·çš„ç»å¯¹è·ç¦»ï¼‰ï¼Œä»…åœ¨`use_obj_ptrs_in_encoder=True`å’Œ`add_tpos_enc_to_obj_ptrs=True`æ—¶ç›¸å…³ã€‚
            only_obj_ptrs_in_the_past_for_eval (bool): åœ¨è¯„ä¼°æ—¶æ˜¯å¦åªå…³æ³¨è¿‡å»çš„å¯¹è±¡æŒ‡é’ˆã€‚
            pred_obj_scores (bool): æ˜¯å¦é¢„æµ‹å¸§ä¸­æ˜¯å¦æœ‰å¯¹è±¡ã€‚
            pred_obj_scores_mlp (bool): æ˜¯å¦ä½¿ç”¨MLPæ¥é¢„æµ‹å¯¹è±¡å¾—åˆ†ã€‚
            fixed_no_obj_ptr (bool): æ˜¯å¦åœ¨æ²¡æœ‰å¯¹è±¡å­˜åœ¨æ—¶ä½¿ç”¨å›ºå®šçš„æ— å¯¹è±¡æŒ‡é’ˆã€‚
            soft_no_obj_ptr (bool): æ˜¯å¦é€šè¿‡è½¯æ–¹å¼æ··åˆæ— å¯¹è±¡æŒ‡é’ˆï¼Œä»¥ä¾¿æ›´å®¹æ˜“æ¢å¤å’Œé”™è¯¯ç¼“è§£ã€‚
            use_mlp_for_obj_ptr_proj (bool): æ˜¯å¦ä½¿ç”¨MLPè¿›è¡Œå¯¹è±¡æŒ‡é’ˆçš„æŠ•å½±ã€‚
            no_obj_embed_spatial (bool): æ˜¯å¦åœ¨ç©ºé—´å¸§ä¸­æ·»åŠ æ— å¯¹è±¡åµŒå…¥ã€‚
            sam_mask_decoder_extra_args (Dict | None): ç”¨äºæ„å»ºSAMæ©ç è§£ç å™¨çš„é¢å¤–å‚æ•°ã€‚
            compile_image_encoder (bool): æ˜¯å¦ç¼–è¯‘å›¾åƒç¼–ç å™¨ä»¥æé«˜æ¨ç†é€Ÿåº¦ã€‚

        ç¤ºä¾‹ç”¨æ³•:
            >>> image_encoder = ImageEncoderViT(...)
            >>> memory_attention = SAM2TwoWayTransformer(...)
            >>> memory_encoder = nn.Sequential(...)
            >>> model = SAM2Model(image_encoder, memory_attention, memory_encoder)
            >>> image_batch = torch.rand(1, 3, 512, 512)
            >>> features = model.forward_image(image_batch)
            >>> track_results = model.track_step(0, True, features, None, None, None, {})
        """
        super().__init__()

        # éƒ¨åˆ† 1: å›¾åƒéª¨å¹²ç½‘ç»œ
        self.image_encoder = image_encoder
        # å¦‚æœä½¿ç”¨é«˜åˆ†è¾¨ç‡è®¾ç½®ï¼Œä½¿ç”¨level 0, 1, 2ï¼Œå¦åˆ™é»˜è®¤ä½¿ç”¨level 2
        self.use_high_res_features_in_sam = use_high_res_features_in_sam
        self.num_feature_levels = 3 if use_high_res_features_in_sam else 1
        self.use_obj_ptrs_in_encoder = use_obj_ptrs_in_encoder
        self.max_obj_ptrs_in_encoder = max_obj_ptrs_in_encoder
        if use_obj_ptrs_in_encoder:
            # ä¸€ä¸ªå·ç§¯å±‚ï¼Œç”¨äºå°†æ©ç æç¤ºä¸‹é‡‡æ ·åˆ°æ­¥å¹…4ï¼ˆä¸ä½åˆ†è¾¨ç‡SAMæ©ç logitsç›¸åŒï¼‰ï¼Œ
            # å¹¶å°†å…¶å°ºåº¦ä»0~1è½¬æ¢ä¸ºSAMçš„logitå°ºåº¦ï¼Œä»¥ä¾¿è¾“å…¥SAMæ©ç è§£ç å™¨ç”ŸæˆæŒ‡é’ˆã€‚
            self.mask_downsample = torch.nn.Conv2d(1, 1, kernel_size=4, stride=4)
        self.add_tpos_enc_to_obj_ptrs = add_tpos_enc_to_obj_ptrs
        if proj_tpos_enc_in_obj_ptrs:
            assert add_tpos_enc_to_obj_ptrs  # è¿™ä¸¤ä¸ªé€‰é¡¹å¿…é¡»ä¸€èµ·ä½¿ç”¨
        self.proj_tpos_enc_in_obj_ptrs = proj_tpos_enc_in_obj_ptrs
        self.use_signed_tpos_enc_to_obj_ptrs = use_signed_tpos_enc_to_obj_ptrs
        self.only_obj_ptrs_in_the_past_for_eval = only_obj_ptrs_in_the_past_for_eval

        # éƒ¨åˆ† 2: è®°å¿†æ³¨æ„åŠ›ï¼Œç”¨äºå°†å½“å‰å¸§çš„è§†è§‰ç‰¹å¾ä¸æ¥è‡ªå…ˆå‰å¸§çš„è®°å¿†ï¼ˆå’Œå¯¹è±¡æŒ‡é’ˆï¼‰è¿›è¡Œæ¡ä»¶å¤„ç†
        self.memory_attention = memory_attention
        self.hidden_dim = memory_attention.d_model

        # éƒ¨åˆ† 3: è®°å¿†ç¼–ç å™¨ï¼Œç”¨äºå¤„ç†å…ˆå‰å¸§çš„è¾“å‡º
        self.memory_encoder = memory_encoder
        self.mem_dim = self.hidden_dim
        if hasattr(self.memory_encoder, "out_proj") and hasattr(self.memory_encoder.out_proj, "weight"):
            # å¦‚æœè®°å¿†åœ¨é€šé“ç»´åº¦ä¸Šè¢«å‹ç¼©
            self.mem_dim = self.memory_encoder.out_proj.weight.shape[0]
        self.num_maskmem = num_maskmem  # å¯è®¿é—®çš„è®°å¿†æ•°é‡
        # è®°å¿†çš„æ—¶é—´ç¼–ç 
        self.maskmem_tpos_enc = torch.nn.Parameter(torch.zeros(num_maskmem, 1, 1, self.mem_dim))
        trunc_normal_(self.maskmem_tpos_enc, std=0.02)
        # ç”¨äºè¡¨ç¤ºæ²¡æœ‰æ¥è‡ªå…ˆå‰å¸§çš„è®°å¿†åµŒå…¥çš„å•ä¸ªtoken
        self.no_mem_embed = torch.nn.Parameter(torch.zeros(1, 1, self.hidden_dim))
        self.no_mem_pos_enc = torch.nn.Parameter(torch.zeros(1, 1, self.hidden_dim))
        trunc_normal_(self.no_mem_embed, std=0.02)
        trunc_normal_(self.no_mem_pos_enc, std=0.02)
        self.directly_add_no_mem_embed = directly_add_no_mem_embed
        # åœ¨å°†æ©ç ä¼ å…¥è®°å¿†ç¼–ç å™¨ä¹‹å‰ï¼Œæ˜¯å¦åº”ç”¨sigmoidå°†æ©ç çš„åŸå§‹logitsè½¬åŒ–ä¸ºï¼ˆ0, 1ï¼‰èŒƒå›´
        self.sigmoid_scale_for_mem_enc = sigmoid_scale_for_mem_enc
        self.sigmoid_bias_for_mem_enc = sigmoid_bias_for_mem_enc
        self.binarize_mask_from_pts_for_mem_enc = binarize_mask_from_pts_for_mem_enc
        self.non_overlap_masks_for_mem_enc = non_overlap_masks_for_mem_enc
        self.memory_temporal_stride_for_eval = memory_temporal_stride_for_eval
        # åœ¨æœ‰æ©ç è¾“å…¥çš„å¸§ä¸Šï¼Œæ˜¯å¦ç›´æ¥è¾“å‡ºè¾“å…¥æ©ç ï¼Œè€Œä¸ä½¿ç”¨SAMæç¤ºç¼–ç å™¨+æ©ç è§£ç å™¨
        self.use_mask_input_as_output_without_sam = use_mask_input_as_output_without_sam
        self.multimask_output_in_sam = multimask_output_in_sam
        self.multimask_min_pt_num = multimask_min_pt_num
        self.multimask_max_pt_num = multimask_max_pt_num
        self.multimask_output_for_tracking = multimask_output_for_tracking
        self.use_multimask_token_for_obj_ptr = use_multimask_token_for_obj_ptr
        self.iou_prediction_use_sigmoid = iou_prediction_use_sigmoid

        # éƒ¨åˆ† 4: SAM é£æ ¼çš„æç¤ºç¼–ç å™¨ï¼ˆç”¨äºæ©ç å’Œç‚¹è¾“å…¥ï¼‰ï¼Œ
        # ä»¥åŠSAMé£æ ¼çš„æ©ç è§£ç å™¨ç”¨äºæœ€ç»ˆçš„æ©ç è¾“å‡º
        self.image_size = image_size
        self.backbone_stride = backbone_stride
        self.sam_mask_decoder_extra_args = sam_mask_decoder_extra_args
        self.pred_obj_scores = pred_obj_scores
        self.pred_obj_scores_mlp = pred_obj_scores_mlp
        self.fixed_no_obj_ptr = fixed_no_obj_ptr
        self.soft_no_obj_ptr = soft_no_obj_ptr
        if self.fixed_no_obj_ptr:
            assert self.pred_obj_scores
            assert self.use_obj_ptrs_in_encoder
        if self.pred_obj_scores and self.use_obj_ptrs_in_encoder:
            self.no_obj_ptr = torch.nn.Parameter(torch.zeros(1, self.hidden_dim))
            trunc_normal_(self.no_obj_ptr, std=0.02)
        self.use_mlp_for_obj_ptr_proj = use_mlp_for_obj_ptr_proj
        self.no_obj_embed_spatial = None
        if no_obj_embed_spatial:
            self.no_obj_embed_spatial = torch.nn.Parameter(torch.zeros(1, self.mem_dim))
            trunc_normal_(self.no_obj_embed_spatial, std=0.02)

        self._build_sam_heads()
        self.max_cond_frames_in_attn = max_cond_frames_in_attn

        # æ¨¡å‹ç¼–è¯‘
        if compile_image_encoder:
            # ç¼–è¯‘å‰å‘å‡½æ•°ï¼ˆä¸æ˜¯å®Œæ•´æ¨¡å—ï¼‰ï¼Œä»¥å…è®¸åŠ è½½æ£€æŸ¥ç‚¹ã€‚
            print("å›¾åƒç¼–ç å™¨ç¼–è¯‘å·²å¯ç”¨ã€‚ç¬¬ä¸€æ¬¡å‰å‘ä¼ æ’­å°†è¾ƒæ…¢ã€‚")
            self.image_encoder.forward = torch.compile(
                self.image_encoder.forward,
                mode="max-autotune",
                fullgraph=True,
                dynamic=False,
            )

    @property
    def device(self):
        """è¿”å›æ¨¡å‹å‚æ•°æ‰€åœ¨çš„è®¾å¤‡ã€‚"""
        return next(self.parameters()).device

    def forward(self, *args, **kwargs):
        """å¤„ç†å›¾åƒå’Œæç¤ºè¾“å…¥ï¼Œä»¥åœ¨è§†é¢‘åºåˆ—ä¸­ç”Ÿæˆç›®æ ‡æ©ç å’Œå¾—åˆ†ã€‚"""
        raise NotImplementedError(
            "è¯·ä½¿ç”¨ SAM2VideoPredictor ä¸­çš„ç›¸åº”æ–¹æ³•è¿›è¡Œæ¨ç†ã€‚"
            "ç¤ºä¾‹è¯·å‚è€ƒ notebooks/video_predictor_example.ipynbã€‚"
        )
1
def _build_sam_heads(self):
    """æ„å»º SAM é£æ ¼çš„æç¤ºç¼–ç å™¨å’Œæ©ç è§£ç å™¨ï¼Œç”¨äºå›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚"""
    self.sam_prompt_embed_dim = self.hidden_dim
    self.sam_image_embedding_size = self.image_size // self.backbone_stride

    # ä» SAM æ„å»º PromptEncoder å’Œ MaskDecoderï¼ˆåƒ `mask_in_chans=16` ç­‰è¶…å‚æ•°æ¥è‡ª SAM ä»£ç ï¼‰
    self.sam_prompt_encoder = PromptEncoder(
        embed_dim=self.sam_prompt_embed_dim,
        image_embedding_size=(
            self.sam_image_embedding_size,
            self.sam_image_embedding_size,
        ),
        input_image_size=(self.image_size, self.image_size),
        mask_in_chans=16,
    )
    self.sam_mask_decoder = SAM2MaskDecoder(
        num_multimask_outputs=3,
        transformer=SAM2TwoWayTransformer(
            depth=2,
            embedding_dim=self.sam_prompt_embed_dim,
            mlp_dim=2048,
            num_heads=8,
        ),
        transformer_dim=self.sam_prompt_embed_dim,
        iou_head_depth=3,
        iou_head_hidden_dim=256,
        use_high_res_features=self.use_high_res_features_in_sam,
        iou_prediction_use_sigmoid=self.iou_prediction_use_sigmoid,
        pred_obj_scores=self.pred_obj_scores,
        pred_obj_scores_mlp=self.pred_obj_scores_mlp,
        use_multimask_token_for_obj_ptr=self.use_multimask_token_for_obj_ptr,
        **(self.sam_mask_decoder_extra_args or {}),
    )
    if self.use_obj_ptrs_in_encoder:
        # åœ¨ SAM è¾“å‡º token ä¸Šè¿›è¡Œçº¿æ€§æŠ•å½±ï¼Œå°†å…¶è½¬æ¢ä¸ºå¯¹è±¡æŒ‡é’ˆ
        self.obj_ptr_proj = torch.nn.Linear(self.hidden_dim, self.hidden_dim)
        if self.use_mlp_for_obj_ptr_proj:
            self.obj_ptr_proj = MLP(self.hidden_dim, self.hidden_dim, self.hidden_dim, 3)
    else:
        self.obj_ptr_proj = torch.nn.Identity()
    if self.proj_tpos_enc_in_obj_ptrs:
        # å¯¹å¯¹è±¡æŒ‡é’ˆä¸­çš„æ—¶é—´ä½ç½®ç¼–ç è¿›è¡Œçº¿æ€§æŠ•å½±ï¼Œä»¥é¿å…ä¸ç©ºé—´ä½ç½®ç¼–ç çš„æ½œåœ¨å¹²æ‰°
        self.obj_ptr_tpos_proj = torch.nn.Linear(self.hidden_dim, self.mem_dim)
    else:
        self.obj_ptr_tpos_proj = torch.nn.Identity()

    def _forward_sam_heads(
        self,
        backbone_features,
        point_inputs=None,
        mask_inputs=None,
        high_res_features=None,
        multimask_output=False,
    ):
        """
        é€šè¿‡ SAM æç¤ºç¼–ç å™¨å’Œæ©ç å¤´è¿›è¡Œå‰å‘ä¼ æ’­ã€‚

        è¯¥æ–¹æ³•å¤„ç†å›¾åƒç‰¹å¾å’Œå¯é€‰çš„ç‚¹/æ©ç è¾“å…¥ï¼Œä»¥ç”Ÿæˆå¯¹è±¡æ©ç å’Œå¾—åˆ†ã€‚

        å‚æ•°:
            backbone_features (torch.Tensor): å›¾åƒç‰¹å¾ï¼Œå½¢çŠ¶ä¸º (B, C, H, W)ã€‚
            point_inputs (Dict[str, torch.Tensor] | None): åŒ…å«ç‚¹æç¤ºçš„å­—å…¸ã€‚
                'point_coords': å½¢çŠ¶ä¸º (B, P, 2) çš„å¼ é‡ï¼Œæ•°æ®ç±»å‹ä¸º float32ï¼ŒåŒ…å« P ä¸ªè¾“å…¥ç‚¹çš„ç»å¯¹åƒç´ åæ ‡ (x, y)ã€‚
                'point_labels': å½¢çŠ¶ä¸º (B, P) çš„å¼ é‡ï¼Œæ•°æ®ç±»å‹ä¸º int32ï¼Œ1 è¡¨ç¤ºæ­£å‘ç‚¹å‡»ï¼Œ0 è¡¨ç¤ºè´Ÿå‘ç‚¹å‡»ï¼Œ-1 è¡¨ç¤ºå¡«å……ã€‚
            mask_inputs (torch.Tensor | None): æ©ç ï¼Œå½¢çŠ¶ä¸º (B, 1, H*16, W*16)ï¼Œæ•°æ®ç±»å‹ä¸º float æˆ– boolï¼Œç©ºé—´å¤§å°ä¸å›¾åƒç›¸åŒã€‚
            high_res_features (List[torch.Tensor] | None): åŒ…å«ä¸¤ä¸ªç‰¹å¾å›¾çš„åˆ—è¡¨ï¼Œå½¢çŠ¶åˆ†åˆ«ä¸º (B, C, 4*H, 4*W) å’Œ (B, C, 2*H, 2*W)ï¼Œç”¨äº SAM è§£ç å™¨çš„é«˜åˆ†è¾¨ç‡ç‰¹å¾å›¾ã€‚
            multimask_output (bool): å¦‚æœä¸º Trueï¼Œè¾“å‡º 3 ä¸ªå€™é€‰æ©ç åŠå…¶ IoU ä¼°è®¡ï¼›å¦‚æœä¸º Falseï¼Œåªè¾“å‡º 1 ä¸ªæ©ç åŠå…¶ IoU ä¼°è®¡ã€‚

        è¿”å›:
            (Tuple[torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor, torch.Tensor]):
                low_res_multimasks: å½¢çŠ¶ä¸º (B, M, H*4, W*4) çš„å¼ é‡ï¼ŒåŒ…å« SAM è¾“å‡ºçš„æ©ç  logitsã€‚
                high_res_multimasks: å½¢çŠ¶ä¸º (B, M, H*16, W*16) çš„å¼ é‡ï¼ŒåŒ…å«ä¸Šé‡‡æ ·åçš„æ©ç  logitsã€‚
                ious: å½¢çŠ¶ä¸º (B, M) çš„å¼ é‡ï¼ŒåŒ…å«æ¯ä¸ªè¾“å‡ºæ©ç çš„ä¼°è®¡ IoUã€‚
                low_res_masks: å½¢çŠ¶ä¸º (B, 1, H*4, W*4) çš„å¼ é‡ï¼ŒåŒ…å«æœ€ä½³ä½åˆ†è¾¨ç‡æ©ç ã€‚
                high_res_masks: å½¢çŠ¶ä¸º (B, 1, H*16, W*16) çš„å¼ é‡ï¼ŒåŒ…å«æœ€ä½³é«˜åˆ†è¾¨ç‡æ©ç ã€‚
                obj_ptr: å½¢çŠ¶ä¸º (B, C) çš„å¼ é‡ï¼ŒåŒ…å«è¾“å‡ºæ©ç çš„å¯¹è±¡æŒ‡é’ˆå‘é‡ã€‚
                object_score_logits: å½¢çŠ¶ä¸º (B) çš„å¼ é‡ï¼ŒåŒ…å«å¯¹è±¡å¾—åˆ†çš„ logitsã€‚

                å…¶ä¸­ M ä¸º 3ï¼ˆå¦‚æœ multimask_output=Trueï¼‰ï¼Œä¸º 1ï¼ˆå¦‚æœ multimask_output=Falseï¼‰ã€‚

        ç¤ºä¾‹:
            >>> backbone_features = torch.rand(1, 256, 32, 32)
            >>> point_inputs = {"point_coords": torch.rand(1, 2, 2), "point_labels": torch.tensor([[1, 0]])}
            >>> mask_inputs = torch.rand(1, 1, 512, 512)
            >>> results = model._forward_sam_heads(backbone_features, point_inputs, mask_inputs)
            >>> (
            ...     low_res_multimasks,
            ...     high_res_multimasks,
            ...     ious,
            ...     low_res_masks,
            ...     high_res_masks,
            ...     obj_ptr,
            ...     object_score_logits,
            ... ) = results
        """
        B = backbone_features.size(0)
        device = backbone_features.device
        assert backbone_features.size(1) == self.sam_prompt_embed_dim
        assert backbone_features.size(2) == self.sam_image_embedding_size
        assert backbone_features.size(3) == self.sam_image_embedding_size

        # a) å¤„ç†ç‚¹æç¤º
        if point_inputs is not None:
            sam_point_coords = point_inputs["point_coords"]
            sam_point_labels = point_inputs["point_labels"]
            assert sam_point_coords.size(0) == B and sam_point_labels.size(0) == B
        else:
            # å¦‚æœæ²¡æœ‰æä¾›ç‚¹æç¤ºï¼Œåˆ™ç”¨ç©ºç‚¹è¿›è¡Œå¡«å……ï¼ˆæ ‡ç­¾ä¸º -1ï¼‰
            sam_point_coords = torch.zeros(B, 1, 2, device=device)
            sam_point_labels = -torch.ones(B, 1, dtype=torch.int32, device=device)

        # b) å¤„ç†æ©ç æç¤º
        if mask_inputs is not None:
            # å¦‚æœæä¾›äº†æ©ç è¾“å…¥ï¼Œå°†å…¶ä¸‹é‡‡æ ·ä¸ºä½åˆ†è¾¨ç‡æ©ç è¾“å…¥ï¼Œå¿…è¦æ—¶å°†å…¶ä½œä¸ºå¯†é›†æ©ç æç¤ºè¾“å…¥åˆ° SAM æ©ç ç¼–ç å™¨
            assert len(mask_inputs.shape) == 4 and mask_inputs.shape[:2] == (B, 1)
            if mask_inputs.shape[-2:] != self.sam_prompt_encoder.mask_input_size:
                sam_mask_prompt = F.interpolate(
                    mask_inputs.float(),
                    size=self.sam_prompt_encoder.mask_input_size,
                    align_corners=False,
                    mode="bilinear",
                    antialias=True,  # ä½¿ç”¨æŠ—é”¯é½¿è¿›è¡Œä¸‹é‡‡æ ·
                )
            else:
                sam_mask_prompt = mask_inputs
        else:
            # å¦åˆ™ï¼Œç›´æ¥è¾“å…¥ Noneï¼ˆSAM çš„æç¤ºç¼–ç å™¨ä¼šæ·»åŠ ä¸€ä¸ªå­¦ä¹ åˆ°çš„ `no_mask_embed` æ¥è¡¨ç¤ºæ²¡æœ‰æ©ç è¾“å…¥çš„æƒ…å†µï¼‰ã€‚
            sam_mask_prompt = None

        sparse_embeddings, dense_embeddings = self.sam_prompt_encoder(
            points=(sam_point_coords, sam_point_labels),
            boxes=None,
            masks=sam_mask_prompt,
        )
        low_res_multimasks, ious, sam_output_tokens, object_score_logits = self.sam_mask_decoder(
            image_embeddings=backbone_features,
            image_pe=self.sam_prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=multimask_output,
            repeat_image=False,  # å›¾åƒå·²ç»æ˜¯æ‰¹é‡å¤„ç†çš„
            high_res_features=high_res_features,
        )
        if self.pred_obj_scores:
            is_obj_appearing = object_score_logits > 0

            # ç©ºé—´è®°å¿†æ©ç æ˜¯å¯¹è±¡ä¸éå¯¹è±¡ä¹‹é—´çš„ç¡¬é€‰æ‹©ï¼Œä¸å®é™…æ©ç é¢„æµ‹ä¸€è‡´
            low_res_multimasks = torch.where(is_obj_appearing[:, None, None], low_res_multimasks, NO_OBJ_SCORE)

        # å°†æ©ç ä»å¯èƒ½çš„ bfloat16ï¼ˆæˆ– float16ï¼‰è½¬æ¢ä¸º float32
        # ï¼ˆæ—§ç‰ˆ PyTorchï¼ˆ2.1 ä¹‹å‰ï¼‰ä¸æ”¯æŒ `interpolate` çš„ bf16ï¼‰
        low_res_multimasks = low_res_multimasks.float()
        high_res_multimasks = F.interpolate(
            low_res_multimasks,
            size=(self.image_size, self.image_size),
            mode="bilinear",
            align_corners=False,
        )

        sam_output_token = sam_output_tokens[:, 0]
        if multimask_output:
            # é€‰æ‹©æœ€ä½³æ©ç é¢„æµ‹ï¼ˆå…·æœ‰æœ€é«˜çš„ IoU ä¼°è®¡ï¼‰
            best_iou_inds = torch.argmax(ious, dim=-1)
            batch_inds = torch.arange(B, device=device)
            low_res_masks = low_res_multimasks[batch_inds, best_iou_inds].unsqueeze(1)
            high_res_masks = high_res_multimasks[batch_inds, best_iou_inds].unsqueeze(1)
            if sam_output_tokens.size(1) > 1:
                sam_output_token = sam_output_tokens[batch_inds, best_iou_inds]
        else:
            low_res_masks, high_res_masks = low_res_multimasks, high_res_multimasks

        # ä» SAM è¾“å‡º token æå–å¯¹è±¡æŒ‡é’ˆï¼ˆå¸¦é®æŒ¡å¤„ç†ï¼‰
        obj_ptr = self.obj_ptr_proj(sam_output_token)
        if self.pred_obj_scores:
            # å…è®¸ *è½¯* éå¯¹è±¡æŒ‡é’ˆï¼Œä¸æ©ç ä¸åŒ
            if self.soft_no_obj_ptr:
                lambda_is_obj_appearing = object_score_logits.sigmoid()
            else:
                lambda_is_obj_appearing = is_obj_appearing.float()

            if self.fixed_no_obj_ptr:
                obj_ptr = lambda_is_obj_appearing * obj_ptr
            obj_ptr = obj_ptr + (1 - lambda_is_obj_appearing) * self.no_obj_ptr

        return (
            low_res_multimasks,
            high_res_multimasks,
            ious,
            low_res_masks,
            high_res_masks,
            obj_ptr,
            object_score_logits,
        )

    def _use_mask_as_output(self, backbone_features, high_res_features, mask_inputs):
        """ç›´æ¥å°†æ©ç è¾“å…¥ä½œä¸ºè¾“å‡ºï¼Œç»•è¿‡ SAM ç¼–ç å™¨/è§£ç å™¨ã€‚"""
        # ä½¿ç”¨ -10/+10 ä½œä¸ºè´Ÿ/æ­£åƒç´ çš„ logitsï¼ˆåœ¨ sigmoid åéå¸¸æ¥è¿‘ 0/1 çš„æ¦‚ç‡å€¼ï¼‰ã€‚
        out_scale, out_bias = 20.0, -10.0  # sigmoid(-10.0)=4.5398e-05
        mask_inputs_float = mask_inputs.float()
        high_res_masks = mask_inputs_float * out_scale + out_bias
        low_res_masks = F.interpolate(
            high_res_masks,
            size=(high_res_masks.size(-2) // 4, high_res_masks.size(-1) // 4),
            align_corners=False,
            mode="bilinear",
            antialias=True,  # ä½¿ç”¨æŠ—é”¯é½¿è¿›è¡Œä¸‹é‡‡æ ·
        )
        # ä¸€ä¸ªå‡è®¾çš„ IoU é¢„æµ‹ï¼Œå‡è®¾æ‰€æœ‰è¾“å…¥æ©ç çš„ IoU éƒ½ä¸º 1
        ious = mask_inputs.new_ones(mask_inputs.size(0), 1).float()
        if not self.use_obj_ptrs_in_encoder:
            # ä½¿ç”¨å…¨é›¶çš„å‡å¯¹è±¡æŒ‡é’ˆï¼ˆå½¢çŠ¶ä¸º [B, C]ï¼‰
            obj_ptr = torch.zeros(mask_inputs.size(0), self.hidden_dim, device=mask_inputs.device)
        else:
            # ä½¿ç”¨ SAM è§£ç å™¨æ ¹æ®æ©ç è¾“å…¥ç”Ÿæˆå¯¹è±¡æŒ‡é’ˆ
            _, _, _, _, _, obj_ptr, _ = self._forward_sam_heads(
                backbone_features=backbone_features,
                mask_inputs=self.mask_downsample(mask_inputs_float),
                high_res_features=high_res_features,
            )
        # åœ¨æ­¤æ–¹æ³•ä¸­ï¼Œæˆ‘ä»¬å°† mask_input è§†ä¸ºè¾“å‡ºï¼Œä¾‹å¦‚ç›´æ¥ä½¿ç”¨å®ƒæ¥åˆ›å»ºç©ºé—´è®°å¿†ï¼›
        # ä¸‹é¢ï¼Œæˆ‘ä»¬éµå¾ªç›¸åŒçš„è®¾è®¡å‡†åˆ™ï¼Œä½¿ç”¨ mask_input æ¥å†³å®šå¯¹è±¡æ˜¯å¦å‡ºç°ï¼Œè€Œä¸æ˜¯ä¾èµ–
        # SAM è§£ç å™¨ä¸­çš„ object_scoresã€‚
        is_obj_appearing = torch.any(mask_inputs.flatten(1).float() > 0.0, dim=1)
        is_obj_appearing = is_obj_appearing[..., None]
        lambda_is_obj_appearing = is_obj_appearing.float()
        object_score_logits = out_scale * lambda_is_obj_appearing + out_bias
        if self.pred_obj_scores:
            if self.fixed_no_obj_ptr:
                obj_ptr = lambda_is_obj_appearing * obj_ptr
            obj_ptr = obj_ptr + (1 - lambda_is_obj_appearing) * self.no_obj_ptr

        return (
            low_res_masks,
            high_res_masks,
            ious,
            low_res_masks,
            high_res_masks,
            obj_ptr,
            object_score_logits,
        )

    def forward_image(self, img_batch: torch.Tensor):
        """é€šè¿‡ç¼–ç å™¨å¤„ç†å›¾åƒæ‰¹æ¬¡ï¼Œæå– SAM æ¨¡å‹æ‰€éœ€çš„å¤šå±‚ç‰¹å¾ã€‚"""
        backbone_out = self.image_encoder(img_batch)
        if self.use_high_res_features_in_sam:
            # é¢„è®¡ç®— SAM è§£ç å™¨ä¸­çš„ç¬¬ 0 å±‚å’Œç¬¬ 1 å±‚ç‰¹å¾
            # ä»¥é¿å…åœ¨æ¯æ¬¡ SAM ç‚¹å‡»æ—¶éƒ½é‡æ–°è®¡ç®—
            backbone_out["backbone_fpn"][0] = self.sam_mask_decoder.conv_s0(backbone_out["backbone_fpn"][0])
            backbone_out["backbone_fpn"][1] = self.sam_mask_decoder.conv_s1(backbone_out["backbone_fpn"][1])
        return backbone_out

    def _prepare_backbone_features(self, backbone_out):
        """å‡†å¤‡å¹¶å±•å¼€å›¾åƒéª¨å¹²ç½‘ç»œè¾“å‡ºçš„è§†è§‰ç‰¹å¾ï¼Œä»¥ä¾¿è¿›ä¸€æ­¥å¤„ç†ã€‚"""
        assert len(backbone_out["backbone_fpn"]) == len(backbone_out["vision_pos_enc"])
        assert len(backbone_out["backbone_fpn"]) >= self.num_feature_levels

        feature_maps = backbone_out["backbone_fpn"][-self.num_feature_levels :]
        vision_pos_embeds = backbone_out["vision_pos_enc"][-self.num_feature_levels :]

        feat_sizes = [(x.shape[-2], x.shape[-1]) for x in vision_pos_embeds]
        # å°† NxCxHxW å±•å¼€ä¸º HWxNxC
        vision_feats = [x.flatten(2).permute(2, 0, 1) for x in feature_maps]
        vision_pos_embeds = [x.flatten(2).permute(2, 0, 1) for x in vision_pos_embeds]

        return backbone_out, vision_feats, vision_pos_embeds, feat_sizes

    def _prepare_memory_conditioned_features(
        self,
        frame_idx,
        is_init_cond_frame,
        current_vision_feats,
        current_vision_pos_embeds,
        feat_sizes,
        output_dict,
        num_frames,
        track_in_reverse=False,  # æ˜¯å¦æŒ‰æ—¶é—´é€†åºè·Ÿè¸ªï¼ˆç”¨äºæ¼”ç¤ºï¼‰
    ):
        """é€šè¿‡å°†å½“å‰å¸§çš„è§†è§‰ç‰¹å¾ä¸å…ˆå‰çš„è®°å¿†èåˆï¼Œå‡†å¤‡åŸºäºè®°å¿†çš„æ¡ä»¶ç‰¹å¾ã€‚"""
        B = current_vision_feats[-1].size(1)  # å½“å‰å¸§çš„æ‰¹æ¬¡å¤§å°
        C = self.hidden_dim
        H, W = feat_sizes[-1]  # æœ€é¡¶å±‚ï¼ˆæœ€ä½åˆ†è¾¨ç‡ï¼‰çš„ç‰¹å¾å¤§å°
        device = current_vision_feats[-1].device
        # å½“ `self.num_maskmem == 0` æ—¶ï¼Œä¸»è¦ç”¨äºå¤ç° SAM åœ¨å›¾åƒä¸Šçš„æ•ˆæœã€‚
        # åœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œæˆ‘ä»¬è·³è¿‡ä¸ä»»ä½•è®°å¿†çš„èåˆã€‚
        if self.num_maskmem == 0:  # ç¦ç”¨è®°å¿†å¹¶è·³è¿‡èåˆ
            return current_vision_feats[-1].permute(1, 2, 0).view(B, C, H, W)
        num_obj_ptr_tokens = 0
        tpos_sign_mul = -1 if track_in_reverse else 1
        # æ­¥éª¤ 1: å°†å½“å‰å¸§çš„è§†è§‰ç‰¹å¾ä¸å…ˆå‰çš„è®°å¿†è¿›è¡Œæ¡ä»¶åŒ–
        if not is_init_cond_frame:
            # è·å–é€šè¿‡ maskmem éª¨å¹²ç½‘ç»œç¼–ç çš„è®°å¿†
            to_cat_memory, to_cat_memory_pos_embed = [], []
            # é¦–å…ˆæ·»åŠ å½“å‰æ¡ä»¶å¸§çš„è¾“å‡ºï¼ˆæ‰€æœ‰æ¡ä»¶å¸§çš„ t_pos = 0ï¼‰
            assert len(output_dict["cond_frame_outputs"]) > 0
            # é€‰æ‹©æœ€å¤§æ•°é‡çš„æ—¶é—´ä¸Šæœ€æ¥è¿‘çš„æ¡ä»¶å¸§è¿›è¡Œäº¤å‰æ³¨æ„
            cond_outputs = output_dict["cond_frame_outputs"]
            selected_cond_outputs, unselected_cond_outputs = select_closest_cond_frames(
                frame_idx, cond_outputs, self.max_cond_frames_in_attn
            )
            t_pos_and_prevs = [(0, out) for out in selected_cond_outputs.values()]
            # åœ¨å½“å‰å¸§ä¹‹å‰æ·»åŠ æœ€å (self.num_maskmem - 1) å¸§çš„éæ¡ä»¶è®°å¿†
            # æœ€æ—©çš„å¸§ t_pos=1ï¼Œæœ€æ–°çš„å¸§ t_pos=self.num_maskmem-1
            # å…è®¸ä¸è¿ç»­é€‰æ‹©è®°å¿†å¸§ï¼ˆr>1ï¼‰ï¼Œåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œ
            # æˆ‘ä»¬é€‰æ‹©æ¯ r å¸§ä¸­çš„ (self.num_maskmem - 2) å¸§å’Œæœ€åä¸€å¸§ã€‚
            r = 1 if self.training else self.memory_temporal_stride_for_eval
            for t_pos in range(1, self.num_maskmem):
                t_rel = self.num_maskmem - t_pos  # è·ç¦»å½“å‰å¸§çš„å¸§æ•°
                if t_rel == 1:
                    # å¯¹äº t_rel == 1ï¼Œæˆ‘ä»¬é€‰æ‹©æœ€åä¸€å¸§ï¼ˆä¸è€ƒè™‘ rï¼‰
                    prev_frame_idx = frame_idx + t_rel if track_in_reverse else frame_idx - t_rel
                elif not track_in_reverse:
                    # é¦–å…ˆé€‰æ‹©æ¯ r å¸§ä¹‹å‰çš„æœ€æ¥è¿‘å¸§
                    prev_frame_idx = ((frame_idx - 2) // r) * r
                    prev_frame_idx = prev_frame_idx - (t_rel - 2) * r
                else:
                    # é¦–å…ˆé€‰æ‹©æ¯ r å¸§ä¹‹åçš„æœ€æ¥è¿‘å¸§
                    prev_frame_idx = -(-(frame_idx + 2) // r) * r
                    prev_frame_idx = prev_frame_idx + (t_rel - 2) * r
                out = output_dict["non_cond_frame_outputs"].get(prev_frame_idx, None)
                if out is None:
                    out = unselected_cond_outputs.get(prev_frame_idx, None)
                t_pos_and_prevs.append((t_pos, out))

            for t_pos, prev in t_pos_and_prevs:
                if prev is None:
                    continue  # è·³è¿‡å¡«å……å¸§
                feats = prev["maskmem_features"].to(device=device, non_blocking=True)
                to_cat_memory.append(feats.flatten(2).permute(2, 0, 1))
                maskmem_enc = prev["maskmem_pos_enc"][-1].to(device=device)
                maskmem_enc = maskmem_enc.flatten(2).permute(2, 0, 1)
                maskmem_enc = maskmem_enc + self.maskmem_tpos_enc[self.num_maskmem - t_pos - 1]
                to_cat_memory_pos_embed.append(maskmem_enc)

            if self.use_obj_ptrs_in_encoder:
                max_obj_ptrs_in_encoder = min(num_frames, self.max_obj_ptrs_in_encoder)
                if not self.training and self.only_obj_ptrs_in_the_past_for_eval:
                    ptr_cond_outputs = {
                        t: out
                        for t, out in selected_cond_outputs.items()
                        if (t >= frame_idx if track_in_reverse else t <= frame_idx)
                    }
                else:
                    ptr_cond_outputs = selected_cond_outputs
                pos_and_ptrs = [
                    (
                        (
                            (frame_idx - t) * tpos_sign_mul
                            if self.use_signed_tpos_enc_to_obj_ptrs
                            else abs(frame_idx - t)
                        ),
                        out["obj_ptr"],
                    )
                    for t, out in ptr_cond_outputs.items()
                ]
                for t_diff in range(1, max_obj_ptrs_in_encoder):
                    t = frame_idx + t_diff if track_in_reverse else frame_idx - t_diff
                    if t < 0 or (num_frames is not None and t >= num_frames):
                        break
                    out = output_dict["non_cond_frame_outputs"].get(t, unselected_cond_outputs.get(t, None))
                    if out is not None:
                        pos_and_ptrs.append((t_diff, out["obj_ptr"]))
                if pos_and_ptrs:
                    pos_list, ptrs_list = zip(*pos_and_ptrs)
                    obj_ptrs = torch.stack(ptrs_list, dim=0)
                    if self.add_tpos_enc_to_obj_ptrs:
                        t_diff_max = max_obj_ptrs_in_encoder - 1
                        tpos_dim = C if self.proj_tpos_enc_in_obj_ptrs else self.mem_dim
                        obj_pos = torch.tensor(pos_list, device=device)
                        obj_pos = get_1d_sine_pe(obj_pos / t_diff_max, dim=tpos_dim)
                        obj_pos = self.obj_ptr_tpos_proj(obj_pos)
                        obj_pos = obj_pos.unsqueeze(1).expand(-1, B, self.mem_dim)
                    else:
                        obj_pos = obj_ptrs.new_zeros(len(pos_list), B, self.mem_dim)
                    if self.mem_dim < C:
                        obj_ptrs = obj_ptrs.reshape(-1, B, C // self.mem_dim, self.mem_dim)
                        obj_ptrs = obj_ptrs.permute(0, 2, 1, 3).flatten(0, 1)
                        obj_pos = obj_pos.repeat_interleave(C // self.mem_dim, dim=0)
                    to_cat_memory.append(obj_ptrs)
                    to_cat_memory_pos_embed.append(obj_pos)
                    num_obj_ptr_tokens = obj_ptrs.shape[0]
                else:
                    num_obj_ptr_tokens = 0
        else:
            if self.directly_add_no_mem_embed:
                pix_feat_with_mem = current_vision_feats[-1] + self.no_mem_embed
                pix_feat_with_mem = pix_feat_with_mem.permute(1, 2, 0).view(B, C, H, W)
                return pix_feat_with_mem

            to_cat_memory = [self.no_mem_embed.expand(1, B, self.mem_dim)]
            to_cat_memory_pos_embed = [self.no_mem_pos_enc.expand(1, B, self.mem_dim)]

        # æ­¥éª¤ 2: å°†è®°å¿†å’Œä½ç½®ç¼–ç è¿æ¥èµ·æ¥ï¼Œå¹¶é€šè¿‡ Transformer ç¼–ç å™¨è¿›è¡Œå‰å‘ä¼ æ’­
        memory = torch.cat(to_cat_memory, dim=0)
        memory_pos_embed = torch.cat(to_cat_memory_pos_embed, dim=0)

        pix_feat_with_mem = self.memory_attention(
            curr=current_vision_feats,
            curr_pos=current_vision_pos_embeds,
            memory=memory,
            memory_pos=memory_pos_embed,
            num_obj_ptr_tokens=num_obj_ptr_tokens,
        )
        pix_feat_with_mem = pix_feat_with_mem.permute(1, 2, 0).view(B, C, H, W)
        return pix_feat_with_mem

    def _encode_new_memory(
        self,
        current_vision_feats,
        feat_sizes,
        pred_masks_high_res,
        object_score_logits,
        is_mask_from_pts,
    ):
        """å°†å¸§ç‰¹å¾å’Œæ©ç ç¼–ç ä¸ºæ–°çš„è®°å¿†è¡¨ç¤ºï¼Œç”¨äºè§†é¢‘åˆ†å‰²ã€‚"""
        B = current_vision_feats[-1].size(1)  # å½“å‰å¸§çš„æ‰¹é‡å¤§å°
        C = self.hidden_dim
        H, W = feat_sizes[-1]  # é¡¶å±‚ï¼ˆæœ€ä½åˆ†è¾¨ç‡ï¼‰ç‰¹å¾å°ºå¯¸
        # é¡¶å±‚ç‰¹å¾ï¼Œ(HW)BC => BCHW
        pix_feat = current_vision_feats[-1].permute(1, 2, 0).view(B, C, H, W)
        if self.non_overlap_masks_for_mem_enc and not self.training:
            # å¯é€‰ï¼šå¯¹æ©ç åº”ç”¨ä¸é‡å çº¦æŸï¼ˆä»…åœ¨è¯„ä¼°æ—¶ä½¿ç”¨ï¼Œç¡®ä¿æ‰€æœ‰å¯¹è±¡æ¥è‡ªåŒä¸€è§†é¢‘ï¼Œä¸”æ‰¹é‡å¤§å°ä¸º 1ï¼‰ã€‚
            pred_masks_high_res = self._apply_non_overlapping_constraints(pred_masks_high_res)
        # åœ¨åº”ç”¨ sigmoid ä¹‹å‰å¯¹åŸå§‹æ©ç  logits è¿›è¡Œæ¸©åº¦ç¼©æ”¾
        binarize = self.binarize_mask_from_pts_for_mem_enc and is_mask_from_pts
        if binarize and not self.training:
            mask_for_mem = (pred_masks_high_res > 0).float()
        else:
            # å¯¹åŸå§‹æ©ç  logits åº”ç”¨ sigmoidï¼Œå°†å…¶è½¬åŒ–åˆ° (0, 1) èŒƒå›´å†…
            mask_for_mem = torch.sigmoid(pred_masks_high_res)
        # å¯¹ sigmoid æ¦‚ç‡åº”ç”¨ç¼©æ”¾å’Œåç½®é¡¹
        if self.sigmoid_scale_for_mem_enc != 1.0:
            mask_for_mem = mask_for_mem * self.sigmoid_scale_for_mem_enc
        if self.sigmoid_bias_for_mem_enc != 0.0:
            mask_for_mem = mask_for_mem + self.sigmoid_bias_for_mem_enc
        maskmem_out = self.memory_encoder(pix_feat, mask_for_mem, skip_mask_sigmoid=True)  # å·²ç»åº”ç”¨ sigmoid
        maskmem_features = maskmem_out["vision_features"]
        maskmem_pos_enc = maskmem_out["vision_pos_enc"]
        # å‘ç©ºé—´è®°å¿†ä¸­æ·»åŠ ä¸€ä¸ªæ— å¯¹è±¡åµŒå…¥ï¼Œè¡¨ç¤ºè¯¥å¸§é¢„æµ‹ä¸ºè¢«é®æŒ¡ï¼ˆå³å¸§ä¸­æ²¡æœ‰å‡ºç°å¯¹è±¡ï¼‰
        if self.no_obj_embed_spatial is not None:
            is_obj_appearing = (object_score_logits > 0).float()
            maskmem_features += (1 - is_obj_appearing[..., None, None]) * self.no_obj_embed_spatial[
                ..., None, None
            ].expand(*maskmem_features.shape)

        return maskmem_features, maskmem_pos_enc

    def _track_step(
        self,
        frame_idx,
        is_init_cond_frame,
        current_vision_feats,
        current_vision_pos_embeds,
        feat_sizes,
        point_inputs,
        mask_inputs,
        output_dict,
        num_frames,
        track_in_reverse,
        prev_sam_mask_logits,
    ):
        """æ‰§è¡Œå•æ­¥è·Ÿè¸ªï¼Œæ ¹æ®å½“å‰å¸§è¾“å…¥æ›´æ–°ç›®æ ‡æ©ç å’Œè®°å¿†ç‰¹å¾ã€‚"""
        current_out = {"point_inputs": point_inputs, "mask_inputs": mask_inputs}
        # SAM å¤´çš„é«˜åˆ†è¾¨ç‡ç‰¹å¾å›¾ï¼Œé‡å¡‘ (HW)BC => BCHW
        if len(current_vision_feats) > 1:
            high_res_features = [
                x.permute(1, 2, 0).view(x.size(1), x.size(2), *s)
                for x, s in zip(current_vision_feats[:-1], feat_sizes[:-1])
            ]
        else:
            high_res_features = None
        if mask_inputs is not None and self.use_mask_input_as_output_without_sam:
            # å½“ use_mask_input_as_output_without_sam=True æ—¶ï¼Œæˆ‘ä»¬ç›´æ¥è¾“å‡ºæ©ç è¾“å…¥
            # ï¼ˆè§†å…¶ä¸º GT æ©ç ï¼‰ï¼Œè€Œä¸ä½¿ç”¨ SAM æç¤ºç¼–ç å™¨å’Œæ©ç è§£ç å™¨ã€‚
            pix_feat = current_vision_feats[-1].permute(1, 2, 0)
            pix_feat = pix_feat.view(-1, self.hidden_dim, *feat_sizes[-1])
            sam_outputs = self._use_mask_as_output(pix_feat, high_res_features, mask_inputs)
        else:
            # å°†è§†è§‰ç‰¹å¾ä¸ä¹‹å‰çš„è®°å¿†ç‰¹å¾èåˆåœ¨è®°å¿†åº“ä¸­
            pix_feat = self._prepare_memory_conditioned_features(
                frame_idx=frame_idx,
                is_init_cond_frame=is_init_cond_frame,
                current_vision_feats=current_vision_feats[-1:],
                current_vision_pos_embeds=current_vision_pos_embeds[-1:],
                feat_sizes=feat_sizes[-1:],
                output_dict=output_dict,
                num_frames=num_frames,
                track_in_reverse=track_in_reverse,
            )
            # åº”ç”¨ SAM é£æ ¼çš„åˆ†å‰²å¤´
            # è¿™é‡Œå¯èƒ½ä¼šå°†ä¹‹å‰é¢„æµ‹çš„ä½åˆ†è¾¨ç‡ SAM æ©ç  logits è¾“å…¥åˆ° SAM æ©ç è§£ç å™¨ï¼Œ
            # ä¾‹å¦‚åœ¨æ¼”ç¤ºä¸­ï¼Œè¿™äº› logits æ¥è‡ªæ—©æœŸçš„äº¤äº’è€Œä¸æ˜¯ä¿®æ­£é‡‡æ ·
            # ï¼ˆåœ¨è¿™ç§æƒ…å†µä¸‹ï¼Œä»»ä½• `mask_inputs` éƒ½ä¸åº”è¯¥åˆ°è¾¾è¿™é‡Œï¼Œå› ä¸ºå®ƒä»¬è¢«å‘é€åˆ° _use_mask_as_outputï¼‰
            if prev_sam_mask_logits is not None:
                assert point_inputs is not None and mask_inputs is None
                mask_inputs = prev_sam_mask_logits
            multimask_output = self._use_multimask(is_init_cond_frame, point_inputs)
            sam_outputs = self._forward_sam_heads(
                backbone_features=pix_feat,
                point_inputs=point_inputs,
                mask_inputs=mask_inputs,
                high_res_features=high_res_features,
                multimask_output=multimask_output,
            )
        return current_out, sam_outputs, high_res_features, pix_feat

    def _encode_memory_in_output(
        self,
        current_vision_feats,
        feat_sizes,
        point_inputs,
        run_mem_encoder,
        high_res_masks,
        object_score_logits,
        current_out,
    ):
        """æœ€ç»ˆåœ¨é¢„æµ‹æ©ç ä¸Šè¿è¡Œè®°å¿†ç¼–ç å™¨ï¼Œå°†å…¶ç¼–ç ä¸ºæ–°çš„è®°å¿†ç‰¹å¾ï¼ˆå¯ä»¥ç”¨äºæœªæ¥çš„å¸§ï¼‰ã€‚"""
        if run_mem_encoder and self.num_maskmem > 0:
            high_res_masks_for_mem_enc = high_res_masks
            maskmem_features, maskmem_pos_enc = self._encode_new_memory(
                current_vision_feats=current_vision_feats,
                feat_sizes=feat_sizes,
                pred_masks_high_res=high_res_masks_for_mem_enc,
                object_score_logits=object_score_logits,
                is_mask_from_pts=(point_inputs is not None),
            )
            current_out["maskmem_features"] = maskmem_features
            current_out["maskmem_pos_enc"] = maskmem_pos_enc
        else:
            current_out["maskmem_features"] = None
            current_out["maskmem_pos_enc"] = None

    def track_step(
        self,
        frame_idx,
        is_init_cond_frame,
        current_vision_feats,
        current_vision_pos_embeds,
        feat_sizes,
        point_inputs,
        mask_inputs,
        output_dict,
        num_frames,
        track_in_reverse=False,  # åå‘æ—¶é—´é¡ºåºè·Ÿè¸ªï¼ˆç”¨äºæ¼”ç¤ºï¼‰
        # æ˜¯å¦åœ¨é¢„æµ‹æ©ç ä¸Šè¿è¡Œè®°å¿†ç¼–ç å™¨ã€‚æœ‰æ—¶æˆ‘ä»¬å¯èƒ½å¸Œæœ›
        # è·³è¿‡è®°å¿†ç¼–ç å™¨ï¼Œè®¾ç½® run_mem_encoder=Falseã€‚ä¾‹å¦‚ï¼Œ
        # åœ¨æ¼”ç¤ºä¸­ï¼Œæˆ‘ä»¬å¯èƒ½ä¼šä¸ºæ¯ä¸ªç”¨æˆ·ç‚¹å‡»å¤šæ¬¡è°ƒç”¨ `track_step`ï¼Œ
        # å¹¶ä¸”ä»…åœ¨ç”¨æˆ·ç¡®è®¤ç‚¹å‡»åæ‰ç¼–ç è®°å¿†ã€‚è€Œåœ¨å¦‚ SAM é™æ€å›¾åƒè®­ç»ƒç­‰æ¶ˆèè®¾ç½®ä¸­ï¼Œæˆ‘ä»¬ä¸éœ€è¦è®°å¿†ç¼–ç å™¨ã€‚
        run_mem_encoder=True,
        # å…ˆå‰é¢„æµ‹çš„ SAM æ©ç  logitsï¼ˆå¯ä»¥ä¸æ–°çš„ç‚¹å‡»ä¸€èµ·è¾“å…¥ç”¨äºæ¼”ç¤ºï¼‰ã€‚
        prev_sam_mask_logits=None,
    ):
        """æ‰§è¡Œå•æ­¥è·Ÿè¸ªï¼Œæ ¹æ®å½“å‰å¸§è¾“å…¥æ›´æ–°ç›®æ ‡æ©ç å’Œè®°å¿†ç‰¹å¾ã€‚"""
        current_out, sam_outputs, _, _ = self._track_step(
            frame_idx,
            is_init_cond_frame,
            current_vision_feats,
            current_vision_pos_embeds,
            feat_sizes,
            point_inputs,
            mask_inputs,
            output_dict,
            num_frames,
            track_in_reverse,
            prev_sam_mask_logits,
        )
        _, _, _, low_res_masks, high_res_masks, obj_ptr, object_score_logits = sam_outputs

        current_out["pred_masks"] = low_res_masks
        current_out["pred_masks_high_res"] = high_res_masks
        current_out["obj_ptr"] = obj_ptr
        if not self.training:
            # ä»…åœ¨æ¨ç†æ—¶æ·»åŠ ï¼ˆé¿å…åœ¨æ¿€æ´»æ£€æŸ¥ç‚¹ä¸­ä½¿ç”¨æœªä½¿ç”¨çš„å‚æ•°ï¼›
            # ä¸»è¦ç”¨äºæ¼”ç¤ºä¸­é€šè¿‡ç»Ÿä¸€çš„æ©ç ç¼–ç ç©ºé—´è®°å¿†ï¼‰
            current_out["object_score_logits"] = object_score_logits

        # åœ¨é¢„æµ‹æ©ç ä¸Šè¿è¡Œè®°å¿†ç¼–ç å™¨ï¼Œå°†å…¶ç¼–ç ä¸ºæ–°çš„è®°å¿†ç‰¹å¾ï¼ˆä¾›æœªæ¥å¸§ä½¿ç”¨ï¼‰
        self._encode_memory_in_output(
            current_vision_feats,
            feat_sizes,
            point_inputs,
            run_mem_encoder,
            high_res_masks,
            object_score_logits,
            current_out,
        )

        return current_out

    def _use_multimask(self, is_init_cond_frame, point_inputs):
        """æ ¹æ®é…ç½®å’Œè¾“å…¥å†³å®šæ˜¯å¦åœ¨SAMå¤´ä¸­ä½¿ç”¨å¤šä¸ªæ©ç è¾“å‡ºã€‚"""
        num_pts = 0 if point_inputs is None else point_inputs["point_labels"].size(1)
        return (
            self.multimask_output_in_sam
            and (is_init_cond_frame or self.multimask_output_for_tracking)
            and (self.multimask_min_pt_num <= num_pts <= self.multimask_max_pt_num)
        )

    @staticmethod
    def _apply_non_overlapping_constraints(pred_masks):
        """å¯¹æ©ç åº”ç”¨ä¸é‡å çº¦æŸï¼Œä¿æŒæ¯ä¸ªä½ç½®å¾—åˆ†æœ€é«˜çš„å¯¹è±¡ã€‚"""
        batch_size = pred_masks.size(0)
        if batch_size == 1:
            return pred_masks

        device = pred_masks.device
        # "max_obj_inds"ï¼šæ¯ä¸ªä½ç½®å¾—åˆ†æœ€é«˜çš„å¯¹è±¡çš„ç´¢å¼•
        max_obj_inds = torch.argmax(pred_masks, dim=0, keepdim=True)
        # "batch_obj_inds"ï¼šæ¯ä¸ªå¯¹è±¡åˆ‡ç‰‡åœ¨`pred_masks`ä¸­çš„ç´¢å¼•ï¼ˆæ²¿ç¬¬0ç»´ï¼‰
        batch_obj_inds = torch.arange(batch_size, device=device)[:, None, None, None]
        keep = max_obj_inds == batch_obj_inds
        # å¯¹é‡å åŒºåŸŸçš„å¾—åˆ†åº”ç”¨æŠ‘åˆ¶ï¼Œä¿æŒå‰æ™¯åŒºåŸŸä¸é‡å 
        # è¿™é‡Œå°†åˆ†æ•°å°äº-10.0çš„åŒºåŸŸè®¾ä¸ºæ— æ•ˆï¼ˆsigmoid(-10.0)=4.5398e-05ï¼‰
        pred_masks = torch.where(keep, pred_masks, torch.clamp(pred_masks, max=-10.0))
        return pred_masks

    def set_binarize(self, binarize=False):
        """ä¸º VideoPredictor è®¾ç½®äºŒå€¼åŒ–å‚æ•°ã€‚"""
        self.binarize_mask_from_pts_for_mem_enc = binarize

    def set_imgsz(self, imgsz):
        """
        è®¾ç½®å›¾åƒå¤§å°ä»¥ä½¿æ¨¡å‹å…¼å®¹ä¸åŒçš„å›¾åƒå°ºå¯¸ã€‚

        å‚æ•°è¯´æ˜:
            imgsz (Tuple[int, int]): è¾“å…¥å›¾åƒçš„å°ºå¯¸ã€‚
        """
        self.image_size = imgsz[0]
        self.sam_prompt_encoder.input_image_size = imgsz
        self.sam_prompt_encoder.image_embedding_size = [x // 16 for x in imgsz]  # å›ºå®šçš„ViTè¡¥ä¸å¤§å°ä¸º16

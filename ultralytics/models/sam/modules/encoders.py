# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license

from typing import List, Optional, Tuple, Type

import torch
import torch.nn as nn
import torch.nn.functional as F

from ultralytics.nn.modules import LayerNorm2d

from .blocks import (
    Block,
    CXBlock,
    Fuser,
    MaskDownSampler,
    MultiScaleBlock,
    PatchEmbed,
    PositionEmbeddingRandom,
    PositionEmbeddingSine,
)


class ImageEncoderViT(nn.Module):
    """
    ä½¿ç”¨è§†è§‰ Transformer (ViT) æ¶æ„çš„å›¾åƒç¼–ç å™¨ï¼Œå°†å›¾åƒç¼–ç ä¸ºç´§å‡‘çš„æ½œåœ¨ç©ºé—´ã€‚

    è¯¥ç±»é€šè¿‡å°†å›¾åƒåˆ†å‰²ä¸ºè¡¥ä¸ã€åº”ç”¨ Transformer å—å¹¶é€šè¿‡é¢ˆéƒ¨æ¨¡å—ç”Ÿæˆæœ€ç»ˆçš„ç¼–ç è¡¨ç¤ºæ¥å¤„ç†å›¾åƒã€‚

    å±æ€§:
        img_size (int): è¾“å…¥å›¾åƒçš„å°ºå¯¸ï¼Œå‡å®šä¸ºæ­£æ–¹å½¢ã€‚
        patch_embed (PatchEmbed): ç”¨äºè¡¥ä¸åµŒå…¥çš„æ¨¡å—ã€‚
        pos_embed (nn.Parameter | None): ç”¨äºè¡¥ä¸çš„ç»å¯¹ä½ç½®åµŒå…¥ã€‚
        blocks (nn.ModuleList): ç”¨äºå¤„ç†è¡¥ä¸åµŒå…¥çš„ Transformer å—åˆ—è¡¨ã€‚
        neck (nn.Sequential): ç”¨äºè¿›ä¸€æ­¥å¤„ç†è¾“å‡ºçš„é¢ˆéƒ¨æ¨¡å—ã€‚

    æ–¹æ³•:
        forward: é€šè¿‡è¡¥ä¸åµŒå…¥ã€ä½ç½®åµŒå…¥ã€Transformer å—å’Œé¢ˆéƒ¨æ¨¡å—å¤„ç†è¾“å…¥ã€‚

    ç¤ºä¾‹:
        >>> import torch
        >>> encoder = ImageEncoderViT(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12)
        >>> input_image = torch.randn(1, 3, 224, 224)
        >>> output = encoder(input_image)
        >>> print(output.shape)
    """

    def __init__(
        self,
        img_size: int = 1024,
        patch_size: int = 16,
        in_chans: int = 3,
        embed_dim: int = 768,
        depth: int = 12,
        num_heads: int = 12,
        mlp_ratio: float = 4.0,
        out_chans: int = 256,
        qkv_bias: bool = True,
        norm_layer: Type[nn.Module] = nn.LayerNorm,
        act_layer: Type[nn.Module] = nn.GELU,
        use_abs_pos: bool = True,
        use_rel_pos: bool = False,
        rel_pos_zero_init: bool = True,
        window_size: int = 0,
        global_attn_indexes: Tuple[int, ...] = (),
    ) -> None:
        """
        ä½¿ç”¨è§†è§‰ Transformer æ¶æ„åˆå§‹åŒ– ImageEncoderViT å®ä¾‹ï¼Œç”¨äºç¼–ç å›¾åƒã€‚

        å‚æ•°:
            img_size (int): è¾“å…¥å›¾åƒå°ºå¯¸ï¼Œå‡å®šä¸ºæ­£æ–¹å½¢ã€‚
            patch_size (int): å›¾åƒè¡¥ä¸çš„å¤§å°ã€‚
            in_chans (int): è¾“å…¥å›¾åƒçš„é€šé“æ•°ã€‚
            embed_dim (int): è¡¥ä¸åµŒå…¥çš„ç»´åº¦ã€‚
            depth (int): Transformer å—çš„æ•°é‡ã€‚
            num_heads (int): æ¯ä¸ªå—ä¸­çš„æ³¨æ„åŠ›å¤´æ•°ã€‚
            mlp_ratio (float): MLP éšè—å±‚ç»´åº¦ä¸åµŒå…¥ç»´åº¦çš„æ¯”ç‡ã€‚
            out_chans (int): é¢ˆéƒ¨æ¨¡å—çš„è¾“å‡ºé€šé“æ•°ã€‚
            qkv_bias (bool): å¦‚æœä¸º Trueï¼Œåˆ™åœ¨æŸ¥è¯¢ã€é”®ã€å€¼æŠ•å½±ä¸­æ·»åŠ å¯å­¦ä¹ çš„åç½®ã€‚
            norm_layer (Type[nn.Module]): è¦ä½¿ç”¨çš„å½’ä¸€åŒ–å±‚ç±»å‹ã€‚
            act_layer (Type[nn.Module]): è¦ä½¿ç”¨çš„æ¿€æ´»å±‚ç±»å‹ã€‚
            use_abs_pos (bool): å¦‚æœä¸º Trueï¼Œåˆ™ä½¿ç”¨ç»å¯¹ä½ç½®åµŒå…¥ã€‚
            use_rel_pos (bool): å¦‚æœä¸º Trueï¼Œåˆ™å‘æ³¨æ„åŠ›å›¾æ·»åŠ ç›¸å¯¹ä½ç½®åµŒå…¥ã€‚
            rel_pos_zero_init (bool): å¦‚æœä¸º Trueï¼Œåˆ™å°†ç›¸å¯¹ä½ç½®å‚æ•°åˆå§‹åŒ–ä¸ºé›¶ã€‚
            window_size (int): çª—å£åŒ–æ³¨æ„åŠ›å—çš„æ³¨æ„åŠ›çª—å£å¤§å°ã€‚
            global_attn_indexes (Tuple[int, ...]): ä½¿ç”¨å…¨å±€æ³¨æ„åŠ›çš„å—çš„ç´¢å¼•ã€‚

        å±æ€§:
            img_size (int): è¾“å…¥å›¾åƒçš„å°ºå¯¸ã€‚
            patch_embed (PatchEmbed): ç”¨äºè¡¥ä¸åµŒå…¥çš„æ¨¡å—ã€‚
            pos_embed (nn.Parameter | None): ç”¨äºè¡¥ä¸çš„ç»å¯¹ä½ç½®åµŒå…¥ã€‚
            blocks (nn.ModuleList): Transformer å—çš„åˆ—è¡¨ã€‚
            neck (nn.Sequential): æœ€ç»ˆå¤„ç†çš„é¢ˆéƒ¨æ¨¡å—ã€‚

        ç¤ºä¾‹:
            >>> encoder = ImageEncoderViT(img_size=224, patch_size=16, embed_dim=768, depth=12, num_heads=12)
            >>> input_image = torch.randn(1, 3, 224, 224)
            >>> output = encoder(input_image)
            >>> print(output.shape)
        """
        super().__init__()
        self.img_size = img_size

        self.patch_embed = PatchEmbed(
            kernel_size=(patch_size, patch_size),
            stride=(patch_size, patch_size),
            in_chans=in_chans,
            embed_dim=embed_dim,
        )

        self.pos_embed: Optional[nn.Parameter] = None
        if use_abs_pos:
            # åˆå§‹åŒ–ç»å¯¹ä½ç½®åµŒå…¥ï¼Œé¢„è®¾å›¾åƒå¤§å°ã€‚
            self.pos_embed = nn.Parameter(torch.zeros(1, img_size // patch_size, img_size // patch_size, embed_dim))

        self.blocks = nn.ModuleList()
        for i in range(depth):
            block = Block(
                dim=embed_dim,
                num_heads=num_heads,
                mlp_ratio=mlp_ratio,
                qkv_bias=qkv_bias,
                norm_layer=norm_layer,
                act_layer=act_layer,
                use_rel_pos=use_rel_pos,
                rel_pos_zero_init=rel_pos_zero_init,
                window_size=window_size if i not in global_attn_indexes else 0,
                input_size=(img_size // patch_size, img_size // patch_size),
            )
            self.blocks.append(block)

        self.neck = nn.Sequential(
            nn.Conv2d(
                embed_dim,
                out_chans,
                kernel_size=1,
                bias=False,
            ),
            LayerNorm2d(out_chans),
            nn.Conv2d(
                out_chans,
                out_chans,
                kernel_size=3,
                padding=1,
                bias=False,
            ),
            LayerNorm2d(out_chans),
        )

    def forward(self, x: torch.Tensor) -> torch.Tensor:
        """é€šè¿‡è¡¥ä¸åµŒå…¥ã€ä½ç½®åµŒå…¥ã€Transformer å—å’Œé¢ˆéƒ¨æ¨¡å—å¤„ç†è¾“å…¥æ•°æ®ã€‚"""
        x = self.patch_embed(x)
        if self.pos_embed is not None:
            pos_embed = (
                F.interpolate(self.pos_embed.permute(0, 3, 1, 2), scale_factor=self.img_size / 1024).permute(0, 2, 3, 1)
                if self.img_size != 1024
                else self.pos_embed
            )
            x = x + pos_embed
        for blk in self.blocks:
            x = blk(x)
        return self.neck(x.permute(0, 3, 1, 2))


class PromptEncoder(nn.Module):
    """
    å¯¹ä¸åŒç±»å‹çš„æç¤ºè¿›è¡Œç¼–ç ï¼Œä¾› SAM çš„æ©ç è§£ç å™¨ä½¿ç”¨ï¼Œç”Ÿæˆç¨€ç–å’Œå¯†é›†çš„åµŒå…¥è¡¨ç¤ºã€‚

    å±æ€§è¯´æ˜:
        embed_dim (int): åµŒå…¥çš„ç»´åº¦ã€‚
        input_image_size (Tuple[int, int]): è¾“å…¥å›¾åƒçš„å¤§å°ï¼Œæ ¼å¼ä¸º (H, W)ã€‚
        image_embedding_size (Tuple[int, int]): å›¾åƒåµŒå…¥çš„ç©ºé—´å¤§å°ï¼Œæ ¼å¼ä¸º (H, W)ã€‚
        pe_layer (PositionEmbeddingRandom): éšæœºä½ç½®åµŒå…¥çš„æ¨¡å—ã€‚
        num_point_embeddings (int): ç”¨äºä¸åŒç±»å‹ç‚¹çš„ç‚¹åµŒå…¥æ•°é‡ã€‚
        point_embeddings (nn.ModuleList): ç‚¹åµŒå…¥çš„åˆ—è¡¨ã€‚
        not_a_point_embed (nn.Embedding): ç”¨äºè¡¨ç¤ºéæ ‡ç­¾ç‚¹çš„åµŒå…¥ã€‚
        mask_input_size (Tuple[int, int]): è¾“å…¥æ©ç çš„å¤§å°ã€‚
        mask_downscaling (nn.Sequential): ç”¨äºä¸‹é‡‡æ ·æ©ç çš„ç¥ç»ç½‘ç»œã€‚
        no_mask_embed (nn.Embedding): ç”¨äºæœªæä¾›æ©ç çš„æƒ…å†µçš„åµŒå…¥ã€‚

    æ–¹æ³•è¯´æ˜:
        get_dense_pe: è¿”å›ç”¨äºç¼–ç ç‚¹æç¤ºçš„ä½ç½®ä¿¡æ¯ç¼–ç ã€‚
        forward: å¯¹ä¸åŒç±»å‹çš„æç¤ºè¿›è¡ŒåµŒå…¥ï¼Œè¿”å›ç¨€ç–å’Œå¯†é›†çš„åµŒå…¥è¡¨ç¤ºã€‚

    ç¤ºä¾‹ç”¨æ³•:
        >>> prompt_encoder = PromptEncoder(256, (64, 64), (1024, 1024), 16)
        >>> points = (torch.rand(1, 5, 2), torch.randint(0, 4, (1, 5)))
        >>> boxes = torch.rand(1, 2, 2)
        >>> masks = torch.rand(1, 1, 256, 256)
        >>> sparse_embeddings, dense_embeddings = prompt_encoder(points, boxes, masks)
        >>> print(sparse_embeddings.shape, dense_embeddings.shape)
        torch.Size([1, 7, 256]) torch.Size([1, 256, 64, 64])
    """

    def __init__(
        self,
        embed_dim: int,
        image_embedding_size: Tuple[int, int],
        input_image_size: Tuple[int, int],
        mask_in_chans: int,
        activation: Type[nn.Module] = nn.GELU,
    ) -> None:
        """
        åˆå§‹åŒ– PromptEncoder æ¨¡å—ï¼Œç”¨äºç¼–ç å„ç§ç±»å‹çš„æç¤ºã€‚

        è¯¥æ¨¡å—å°†ä¸åŒç±»å‹çš„æç¤ºï¼ˆç‚¹ã€æ¡†ã€æ©ç ï¼‰ç¼–ç ä¸º SAM çš„æ©ç è§£ç å™¨è¾“å…¥ï¼Œ
        ç”Ÿæˆç¨€ç–å’Œå¯†é›†çš„åµŒå…¥è¡¨ç¤ºã€‚

        å‚æ•°è¯´æ˜:
            embed_dim (int): åµŒå…¥çš„ç»´åº¦ã€‚
            image_embedding_size (Tuple[int, int]): å›¾åƒåµŒå…¥çš„ç©ºé—´å¤§å°ï¼Œæ ¼å¼ä¸º (H, W)ã€‚
            input_image_size (Tuple[int, int]): è¾“å…¥å›¾åƒçš„å¡«å……å¤§å°ï¼Œæ ¼å¼ä¸º (H, W)ã€‚
            mask_in_chans (int): ç”¨äºç¼–ç è¾“å…¥æ©ç çš„éšè—é€šé“æ•°ã€‚
            activation (Type[nn.Module]): ç”¨äºç¼–ç è¾“å…¥æ©ç æ—¶çš„æ¿€æ´»å‡½æ•°ã€‚

        å±æ€§è¯´æ˜:
            embed_dim (int): åµŒå…¥çš„ç»´åº¦ã€‚
            input_image_size (Tuple[int, int]): è¾“å…¥å›¾åƒçš„å¤§å°ï¼Œæ ¼å¼ä¸º (H, W)ã€‚
            image_embedding_size (Tuple[int, int]): å›¾åƒåµŒå…¥çš„ç©ºé—´å¤§å°ï¼Œæ ¼å¼ä¸º (H, W)ã€‚
            pe_layer (PositionEmbeddingRandom): éšæœºä½ç½®åµŒå…¥çš„æ¨¡å—ã€‚
            num_point_embeddings (int): ç”¨äºä¸åŒç±»å‹ç‚¹çš„ç‚¹åµŒå…¥æ•°é‡ã€‚
            point_embeddings (nn.ModuleList): ç‚¹åµŒå…¥çš„åˆ—è¡¨ã€‚
            not_a_point_embed (nn.Embedding): ç”¨äºè¡¨ç¤ºéæ ‡ç­¾ç‚¹çš„åµŒå…¥ã€‚
            mask_input_size (Tuple[int, int]): è¾“å…¥æ©ç çš„å¤§å°ã€‚
            mask_downscaling (nn.Sequential): ç”¨äºä¸‹é‡‡æ ·æ©ç çš„ç¥ç»ç½‘ç»œã€‚

        ç¤ºä¾‹ç”¨æ³•:
            >>> prompt_encoder = PromptEncoder(256, (64, 64), (1024, 1024), 16)
            >>> points = (torch.rand(1, 5, 2), torch.randint(0, 4, (1, 5)))
            >>> boxes = torch.rand(1, 2, 2)
            >>> masks = torch.rand(1, 1, 256, 256)
            >>> sparse_embeddings, dense_embeddings = prompt_encoder(points, boxes, masks)
            >>> print(sparse_embeddings.shape, dense_embeddings.shape)
            torch.Size([1, 7, 256]) torch.Size([1, 256, 64, 64])
        """
        super().__init__()
        self.embed_dim = embed_dim
        self.input_image_size = input_image_size
        self.image_embedding_size = image_embedding_size
        self.pe_layer = PositionEmbeddingRandom(embed_dim // 2)

        self.num_point_embeddings: int = 4  # pos/neg ç‚¹ + 2 ä¸ªæ¡†è§’ç‚¹
        point_embeddings = [nn.Embedding(1, embed_dim) for _ in range(self.num_point_embeddings)]
        self.point_embeddings = nn.ModuleList(point_embeddings)
        self.not_a_point_embed = nn.Embedding(1, embed_dim)

        self.mask_input_size = (4 * image_embedding_size[0], 4 * image_embedding_size[1])
        self.mask_downscaling = nn.Sequential(
            nn.Conv2d(1, mask_in_chans // 4, kernel_size=2, stride=2),
            LayerNorm2d(mask_in_chans // 4),
            activation(),
            nn.Conv2d(mask_in_chans // 4, mask_in_chans, kernel_size=2, stride=2),
            LayerNorm2d(mask_in_chans),
            activation(),
            nn.Conv2d(mask_in_chans, embed_dim, kernel_size=1),
        )
        self.no_mask_embed = nn.Embedding(1, embed_dim)

    def get_dense_pe(self) -> torch.Tensor:
        """
        è¿”å›ç”¨äºç¼–ç ç‚¹æç¤ºçš„å¯†é›†ä½ç½®ç¼–ç ã€‚

        è¯¥æ–¹æ³•ç”Ÿæˆä¸€ä¸ªä¸å›¾åƒç¼–ç å½¢çŠ¶åŒ¹é…çš„å¯†é›†ä½ç½®ç¼–ç ã€‚æ­¤ç¼–ç ç”¨äºåœ¨å¤„ç†ç‚¹æç¤ºæ—¶ï¼Œå‘æ¨¡å‹æä¾›ç©ºé—´ä¿¡æ¯ã€‚

        è¿”å›:
            (torch.Tensor): ä½ç½®ç¼–ç å¼ é‡ï¼Œå½¢çŠ¶ä¸º (1, embed_dim, H, W)ï¼Œå…¶ä¸­ H å’Œ W æ˜¯å›¾åƒåµŒå…¥å¤§å°çš„é«˜åº¦å’Œå®½åº¦ã€‚

        ç¤ºä¾‹:
            >>> prompt_encoder = PromptEncoder(256, (64, 64), (1024, 1024), 16)
            >>> dense_pe = prompt_encoder.get_dense_pe()
            >>> print(dense_pe.shape)
            torch.Size([1, 256, 64, 64])
        """
        return self.pe_layer(self.image_embedding_size).unsqueeze(0)

    def _embed_points(self, points: torch.Tensor, labels: torch.Tensor, pad: bool) -> torch.Tensor:
        """é€šè¿‡åº”ç”¨ä½ç½®ç¼–ç å’Œæ ‡ç­¾ç‰¹å®šçš„åµŒå…¥ï¼ŒåµŒå…¥ç‚¹æç¤ºã€‚"""
        points = points + 0.5  # å°†ç‚¹ç§»åŠ¨åˆ°åƒç´ ä¸­å¿ƒ
        if pad:
            padding_point = torch.zeros((points.shape[0], 1, 2), device=points.device)
            padding_label = -torch.ones((labels.shape[0], 1), device=labels.device)
            points = torch.cat([points, padding_point], dim=1)
            labels = torch.cat([labels, padding_label], dim=1)
        point_embedding = self.pe_layer.forward_with_coords(points, self.input_image_size)
        point_embedding[labels == -1] = 0.0
        point_embedding[labels == -1] += self.not_a_point_embed.weight
        point_embedding[labels == 0] += self.point_embeddings[0].weight
        point_embedding[labels == 1] += self.point_embeddings[1].weight
        point_embedding[labels == 2] += self.point_embeddings[2].weight
        point_embedding[labels == 3] += self.point_embeddings[3].weight
        return point_embedding

    def _embed_boxes(self, boxes: torch.Tensor) -> torch.Tensor:
        """é€šè¿‡åº”ç”¨ä½ç½®ç¼–ç å¹¶æ·»åŠ è§’è½åµŒå…¥ï¼ŒåµŒå…¥æ¡†æç¤ºã€‚"""
        boxes = boxes + 0.5  # å°†æ¡†ç§»åŠ¨åˆ°åƒç´ ä¸­å¿ƒ
        coords = boxes.reshape(-1, 2, 2)
        corner_embedding = self.pe_layer.forward_with_coords(coords, self.input_image_size)
        corner_embedding[:, 0, :] += self.point_embeddings[2].weight
        corner_embedding[:, 1, :] += self.point_embeddings[3].weight
        return corner_embedding

    def _embed_masks(self, masks: torch.Tensor) -> torch.Tensor:
        """é€šè¿‡ä¸‹é‡‡æ ·å¹¶é€šè¿‡å·ç§¯å±‚å¤„ç†æ©ç è¾“å…¥ï¼ŒåµŒå…¥æ©ç ã€‚"""
        return self.mask_downscaling(masks)

    @staticmethod
    def _get_batch_size(
        points: Optional[Tuple[torch.Tensor, torch.Tensor]],
        boxes: Optional[torch.Tensor],
        masks: Optional[torch.Tensor],
    ) -> int:
        """æ ¹æ®è¾“å…¥æç¤ºçš„æ‰¹é‡å¤§å°è·å–è¾“å‡ºçš„æ‰¹é‡å¤§å°ã€‚"""
        if points is not None:
            return points[0].shape[0]
        elif boxes is not None:
            return boxes.shape[0]
        elif masks is not None:
            return masks.shape[0]
        else:
            return 1

    def _get_device(self) -> torch.device:
        """è¿”å›ç¬¬ä¸€ä¸ªç‚¹åµŒå…¥çš„æƒé‡å¼ é‡çš„è®¾å¤‡ã€‚"""
        return self.point_embeddings[0].weight.device

    def forward(
        self,
        points: Optional[Tuple[torch.Tensor, torch.Tensor]],
        boxes: Optional[torch.Tensor],
        masks: Optional[torch.Tensor],
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """
        åµŒå…¥ä¸åŒç±»å‹çš„æç¤ºï¼Œè¿”å›ç¨€ç–å’Œå¯†é›†çš„åµŒå…¥ã€‚

        å‚æ•°:
            points (Tuple[torch.Tensor, torch.Tensor] | None): è¦åµŒå…¥çš„ç‚¹åæ ‡å’Œæ ‡ç­¾ã€‚ç¬¬ä¸€ä¸ªå¼ é‡åŒ…å«åæ ‡ï¼Œå½¢çŠ¶ä¸º (B, N, 2)ï¼Œç¬¬äºŒä¸ªå¼ é‡åŒ…å«æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º (B, N)ã€‚
            boxes (torch.Tensor | None): è¦åµŒå…¥çš„æ¡†ï¼Œå½¢çŠ¶ä¸º (B, M, 2, 2)ï¼Œå…¶ä¸­ M æ˜¯æ¡†çš„æ•°é‡ã€‚
            masks (torch.Tensor | None): è¦åµŒå…¥çš„æ©ç ï¼Œå½¢çŠ¶ä¸º (B, 1, H, W)ã€‚

        è¿”å›:
            (Tuple[torch.Tensor, torch.Tensor]): ä¸€ä¸ªåŒ…å«ä»¥ä¸‹å†…å®¹çš„å…ƒç»„:
                - sparse_embeddings (torch.Tensor): ç‚¹å’Œæ¡†çš„ç¨€ç–åµŒå…¥ï¼Œå½¢çŠ¶ä¸º (B, N, embed_dim)ã€‚
                - dense_embeddings (torch.Tensor): æ©ç çš„å¯†é›†åµŒå…¥ï¼Œå½¢çŠ¶ä¸º (B, embed_dim, embed_H, embed_W)ã€‚

        ç¤ºä¾‹:
            >>> encoder = PromptEncoder(256, (64, 64), (1024, 1024), 16)
            >>> points = (torch.rand(1, 5, 2), torch.randint(0, 4, (1, 5)))
            >>> boxes = torch.rand(1, 2, 2, 2)
            >>> masks = torch.rand(1, 1, 256, 256)
            >>> sparse_emb, dense_emb = encoder(points, boxes, masks)
            >>> print(sparse_emb.shape, dense_emb.shape)
            torch.Size([1, 7, 256]) torch.Size([1, 256, 64, 64])
        """
        bs = self._get_batch_size(points, boxes, masks)
        sparse_embeddings = torch.empty((bs, 0, self.embed_dim), device=self._get_device())
        if points is not None:
            coords, labels = points
            point_embeddings = self._embed_points(coords, labels, pad=(boxes is None))
            sparse_embeddings = torch.cat([sparse_embeddings, point_embeddings], dim=1)
        if boxes is not None:
            box_embeddings = self._embed_boxes(boxes)
            sparse_embeddings = torch.cat([sparse_embeddings, box_embeddings], dim=1)

        if masks is not None:
            dense_embeddings = self._embed_masks(masks)
        else:
            dense_embeddings = self.no_mask_embed.weight.reshape(1, -1, 1, 1).expand(
                bs, -1, self.image_embedding_size[0], self.image_embedding_size[1]
            )

        return sparse_embeddings, dense_embeddings


class MemoryEncoder(nn.Module):
    """
    å°†åƒç´ ç‰¹å¾å’Œæ©ç ç¼–ç ä¸ºå†…å­˜è¡¨ç¤ºï¼Œä»¥å®ç°é«˜æ•ˆçš„å›¾åƒåˆ†å‰²ã€‚

    è¯¥ç±»å¤„ç†åƒç´ çº§ç‰¹å¾å’Œæ©ç ï¼Œå°†å…¶èåˆç”Ÿæˆé€‚ç”¨äºä¸‹æ¸¸ä»»åŠ¡ï¼ˆå¦‚ SAMï¼ˆSegment Anything Modelï¼‰ï¼‰çš„ç¼–ç å†…å­˜è¡¨ç¤ºã€‚

    å±æ€§:
        mask_downsampler (MaskDownSampler): ç”¨äºä¸‹é‡‡æ ·è¾“å…¥æ©ç çš„æ¨¡å—ã€‚
        pix_feat_proj (nn.Conv2d): ç”¨äºå°†åƒç´ ç‰¹å¾æŠ•å½±åˆ°å¦ä¸€ä¸ªç©ºé—´çš„å·ç§¯å±‚ã€‚
        fuser (Fuser): ç”¨äºèåˆåƒç´ ç‰¹å¾å’Œæ©ç çš„æ¨¡å—ã€‚
        position_encoding (PositionEmbeddingSine): ç”¨äºå‘ç‰¹å¾æ·»åŠ ä½ç½®ç¼–ç çš„æ¨¡å—ã€‚
        out_proj (nn.Module): è¾“å‡ºæŠ•å½±å±‚ï¼Œå¯ä»¥æ˜¯ nn.Identity æˆ– nn.Conv2dã€‚

    æ–¹æ³•:
        forward: å¤„ç†è¾“å…¥åƒç´ ç‰¹å¾å’Œæ©ç ï¼Œç”Ÿæˆç¼–ç çš„å†…å­˜è¡¨ç¤ºã€‚

    ç¤ºä¾‹:
        >>> import torch
        >>> encoder = MemoryEncoder(out_dim=256, in_dim=256)
        >>> pix_feat = torch.randn(1, 256, 64, 64)
        >>> masks = torch.randn(1, 1, 64, 64)
        >>> encoded_feat, pos = encoder(pix_feat, masks)
        >>> print(encoded_feat.shape, pos.shape)
        torch.Size([1, 256, 64, 64]) torch.Size([1, 128, 64, 64])
    """

    def __init__(
        self,
        out_dim,
        in_dim=256,  # pix_feats çš„ in_dim
    ):
        """åˆå§‹åŒ– MemoryEncoderï¼Œç”¨äºå°†åƒç´ ç‰¹å¾å’Œæ©ç ç¼–ç ä¸ºå†…å­˜è¡¨ç¤ºã€‚"""
        super().__init__()

        self.mask_downsampler = MaskDownSampler(kernel_size=3, stride=2, padding=1)

        self.pix_feat_proj = nn.Conv2d(in_dim, in_dim, kernel_size=1)
        self.fuser = Fuser(CXBlock(dim=256), num_layers=2)
        self.position_encoding = PositionEmbeddingSine(num_pos_feats=64)
        self.out_proj = nn.Identity()
        if out_dim != in_dim:
            self.out_proj = nn.Conv2d(in_dim, out_dim, kernel_size=1)

    def forward(
        self,
        pix_feat: torch.Tensor,
        masks: torch.Tensor,
        skip_mask_sigmoid: bool = False,
    ) -> Tuple[torch.Tensor, torch.Tensor]:
        """å¤„ç†åƒç´ ç‰¹å¾å’Œæ©ç ï¼Œç”Ÿæˆç”¨äºåˆ†å‰²çš„ç¼–ç å†…å­˜è¡¨ç¤ºã€‚"""
        if not skip_mask_sigmoid:
            masks = F.sigmoid(masks)
        masks = self.mask_downsampler(masks)

        # èåˆåƒç´ ç‰¹å¾å’Œä¸‹é‡‡æ ·åçš„æ©ç ï¼Œå¦‚æœè§†è§‰ç‰¹å¾åœ¨ CPU ä¸Šï¼Œå°†å…¶è½¬æ¢ä¸º CUDA
        pix_feat = pix_feat.to(masks.device)

        x = self.pix_feat_proj(pix_feat)
        x = x + masks
        x = self.fuser(x)
        x = self.out_proj(x)

        pos = self.position_encoding(x).to(x.dtype)

        return {"vision_features": x, "vision_pos_enc": [pos]}


class ImageEncoder(nn.Module):
    """
    ä½¿ç”¨ trunk-neck æ¶æ„ç¼–ç å›¾åƒï¼Œç”Ÿæˆå¤šå°ºåº¦ç‰¹å¾å’Œä½ç½®ç¼–ç ã€‚

    è¯¥ç±»ç»“åˆäº†ç”¨äºç‰¹å¾æå–çš„ trunk ç½‘ç»œå’Œç”¨äºç‰¹å¾ç»†åŒ–åŠä½ç½®ç¼–ç ç”Ÿæˆçš„ neck ç½‘ç»œã€‚
    å®ƒå¯ä»¥é€‰æ‹©æ€§åœ°ä¸¢å¼ƒæœ€ä½åˆ†è¾¨ç‡çš„ç‰¹å¾ã€‚

    å±æ€§ï¼š
        trunk (nn.Module): ç”¨äºåˆæ­¥ç‰¹å¾æå–çš„ trunk ç½‘ç»œã€‚
        neck (nn.Module): ç”¨äºç‰¹å¾ç»†åŒ–å’Œä½ç½®ç¼–ç ç”Ÿæˆçš„ neck ç½‘ç»œã€‚
        scalp (int): ä¸¢å¼ƒçš„æœ€ä½åˆ†è¾¨ç‡ç‰¹å¾çº§åˆ«æ•°ã€‚

    æ–¹æ³•ï¼š
        forward: é€šè¿‡ trunk å’Œ neck ç½‘ç»œå¤„ç†è¾“å…¥å›¾åƒã€‚

    ç¤ºä¾‹ï¼š
        >>> trunk = SomeTrunkNetwork()
        >>> neck = SomeNeckNetwork()
        >>> encoder = ImageEncoder(trunk, neck, scalp=1)
        >>> image = torch.randn(1, 3, 224, 224)
        >>> output = encoder(image)
        >>> print(output.keys())
        dict_keys(['vision_features', 'vision_pos_enc', 'backbone_fpn'])
    """

    def __init__(
        self,
        trunk: nn.Module,
        neck: nn.Module,
        scalp: int = 0,
    ):
        """åˆå§‹åŒ– ImageEncoderï¼Œä½¿ç”¨ trunk å’Œ neck ç½‘ç»œè¿›è¡Œç‰¹å¾æå–å’Œç»†åŒ–ã€‚"""
        super().__init__()
        self.trunk = trunk
        self.neck = neck
        self.scalp = scalp
        assert self.trunk.channel_list == self.neck.backbone_channel_list, (
            f"trunk {self.trunk.channel_list} å’Œ neck {self.neck.backbone_channel_list} çš„é€šé“ç»´åº¦ä¸åŒ¹é…ã€‚"
        )

    def forward(self, sample: torch.Tensor):
        """é€šè¿‡è¡¥ä¸åµŒå…¥ã€ä½ç½®ç¼–ç ã€transformer å—å’Œ neck æ¨¡å—ç¼–ç è¾“å…¥ã€‚"""
        features, pos = self.neck(self.trunk(sample))
        if self.scalp > 0:
            # ä¸¢å¼ƒæœ€ä½åˆ†è¾¨ç‡çš„ç‰¹å¾
            features, pos = features[: -self.scalp], pos[: -self.scalp]

        src = features[-1]
        return {
            "vision_features": src,
            "vision_pos_enc": pos,
            "backbone_fpn": features,
        }


class FpnNeck(nn.Module):
    """
    ç”¨äºç›®æ ‡æ£€æµ‹æ¨¡å‹ä¸­çš„å¤šå°ºåº¦ç‰¹å¾èåˆçš„ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆFPNï¼‰å˜ä½“ã€‚

    è¯¥ FPN å˜ä½“å»é™¤äº†è¾“å‡ºå·ç§¯ï¼Œå¹¶ä½¿ç”¨åŒä¸‰æ¬¡æ’å€¼è¿›è¡Œç‰¹å¾è°ƒæ•´ï¼Œ
    ç±»ä¼¼äº ViT çš„ä½ç½®ç¼–ç æ’å€¼ã€‚

    å±æ€§ï¼š
        position_encoding (PositionEmbeddingSine): æ­£å¼¦ä½ç½®ç¼–ç æ¨¡å—ã€‚
        convs (nn.ModuleList): æ¯ä¸ªéª¨å¹²ç½‘ç»œçº§åˆ«çš„å·ç§¯å±‚åˆ—è¡¨ã€‚
        backbone_channel_list (List[int]): æ¥è‡ªéª¨å¹²ç½‘ç»œçš„é€šé“ç»´åº¦åˆ—è¡¨ã€‚
        fpn_interp_model (str): FPN ç‰¹å¾è°ƒæ•´çš„æ’å€¼æ¨¡å¼ã€‚
        fuse_type (str): ç‰¹å¾èåˆçš„ç±»å‹ï¼Œ'sum' æˆ– 'avg'ã€‚
        fpn_top_down_levels (List[int]): è¾“å‡ºä¸­åŒ…å«è‡ªä¸Šè€Œä¸‹ç‰¹å¾çš„å±‚çº§ã€‚

    æ–¹æ³•ï¼š
        forward: æ‰§è¡Œ FPN neck çš„å‰å‘ä¼ æ’­ã€‚

    ç¤ºä¾‹ï¼š
        >>> backbone_channels = [64, 128, 256, 512]
        >>> fpn_neck = FpnNeck(256, backbone_channels)
        >>> inputs = [torch.rand(1, c, 32, 32) for c in backbone_channels]
        >>> outputs, positions = fpn_neck(inputs)
        >>> print(len(outputs), len(positions))
        4 4
    """

    def __init__(
        self,
        d_model: int,
        backbone_channel_list: List[int],
        kernel_size: int = 1,
        stride: int = 1,
        padding: int = 0,
        fpn_interp_model: str = "bilinear",
        fuse_type: str = "sum",
        fpn_top_down_levels: Optional[List[int]] = None,
    ):
        """
        åˆå§‹åŒ–ä¸€ä¸ªä¿®æ”¹ç‰ˆçš„ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆFPNï¼‰é¢ˆéƒ¨æ¨¡å—ã€‚

        è¿™ä¸ª FPN å˜ä½“å»é™¤äº†è¾“å‡ºå·ç§¯ï¼Œé‡‡ç”¨äº†ç±»ä¼¼äº ViT ä½ç½®åµŒå…¥æ’å€¼çš„åŒä¸‰æ¬¡æ’å€¼æ¥è¿›è¡Œç‰¹å¾å¤§å°è°ƒæ•´ã€‚

        å‚æ•°:
            d_model (int): æ¨¡å‹çš„ç»´åº¦ã€‚
            backbone_channel_list (List[int]): æ¥è‡ªä¸»å¹²ç½‘ç»œçš„é€šé“ç»´åº¦åˆ—è¡¨ã€‚
            kernel_size (int): å·ç§¯å±‚çš„æ ¸å¤§å°ã€‚
            stride (int): å·ç§¯å±‚çš„æ­¥å¹…ã€‚
            padding (int): å·ç§¯å±‚çš„å¡«å……ã€‚
            fpn_interp_model (str): FPN ç‰¹å¾è°ƒæ•´çš„æ’å€¼æ¨¡å¼ã€‚
            fuse_type (str): ç‰¹å¾èåˆçš„ç±»å‹ï¼Œå¯ä»¥æ˜¯ 'sum' æˆ– 'avg'ã€‚
            fpn_top_down_levels (Optional[List[int]]): è¾“å‡ºä¸­å…·æœ‰è‡ªé¡¶å‘ä¸‹ç‰¹å¾çš„çº§åˆ«ã€‚

        ç¤ºä¾‹:
            >>> backbone_channels = [64, 128, 256, 512]
            >>> fpn_neck = FpnNeck(256, backbone_channels)
            >>> print(fpn_neck)
        """
        super().__init__()
        self.position_encoding = PositionEmbeddingSine(num_pos_feats=256)
        self.convs = nn.ModuleList()
        self.backbone_channel_list = backbone_channel_list
        for dim in backbone_channel_list:
            current = nn.Sequential()
            current.add_module(
                "conv",
                nn.Conv2d(
                    in_channels=dim,
                    out_channels=d_model,
                    kernel_size=kernel_size,
                    stride=stride,
                    padding=padding,
                ),
            )

            self.convs.append(current)
        self.fpn_interp_model = fpn_interp_model
        assert fuse_type in {"sum", "avg"}
        self.fuse_type = fuse_type

        # è¾“å‡ºä¸­å…·æœ‰è‡ªé¡¶å‘ä¸‹ç‰¹å¾çš„çº§åˆ«
        # ä¾‹å¦‚ï¼Œå¦‚æœ fpn_top_down_levels æ˜¯ [2, 3]ï¼Œåˆ™åªæœ‰çº§åˆ« 2 å’Œ 3 çš„è¾“å‡º
        # æ‰ä¼šè¿›è¡Œè‡ªé¡¶å‘ä¸‹ä¼ æ’­ï¼Œè€Œçº§åˆ« 0 å’Œ çº§åˆ« 1 ä»…åŒ…å«æ¥è‡ªåŒä¸€ä¸»å¹²çº§åˆ«çš„æ¨ªå‘ç‰¹å¾ã€‚
        if fpn_top_down_levels is None:
            # é»˜è®¤æƒ…å†µä¸‹ï¼Œæ‰€æœ‰çº§åˆ«éƒ½ä¼šæœ‰è‡ªé¡¶å‘ä¸‹ç‰¹å¾
            fpn_top_down_levels = range(len(self.convs))
        self.fpn_top_down_levels = list(fpn_top_down_levels)

    def forward(self, xs: List[torch.Tensor]):
        """
        æ‰§è¡Œé€šè¿‡ç‰¹å¾é‡‘å­—å¡”ç½‘ç»œï¼ˆFPNï¼‰é¢ˆéƒ¨çš„å‰å‘ä¼ æ’­ã€‚

        è¯¥æ–¹æ³•é€šè¿‡ FPN å¤„ç†æ¥è‡ªä¸»å¹²ç½‘ç»œçš„è¾“å…¥å¼ é‡åˆ—è¡¨ï¼Œåº”ç”¨æ¨ªå‘è¿æ¥å’Œè‡ªé¡¶å‘ä¸‹çš„ç‰¹å¾èåˆã€‚
        å®ƒç”Ÿæˆè¾“å‡ºç‰¹å¾å›¾å’Œç›¸åº”çš„ä½ç½®ä¿¡æ¯ç¼–ç ã€‚

        å‚æ•°:
            xs (List[torch.Tensor]): æ¥è‡ªä¸»å¹²ç½‘ç»œçš„è¾“å…¥å¼ é‡åˆ—è¡¨ï¼Œæ¯ä¸ªå¼ é‡çš„å½¢çŠ¶ä¸º (B, C, H, W)ã€‚

        è¿”å›:
            (Tuple[List[torch.Tensor], List[torch.Tensor]]): ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«ï¼š
                - out (List[torch.Tensor]): ç» FPN å¤„ç†åçš„è¾“å‡ºç‰¹å¾å›¾åˆ—è¡¨ï¼Œæ¯ä¸ªå¼ é‡çš„å½¢çŠ¶ä¸º
                  (B, d_model, H, W)ã€‚
                - pos (List[torch.Tensor]): å¯¹åº”æ¯ä¸ªè¾“å‡ºç‰¹å¾å›¾çš„ä½ç½®ä¿¡æ¯ç¼–ç ã€‚

        ç¤ºä¾‹:
            >>> fpn_neck = FpnNeck(d_model=256, backbone_channel_list=[64, 128, 256, 512])
            >>> inputs = [torch.rand(1, c, 32, 32) for c in [64, 128, 256, 512]]
            >>> outputs, positions = fpn_neck(inputs)
            >>> print(len(outputs), len(positions))
            4 4
        """
        out = [None] * len(self.convs)
        pos = [None] * len(self.convs)
        assert len(xs) == len(self.convs)
        # fpn å‰å‘ä¼ æ’­
        # å‚è€ƒ https://github.com/facebookresearch/detectron2/blob/main/detectron2/modeling/backbone/fpn.py
        prev_features = None
        # æŒ‰ç…§è‡ªé¡¶å‘ä¸‹çš„é¡ºåºè¿›è¡Œå‰å‘ä¼ æ’­ï¼ˆä»ä½åˆ†è¾¨ç‡åˆ°é«˜åˆ†è¾¨ç‡ï¼‰
        n = len(self.convs) - 1
        for i in range(n, -1, -1):
            x = xs[i]
            lateral_features = self.convs[n - i](x)
            if i in self.fpn_top_down_levels and prev_features is not None:
                top_down_features = F.interpolate(
                    prev_features.to(dtype=torch.float32),
                    scale_factor=2.0,
                    mode=self.fpn_interp_model,
                    align_corners=(None if self.fpn_interp_model == "nearest" else False),
                    antialias=False,
                )
                prev_features = lateral_features + top_down_features
                if self.fuse_type == "avg":
                    prev_features /= 2
            else:
                prev_features = lateral_features
            x_out = prev_features
            out[i] = x_out
            pos[i] = self.position_encoding(x_out).to(x_out.dtype)

        return out, pos
1

class Hiera(nn.Module):
    """
    ç”¨äºå›¾åƒå¤„ç†ä»»åŠ¡çš„é«˜æ•ˆå¤šå°ºåº¦ç‰¹å¾æå–çš„åˆ†å±‚è§†è§‰ Transformerã€‚

    è¯¥ç±»å®ç°äº† Hiera æ¨¡å‹ï¼Œè¿™æ˜¯ä¸€ç§åˆ†å±‚è§†è§‰ Transformer æ¶æ„ï¼Œæ—¨åœ¨é«˜æ•ˆåœ°è¿›è¡Œå¤šå°ºåº¦ç‰¹å¾æå–ã€‚
    å®ƒä½¿ç”¨ä¸€ç³»åˆ— Transformer å—ï¼Œå¹¶å°†è¿™äº›å—ç»„ç»‡æˆå¤šä¸ªé˜¶æ®µï¼ŒåŒæ—¶æ”¯æŒå¯é€‰çš„æ± åŒ–å’Œå…¨å±€æ³¨æ„åŠ›æœºåˆ¶ã€‚

    å±æ€§è¯´æ˜:
        window_spec (Tuple[int, ...]): æ¯ä¸ªé˜¶æ®µçš„çª—å£å¤§å°ã€‚
        q_stride (Tuple[int, int]): é˜¶æ®µé—´çš„ä¸‹é‡‡æ ·æ­¥å¹…ã€‚
        stage_ends (List[int]): æ¯ä¸ªé˜¶æ®µä¸­æœ€åä¸€ä¸ªå—çš„ç´¢å¼•ã€‚
        q_pool_blocks (List[int]): è¿›è¡Œæ± åŒ–æ“ä½œçš„å—çš„ç´¢å¼•ã€‚
        return_interm_layers (bool): æ˜¯å¦è¿”å›æ¯ä¸ªé˜¶æ®µçš„ä¸­é—´å±‚è¾“å‡ºã€‚
        patch_embed (PatchEmbed): è¡¥ä¸åµŒå…¥æ¨¡å—ã€‚
        global_att_blocks (Tuple[int, ...]): å«æœ‰å…¨å±€æ³¨æ„åŠ›çš„å—çš„ç´¢å¼•ã€‚
        window_pos_embed_bkg_spatial_size (Tuple[int, int]): çª—å£ä½ç½®åµŒå…¥èƒŒæ™¯çš„ç©ºé—´å¤§å°ã€‚
        pos_embed (nn.Parameter): èƒŒæ™¯çš„ä½ç½®ä¿¡æ¯åµŒå…¥ã€‚
        pos_embed_window (nn.Parameter): çª—å£çš„ä½ç½®ä¿¡æ¯åµŒå…¥ã€‚
        blocks (nn.ModuleList): MultiScaleBlock æ¨¡å—çš„åˆ—è¡¨ã€‚
        channel_list (List[int]): æ¯ä¸ªé˜¶æ®µçš„è¾“å‡ºé€šé“ç»´åº¦åˆ—è¡¨ã€‚

    æ–¹æ³•è¯´æ˜:
        _get_pos_embed: é€šè¿‡æ’å€¼å¹¶ç»„åˆçª—å£å’ŒèƒŒæ™¯åµŒå…¥æ¥ç”Ÿæˆä½ç½®åµŒå…¥ã€‚
        forward: é€šè¿‡ Hiera æ¨¡å‹æ‰§è¡Œå‰å‘ä¼ æ’­ã€‚

    ç¤ºä¾‹ç”¨æ³•:
        >>> model = Hiera(embed_dim=96, num_heads=1, stages=(2, 3, 16, 3))
        >>> input_tensor = torch.randn(1, 3, 224, 224)
        >>> output_features = model(input_tensor)
        >>> for feat in output_features:
        ...     print(feat.shape)
    """

    def __init__(
        self,
        embed_dim: int = 96,  # åˆå§‹åµŒå…¥ç»´åº¦
        num_heads: int = 1,  # åˆå§‹å¤´æ•°
        drop_path_rate: float = 0.0,  # éšæœºæ·±åº¦
        q_pool: int = 3,  # q_pool é˜¶æ®µçš„æ•°é‡
        q_stride: Tuple[int, int] = (2, 2),  # é˜¶æ®µé—´çš„ä¸‹é‡‡æ ·æ­¥å¹…
        stages: Tuple[int, ...] = (2, 3, 16, 3),  # æ¯ä¸ªé˜¶æ®µçš„å—æ•°é‡
        dim_mul: float = 2.0,  # é˜¶æ®µåˆ‡æ¢æ—¶çš„ç»´åº¦å€æ•°
        head_mul: float = 2.0,  # é˜¶æ®µåˆ‡æ¢æ—¶çš„å¤´æ•°å€æ•°
        window_pos_embed_bkg_spatial_size: Tuple[int, int] = (14, 14),
        # æ¯ä¸ªé˜¶æ®µçš„çª—å£å¤§å°ï¼Œè‹¥ä¸ä½¿ç”¨å…¨å±€æ³¨æ„åŠ›æ—¶
        window_spec: Tuple[int, ...] = (
            8,
            4,
            14,
            7,
        ),
        # åœ¨è¿™äº›å—ä¸­ä½¿ç”¨å…¨å±€æ³¨æ„åŠ›
        global_att_blocks: Tuple[int, ...] = (
            12,
            16,
            20,
        ),
        return_interm_layers=True,  # æ˜¯å¦è¿”å›æ¯ä¸ªé˜¶æ®µçš„ç‰¹å¾
    ):
        """åˆå§‹åŒ– Hiera æ¨¡å‹ï¼Œé…ç½®å…¶åˆ†å±‚è§†è§‰ Transformer æ¶æ„ã€‚"""
        super().__init__()

        assert len(stages) == len(window_spec)
        self.window_spec = window_spec

        depth = sum(stages)
        self.q_stride = q_stride
        self.stage_ends = [sum(stages[:i]) - 1 for i in range(1, len(stages) + 1)]
        assert 0 <= q_pool <= len(self.stage_ends[:-1])
        self.q_pool_blocks = [x + 1 for x in self.stage_ends[:-1]][:q_pool]
        self.return_interm_layers = return_interm_layers

        self.patch_embed = PatchEmbed(
            embed_dim=embed_dim,
            kernel_size=(7, 7),
            stride=(4, 4),
            padding=(3, 3),
        )
        # å“ªäº›å—ä½¿ç”¨å…¨å±€æ³¨æ„åŠ›ï¼Ÿ
        self.global_att_blocks = global_att_blocks

        # çª—å£ä½ç½®åµŒå…¥ï¼ˆhttps://arxiv.org/abs/2311.05613ï¼‰
        self.window_pos_embed_bkg_spatial_size = window_pos_embed_bkg_spatial_size
        self.pos_embed = nn.Parameter(torch.zeros(1, embed_dim, *self.window_pos_embed_bkg_spatial_size))
        self.pos_embed_window = nn.Parameter(torch.zeros(1, embed_dim, self.window_spec[0], self.window_spec[0]))

        dpr = [x.item() for x in torch.linspace(0, drop_path_rate, depth)]  # éšæœºæ·±åº¦è¡°å‡è§„åˆ™

        cur_stage = 1
        self.blocks = nn.ModuleList()

        for i in range(depth):
            dim_out = embed_dim
            # å½“å‰å—æ¯”å‰ä¸€é˜¶æ®µæ»åä¸€ä¸ªï¼Œå› æ­¤ä¸‹ä¸€é˜¶æ®µçš„ç¬¬ä¸€ä¸ªå—ä½¿ç”¨å‰ä¸€é˜¶æ®µçš„åˆå§‹çª—å£å¤§å°
            # å’Œå½“å‰é˜¶æ®µçš„æœ€ç»ˆçª—å£å¤§å°
            window_size = self.window_spec[cur_stage - 1]

            if self.global_att_blocks is not None:
                window_size = 0 if i in self.global_att_blocks else window_size

            if i - 1 in self.stage_ends:
                dim_out = int(embed_dim * dim_mul)
                num_heads = int(num_heads * head_mul)
                cur_stage += 1

            block = MultiScaleBlock(
                dim=embed_dim,
                dim_out=dim_out,
                num_heads=num_heads,
                drop_path=dpr[i],
                q_stride=self.q_stride if i in self.q_pool_blocks else None,
                window_size=window_size,
            )

            embed_dim = dim_out
            self.blocks.append(block)

        self.channel_list = (
            [self.blocks[i].dim_out for i in self.stage_ends[::-1]]
            if return_interm_layers
            else [self.blocks[-1].dim_out]
        )

    def _get_pos_embed(self, hw: Tuple[int, int]) -> torch.Tensor:
        """é€šè¿‡æ’å€¼å¹¶ç»„åˆçª—å£å’ŒèƒŒæ™¯åµŒå…¥æ¥ç”Ÿæˆä½ç½®åµŒå…¥ã€‚"""
        h, w = hw
        window_embed = self.pos_embed_window
        pos_embed = F.interpolate(self.pos_embed, size=(h, w), mode="bicubic")
        pos_embed = pos_embed + window_embed.tile([x // y for x, y in zip(pos_embed.shape, window_embed.shape)])
        pos_embed = pos_embed.permute(0, 2, 3, 1)
        return pos_embed

    def forward(self, x: torch.Tensor) -> List[torch.Tensor]:
        """é€šè¿‡ Hiera æ¨¡å‹æ‰§è¡Œå‰å‘ä¼ æ’­ï¼Œä»è¾“å…¥å›¾åƒä¸­æå–å¤šå°ºåº¦ç‰¹å¾ã€‚"""
        x = self.patch_embed(x)
        # x: (B, H, W, C)

        # æ·»åŠ ä½ç½®åµŒå…¥
        x = x + self._get_pos_embed(x.shape[1:3])

        outputs = []
        for i, blk in enumerate(self.blocks):
            x = blk(x)
            if (i == self.stage_ends[-1]) or (i in self.stage_ends and self.return_interm_layers):
                feats = x.permute(0, 3, 1, 2)
                outputs.append(feats)

        return outputs

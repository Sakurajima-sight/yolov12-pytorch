# Ultralytics ğŸš€ AGPL-3.0 License - https://ultralytics.com/license
"""
ä½¿ç”¨Segment Anything Model (SAM)ç”Ÿæˆé¢„æµ‹ã€‚

SAMæ˜¯ä¸€ä¸ªå…ˆè¿›çš„å›¾åƒåˆ†å‰²æ¨¡å‹ï¼Œæä¾›è¯¸å¦‚å¯æç¤ºåˆ†å‰²å’Œé›¶-shotæ€§èƒ½ç­‰åŠŸèƒ½ã€‚
æ­¤æ¨¡å—åŒ…å«äº†æ‰§è¡Œåˆ†å‰²æ‰€éœ€çš„é¢„æµ‹é€»è¾‘å’Œè¾…åŠ©å·¥å…·ã€‚
å®ƒæ˜¯Ultralyticsæ¡†æ¶çš„æ ¸å¿ƒéƒ¨åˆ†ï¼Œæ—¨åœ¨é«˜æ€§èƒ½ã€å®æ—¶å›¾åƒåˆ†å‰²ä»»åŠ¡ä¸­ä½¿ç”¨ã€‚
"""

from collections import OrderedDict

import numpy as np
import torch
import torch.nn.functional as F

from ultralytics.data.augment import LetterBox
from ultralytics.engine.predictor import BasePredictor
from ultralytics.engine.results import Results
from ultralytics.utils import DEFAULT_CFG, ops
from ultralytics.utils.torch_utils import select_device, smart_inference_mode

from .amg import (
    batch_iterator,
    batched_mask_to_box,
    build_all_layer_point_grids,
    calculate_stability_score,
    generate_crop_boxes,
    is_box_near_crop_edge,
    remove_small_regions,
    uncrop_boxes_xyxy,
    uncrop_masks,
)
from .build import build_sam


class Predictor(BasePredictor):
    """
    SAMçš„é¢„æµ‹å™¨ç±»ï¼Œæ”¯æŒå…·æœ‰æç¤ºèƒ½åŠ›çš„å®æ—¶å›¾åƒåˆ†å‰²ã€‚

    è¯¥ç±»æ‰©å±•äº†BasePredictorå¹¶å®ç°äº†Segment Anything Model (SAM)ï¼Œç”¨äºå…ˆè¿›çš„å›¾åƒ
    åˆ†å‰²ä»»åŠ¡ã€‚å®ƒæ”¯æŒå¤šç§è¾“å…¥æç¤ºï¼Œå¦‚ç‚¹ã€è¾¹ç•Œæ¡†å’Œæ©ç ï¼Œä»¥ä¾¿å¯¹åˆ†å‰²ç»“æœè¿›è¡Œç»†ç²’åº¦æ§åˆ¶ã€‚

    å±æ€§ï¼š
        args (SimpleNamespace): é¢„æµ‹å™¨çš„é…ç½®å‚æ•°ã€‚
        model (torch.nn.Module): åŠ è½½çš„SAMæ¨¡å‹ã€‚
        device (torch.device): æ¨¡å‹åŠ è½½çš„è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰ã€‚
        im (torch.Tensor): é¢„å¤„ç†åçš„è¾“å…¥å›¾åƒã€‚
        features (torch.Tensor): æå–çš„å›¾åƒç‰¹å¾ã€‚
        prompts (Dict): å­˜å‚¨å„ç§ç±»å‹æç¤ºï¼ˆä¾‹å¦‚ï¼Œè¾¹ç•Œæ¡†ã€ç‚¹ã€æ©ç ï¼‰çš„å­—å…¸ã€‚
        segment_all (bool): æ ‡è®°æ˜¯å¦è¿›è¡Œå…¨å›¾åˆ†å‰²ã€‚
        mean (torch.Tensor): å›¾åƒå½’ä¸€åŒ–çš„å‡å€¼ã€‚
        std (torch.Tensor): å›¾åƒå½’ä¸€åŒ–çš„æ ‡å‡†å·®ã€‚

    æ–¹æ³•ï¼š
        preprocess: å‡†å¤‡è¾“å…¥å›¾åƒä»¥è¿›è¡Œæ¨¡å‹æ¨ç†ã€‚
        pre_transform: å¯¹è¾“å…¥å›¾åƒè¿›è¡Œåˆæ­¥è½¬æ¢ã€‚
        inference: æ ¹æ®è¾“å…¥æç¤ºæ‰§è¡Œåˆ†å‰²æ¨ç†ã€‚
        prompt_inference: ç”¨äºåŸºäºæç¤ºçš„åˆ†å‰²æ¨ç†çš„å†…éƒ¨å‡½æ•°ã€‚
        generate: ç”Ÿæˆæ•´ä¸ªå›¾åƒçš„åˆ†å‰²æ©ç ã€‚
        setup_model: åˆå§‹åŒ–SAMæ¨¡å‹ä»¥è¿›è¡Œæ¨ç†ã€‚
        get_model: æ„å»ºå¹¶è¿”å›ä¸€ä¸ªSAMæ¨¡å‹ã€‚
        postprocess: åå¤„ç†æ¨¡å‹è¾“å‡ºä»¥ç”Ÿæˆæœ€ç»ˆç»“æœã€‚
        setup_source: è®¾ç½®æ¨ç†çš„æ•°æ®æºã€‚
        set_image: è®¾ç½®å¹¶é¢„å¤„ç†å•å¼ å›¾åƒä»¥è¿›è¡Œæ¨ç†ã€‚
        get_im_features: ä½¿ç”¨SAMå›¾åƒç¼–ç å™¨æå–å›¾åƒç‰¹å¾ã€‚
        set_prompts: è®¾ç½®åç»­æ¨ç†çš„æç¤ºã€‚
        reset_image: é‡ç½®å½“å‰å›¾åƒåŠå…¶ç‰¹å¾ã€‚
        remove_small_regions: ç§»é™¤æ©ç ä¸­å°çš„å­¤ç«‹åŒºåŸŸå’Œå­”æ´ã€‚

    ç¤ºä¾‹ï¼š
        >>> predictor = Predictor()
        >>> predictor.setup_model(model_path="sam_model.pt")
        >>> predictor.set_image("image.jpg")
        >>> bboxes = [[100, 100, 200, 200]]
        >>> results = predictor(bboxes=bboxes)
    """

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """
        ä½¿ç”¨é…ç½®ã€è¦†ç›–é¡¹å’Œå›è°ƒå‡½æ•°åˆå§‹åŒ–é¢„æµ‹å™¨ã€‚

        ä¸ºSAMï¼ˆSegment Anything Modelï¼‰è®¾ç½®é¢„æµ‹å™¨å¯¹è±¡ï¼Œå¹¶åº”ç”¨æä¾›çš„ä»»ä½•é…ç½®è¦†ç›–é¡¹æˆ–
        å›è°ƒå‡½æ•°ã€‚åˆå§‹åŒ–SAMç‰¹å®šçš„è®¾ç½®ï¼Œä¾‹å¦‚å°†retina_masksè®¾ç½®ä¸ºTrueï¼Œä»¥è·å¾—æœ€ä½³ç»“æœã€‚

        å‚æ•°ï¼š
            cfg (Dict): åŒ…å«é»˜è®¤è®¾ç½®çš„é…ç½®å­—å…¸ã€‚
            overrides (Dict | None): è¦†ç›–é»˜è®¤é…ç½®çš„å€¼çš„å­—å…¸ã€‚
            _callbacks (Dict | None): ç”¨äºè‡ªå®šä¹‰è¡Œä¸ºçš„å›è°ƒå‡½æ•°å­—å…¸ã€‚

        ç¤ºä¾‹ï¼š
            >>> predictor_example = Predictor(cfg=DEFAULT_CFG)
            >>> predictor_example_with_imgsz = Predictor(overrides={"imgsz": 640})
            >>> predictor_example_with_callback = Predictor(_callbacks={"on_predict_start": custom_callback})
        """
        if overrides is None:
            overrides = {}
        overrides.update(dict(task="segment", mode="predict", batch=1))
        super().__init__(cfg, overrides, _callbacks)
        self.args.retina_masks = True
        self.im = None
        self.features = None
        self.prompts = {}
        self.segment_all = False

    def preprocess(self, im):
        """
        ä¸ºæ¨¡å‹æ¨ç†é¢„å¤„ç†è¾“å…¥å›¾åƒã€‚

        æ­¤æ–¹æ³•é€šè¿‡åº”ç”¨è½¬æ¢å’Œå½’ä¸€åŒ–å‡†å¤‡è¾“å…¥å›¾åƒã€‚å®ƒæ”¯æŒtorch.Tensorå’Œnp.ndarrayåˆ—è¡¨ä½œä¸ºè¾“å…¥æ ¼å¼ã€‚

        å‚æ•°ï¼š
            im (torch.Tensor | List[np.ndarray]): è¾“å…¥å›¾åƒï¼Œä»¥BCHWå¼ é‡æ ¼å¼æˆ–HWCçš„numpyæ•°ç»„åˆ—è¡¨å½¢å¼ã€‚

        è¿”å›ï¼š
            im (torch.Tensor): é¢„å¤„ç†åçš„å›¾åƒå¼ é‡ï¼Œç»è¿‡å½’ä¸€åŒ–å¹¶è½¬æ¢ä¸ºé€‚å½“çš„æ•°æ®ç±»å‹ã€‚

        ç¤ºä¾‹ï¼š
            >>> predictor = Predictor()
            >>> image = torch.rand(1, 3, 640, 640)
            >>> preprocessed_image = predictor.preprocess(image)
        """
        if self.im is not None:
            return self.im
        not_tensor = not isinstance(im, torch.Tensor)
        if not_tensor:
            im = np.stack(self.pre_transform(im))
            im = im[..., ::-1].transpose((0, 3, 1, 2))
            im = np.ascontiguousarray(im)
            im = torch.from_numpy(im)

        im = im.to(self.device)
        im = im.half() if self.model.fp16 else im.float()
        if not_tensor:
            im = (im - self.mean) / self.std
        return im

    def pre_transform(self, im):
        """
        å¯¹è¾“å…¥å›¾åƒè¿›è¡Œåˆæ­¥è½¬æ¢ä»¥è¿›è¡Œé¢„å¤„ç†ã€‚

        æ­¤æ–¹æ³•åº”ç”¨è¯¸å¦‚è°ƒæ•´å¤§å°ç­‰è½¬æ¢ï¼Œä»¥å‡†å¤‡å›¾åƒè¿›è¡Œè¿›ä¸€æ­¥é¢„å¤„ç†ã€‚
        å½“å‰ä¸æ”¯æŒæ‰¹é‡æ¨ç†ï¼Œå› æ­¤åˆ—è¡¨çš„é•¿åº¦åº”è¯¥ä¸º1ã€‚

        å‚æ•°ï¼š
            im (List[np.ndarray]): åŒ…å«å•å¼ å›¾åƒçš„åˆ—è¡¨ï¼Œå›¾åƒä¸ºHWCæ ¼å¼çš„numpyæ•°ç»„ã€‚

        è¿”å›ï¼š
            (List[np.ndarray]): åŒ…å«è½¬æ¢åå›¾åƒçš„åˆ—è¡¨ã€‚

        å¼‚å¸¸ï¼š
            AssertionError: å¦‚æœè¾“å…¥åˆ—è¡¨åŒ…å«å¤šäºä¸€å¼ å›¾åƒã€‚

        ç¤ºä¾‹ï¼š
            >>> predictor = Predictor()
            >>> image = np.random.rand(480, 640, 3)  # å•å¼ HWCå›¾åƒ
            >>> transformed = predictor.pre_transform([image])
            >>> print(len(transformed))
            1
        """
        assert len(im) == 1, "SAMæ¨¡å‹å½“å‰ä¸æ”¯æŒæ‰¹é‡æ¨ç†"
        letterbox = LetterBox(self.args.imgsz, auto=False, center=False)
        return [letterbox(image=x) for x in im]

    def inference(self, im, bboxes=None, points=None, labels=None, masks=None, multimask_output=False, *args, **kwargs):
        """
        åŸºäºç»™å®šçš„è¾“å…¥æç¤ºï¼Œä½¿ç”¨å½“å‰åŠ è½½çš„å›¾åƒè¿›è¡Œå›¾åƒåˆ†å‰²æ¨ç†ã€‚

        æ­¤æ–¹æ³•åˆ©ç”¨SAMï¼ˆSegment Anything Modelï¼‰æ¶æ„ï¼ŒåŒ…æ‹¬å›¾åƒç¼–ç å™¨ã€æç¤ºç¼–ç å™¨å’Œæ©ç è§£ç å™¨ï¼Œè¿›è¡Œå®æ—¶å’ŒåŸºäºæç¤ºçš„åˆ†å‰²ä»»åŠ¡ã€‚

        å‚æ•°ï¼š
            im (torch.Tensor): é¢„å¤„ç†åçš„è¾“å…¥å›¾åƒï¼Œtensoræ ¼å¼ï¼Œå½¢çŠ¶ä¸º(N, C, H, W)ã€‚
            bboxes (np.ndarray | List | None): è¾¹ç•Œæ¡†ï¼Œå½¢çŠ¶ä¸º(N, 4)ï¼ŒXYXYæ ¼å¼ã€‚
            points (np.ndarray | List | None): è¡¨ç¤ºç‰©ä½“ä½ç½®çš„ç‚¹ï¼Œå½¢çŠ¶ä¸º(N, 2)ï¼Œå•ä½ä¸ºåƒç´ ã€‚
            labels (np.ndarray | List | None): ç‚¹æç¤ºçš„æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(N,)ï¼Œ1è¡¨ç¤ºå‰æ™¯ï¼Œ0è¡¨ç¤ºèƒŒæ™¯ã€‚
            masks (np.ndarray | None): æ¥è‡ªå…ˆå‰é¢„æµ‹çš„ä½åˆ†è¾¨ç‡æ©ç ï¼Œå½¢çŠ¶ä¸º(N, H, W)ï¼Œå¯¹äºSAMæ¥è¯´ï¼ŒH=W=256ã€‚
            multimask_output (bool): æ˜¯å¦è¿”å›å¤šä¸ªæ©ç ã€‚å¯¹æ¨¡ç³Šçš„æç¤ºéå¸¸æœ‰ç”¨ã€‚
            *args (Any): å…¶ä»–ä½ç½®å‚æ•°ã€‚
            **kwargs (Any): å…¶ä»–å…³é”®å­—å‚æ•°ã€‚

        è¿”å›ï¼š
            (np.ndarray): è¾“å‡ºæ©ç ï¼Œå½¢çŠ¶ä¸º(C, H, W)ï¼Œå…¶ä¸­Cæ˜¯ç”Ÿæˆçš„æ©ç æ•°é‡ã€‚
            (np.ndarray): é•¿åº¦ä¸ºCçš„æ•°ç»„ï¼ŒåŒ…å«æ¨¡å‹ä¸ºæ¯ä¸ªæ©ç é¢„æµ‹çš„è´¨é‡åˆ†æ•°ã€‚
            (np.ndarray): ä½åˆ†è¾¨ç‡çš„logitsï¼Œå½¢çŠ¶ä¸º(C, H, W)ï¼Œç”¨äºåç»­æ¨ç†ï¼ŒH=W=256ã€‚

        ç¤ºä¾‹ï¼š
            >>> predictor = Predictor()
            >>> predictor.setup_model(model_path="sam_model.pt")
            >>> predictor.set_image("image.jpg")
            >>> results = predictor(bboxes=[[0, 0, 100, 100]])
        """
        # å¦‚æœæœ‰ä»»ä½•æç¤ºå­˜å‚¨åœ¨self.promptsä¸­ï¼Œåˆ™è¦†ç›–å®ƒä»¬
        bboxes = self.prompts.pop("bboxes", bboxes)
        points = self.prompts.pop("points", points)
        masks = self.prompts.pop("masks", masks)
        labels = self.prompts.pop("labels", labels)

        if all(i is None for i in [bboxes, points, masks]):
            return self.generate(im, *args, **kwargs)

        return self.prompt_inference(im, bboxes, points, labels, masks, multimask_output)

    def prompt_inference(self, im, bboxes=None, points=None, labels=None, masks=None, multimask_output=False):
        """
        åŸºäºè¾“å…¥æç¤ºä½¿ç”¨SAMçš„ä¸“ç”¨æ¶æ„æ‰§è¡Œå›¾åƒåˆ†å‰²æ¨ç†ã€‚

        æ­¤å†…éƒ¨å‡½æ•°åˆ©ç”¨Segment Anything Modelï¼ˆSAMï¼‰è¿›è¡ŒåŸºäºæç¤ºçš„å®æ—¶åˆ†å‰²ã€‚
        å®ƒå¤„ç†å„ç§è¾“å…¥æç¤ºï¼Œå¦‚è¾¹ç•Œæ¡†ã€ç‚¹å’Œæ©ç ï¼Œä»¥ç”Ÿæˆåˆ†å‰²æ©ç ã€‚

        å‚æ•°ï¼š
            im (torch.Tensor): é¢„å¤„ç†åçš„è¾“å…¥å›¾åƒtensorï¼Œå½¢çŠ¶ä¸º(N, C, H, W)ã€‚
            bboxes (np.ndarray | List | None): è¾¹ç•Œæ¡†ï¼ŒXYXYæ ¼å¼ï¼Œå½¢çŠ¶ä¸º(N, 4)ã€‚
            points (np.ndarray | List | None): è¡¨ç¤ºç‰©ä½“ä½ç½®çš„ç‚¹ï¼Œå½¢çŠ¶ä¸º(N, 2)æˆ–(N, num_points, 2)ï¼Œå•ä½ä¸ºåƒç´ ã€‚
            labels (np.ndarray | List | None): ç‚¹æç¤ºæ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(N)æˆ–(N, num_points)ã€‚1è¡¨ç¤ºå‰æ™¯ï¼Œ0è¡¨ç¤ºèƒŒæ™¯ã€‚
            masks (np.ndarray | None): æ¥è‡ªå…ˆå‰é¢„æµ‹çš„ä½åˆ†è¾¨ç‡æ©ç ï¼Œå½¢çŠ¶ä¸º(N, H, W)ã€‚å¯¹äºSAMæ¥è¯´ï¼ŒH=W=256ã€‚
            multimask_output (bool): æ˜¯å¦è¿”å›å¤šä¸ªæ©ç ï¼Œé€‚ç”¨äºæ¨¡ç³Šæç¤ºã€‚

        å¼‚å¸¸ï¼š
            AssertionError: å¦‚æœç‚¹çš„æ•°é‡ä¸æ ‡ç­¾çš„æ•°é‡ä¸åŒ¹é…ï¼ˆå½“æ ‡ç­¾è¢«ä¼ é€’æ—¶ï¼‰ã€‚

        è¿”å›ï¼š
            (np.ndarray): è¾“å‡ºæ©ç ï¼Œå½¢çŠ¶ä¸º(C, H, W)ï¼Œå…¶ä¸­Cæ˜¯ç”Ÿæˆçš„æ©ç æ•°é‡ã€‚
            (np.ndarray): æ¨¡å‹ä¸ºæ¯ä¸ªæ©ç é¢„æµ‹çš„è´¨é‡åˆ†æ•°ï¼Œé•¿åº¦ä¸ºCã€‚

        ç¤ºä¾‹ï¼š
            >>> predictor = Predictor()
            >>> im = torch.rand(1, 3, 1024, 1024)
            >>> bboxes = [[100, 100, 200, 200]]
            >>> masks, scores, logits = predictor.prompt_inference(im, bboxes=bboxes)
        """
        features = self.get_im_features(im) if self.features is None else self.features

        bboxes, points, labels, masks = self._prepare_prompts(im.shape[2:], bboxes, points, labels, masks)
        points = (points, labels) if points is not None else None
        # åµŒå…¥æç¤º
        sparse_embeddings, dense_embeddings = self.model.prompt_encoder(points=points, boxes=bboxes, masks=masks)

        # é¢„æµ‹æ©ç 
        pred_masks, pred_scores = self.model.mask_decoder(
            image_embeddings=features,
            image_pe=self.model.prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=multimask_output,
        )

        # (N, d, H, W) --> (N*d, H, W), (N, d) --> (N*d, )
        # `d` å¯èƒ½æ˜¯1æˆ–3ï¼Œå–å†³äº`multimask_output`ã€‚
        return pred_masks.flatten(0, 1), pred_scores.flatten(0, 1)

    def _prepare_prompts(self, dst_shape, bboxes=None, points=None, labels=None, masks=None):
        """
        æ ¹æ®ç›®æ ‡å½¢çŠ¶å‡†å¤‡å¹¶è½¬æ¢è¾“å…¥çš„æç¤ºä¿¡æ¯ã€‚

        å‚æ•°ï¼š
            dst_shape (tuple): æç¤ºä¿¡æ¯çš„ç›®æ ‡å½¢çŠ¶ï¼ˆé«˜åº¦ï¼Œå®½åº¦ï¼‰ã€‚
            bboxes (np.ndarray | List | None): ä»¥XYXYæ ¼å¼è¡¨ç¤ºçš„è¾¹ç•Œæ¡†ï¼Œå½¢çŠ¶ä¸º(N, 4)ã€‚
            points (np.ndarray | List | None): è¡¨ç¤ºç‰©ä½“ä½ç½®çš„ç‚¹ï¼Œå½¢çŠ¶ä¸º(N, 2)æˆ–(N, num_points, 2)ï¼Œå•ä½ä¸ºåƒç´ ã€‚
            labels (np.ndarray | List | None): ç‚¹æç¤ºæ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(N)æˆ–(N, num_points)ã€‚å‰æ™¯ä¸º1ï¼ŒèƒŒæ™¯ä¸º0ã€‚
            masks (List | np.ndarray, å¯é€‰): ç‰©ä½“çš„æ©è†œï¼Œæ¯ä¸ªæ©è†œæ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ã€‚

        å¼‚å¸¸ï¼š
            AssertionError: å¦‚æœä¼ å…¥æ ‡ç­¾æ—¶ï¼Œç‚¹çš„æ•°é‡ä¸æ ‡ç­¾çš„æ•°é‡ä¸åŒ¹é…ã€‚

        è¿”å›ï¼š
            (tuple): è¿”å›è½¬æ¢åçš„è¾¹ç•Œæ¡†ã€ç‚¹ã€æ ‡ç­¾å’Œæ©è†œã€‚
        """
        src_shape = self.batch[1][0].shape[:2]
        r = 1.0 if self.segment_all else min(dst_shape[0] / src_shape[0], dst_shape[1] / src_shape[1])
        # è½¬æ¢è¾“å…¥æç¤ºä¿¡æ¯
        if points is not None:
            points = torch.as_tensor(points, dtype=torch.float32, device=self.device)
            points = points[None] if points.ndim == 1 else points
            # å‡è®¾å¦‚æœç”¨æˆ·æ²¡æœ‰ä¼ å…¥æ ‡ç­¾ï¼Œåˆ™æ ‡ç­¾éƒ½ä¸ºæ­£æ ·æœ¬ã€‚
            if labels is None:
                labels = np.ones(points.shape[:-1])
            labels = torch.as_tensor(labels, dtype=torch.int32, device=self.device)
            assert points.shape[-2] == labels.shape[-1], (
                f"ç‚¹çš„æ•°é‡ {points.shape[-2]} åº”è¯¥ä¸æ ‡ç­¾çš„æ•°é‡ {labels.shape[-1]} åŒ¹é…ã€‚"
            )
            points *= r
            if points.ndim == 2:
                # (N, 2) --> (N, 1, 2), (N, ) --> (N, 1)
                points, labels = points[:, None, :], labels[:, None]
        if bboxes is not None:
            bboxes = torch.as_tensor(bboxes, dtype=torch.float32, device=self.device)
            bboxes = bboxes[None] if bboxes.ndim == 1 else bboxes
            bboxes *= r
        if masks is not None:
            masks = torch.as_tensor(masks, dtype=torch.float32, device=self.device).unsqueeze(1)
        return bboxes, points, labels, masks

    def generate(
        self,
        im,
        crop_n_layers=0,
        crop_overlap_ratio=512 / 1500,
        crop_downscale_factor=1,
        point_grids=None,
        points_stride=32,
        points_batch_size=64,
        conf_thres=0.88,
        stability_score_thresh=0.95,
        stability_score_offset=0.95,
        crop_nms_thresh=0.7,
    ):
        """
        ä½¿ç”¨Segment Anything Model (SAM)è¿›è¡Œå›¾åƒåˆ†å‰²ã€‚

        è¯¥æ–¹æ³•é€šè¿‡åˆ©ç”¨SAMçš„å…ˆè¿›æ¶æ„å’Œå®æ—¶æ€§èƒ½èƒ½åŠ›ï¼Œå°†æ•´å¼ å›¾åƒåˆ†å‰²ä¸ºç»„æˆéƒ¨åˆ†ã€‚å®ƒè¿˜å¯ä»¥é€‰æ‹©åœ¨å›¾åƒè£å‰ªä¸Šå·¥ä½œï¼Œä»¥å®ç°æ›´ç²¾ç»†çš„åˆ†å‰²ã€‚

        å‚æ•°ï¼š
            im (torch.Tensor): è¾“å…¥å¼ é‡ï¼Œè¡¨ç¤ºé¢„å¤„ç†åçš„å›¾åƒï¼Œå½¢çŠ¶ä¸º(N, C, H, W)ã€‚
            crop_n_layers (int): ç”¨äºå›¾åƒè£å‰ªçš„é¢å¤–æ©è†œé¢„æµ‹çš„å±‚æ•°ã€‚
            crop_overlap_ratio (float): è£å‰ªä¹‹é—´çš„é‡å æ¯”ä¾‹ï¼Œåç»­å±‚ä¼šç¼©å°ã€‚
            crop_downscale_factor (int): æ¯å±‚é‡‡æ ·ç‚¹çš„ç¼©æ”¾å› å­ã€‚
            point_grids (List[np.ndarray] | None): è‡ªå®šä¹‰çš„ç‚¹é‡‡æ ·ç½‘æ ¼ï¼Œå½’ä¸€åŒ–åˆ°[0,1]ã€‚
            points_stride (int): æ¯ä¾§é‡‡æ ·ç‚¹çš„æ•°é‡ã€‚
            points_batch_size (int): æ‰¹é‡å¤§å°ï¼Œè¡¨ç¤ºåŒæ—¶å¤„ç†çš„ç‚¹çš„æ•°é‡ã€‚
            conf_thres (float): ç”¨äºæ ¹æ®æ©è†œè´¨é‡é¢„æµ‹çš„ç½®ä¿¡åº¦é˜ˆå€¼[0,1]ã€‚
            stability_score_thresh (float): ç”¨äºåŸºäºç¨³å®šæ€§è¿›è¡Œæ©è†œè¿‡æ»¤çš„ç¨³å®šæ€§é˜ˆå€¼[0,1]ã€‚
            stability_score_offset (float): è®¡ç®—ç¨³å®šæ€§åˆ†æ•°çš„åç§»å€¼ã€‚
            crop_nms_thresh (float): ç”¨äºNMSçš„IoUæˆªæ­¢å€¼ï¼Œç”¨äºå»é™¤è£å‰ªä¹‹é—´çš„é‡å¤æ©è†œã€‚

        è¿”å›ï¼š
            pred_masks (torch.Tensor): åˆ†å‰²åçš„æ©è†œï¼Œå½¢çŠ¶ä¸º(N, H, W)ã€‚
            pred_scores (torch.Tensor): æ¯ä¸ªæ©è†œçš„ç½®ä¿¡åº¦åˆ†æ•°ï¼Œå½¢çŠ¶ä¸º(N,)ã€‚
            pred_bboxes (torch.Tensor): æ¯ä¸ªæ©è†œçš„è¾¹ç•Œæ¡†ï¼Œå½¢çŠ¶ä¸º(N, 4)ã€‚

        ç¤ºä¾‹ï¼š
            >>> predictor = Predictor()
            >>> im = torch.rand(1, 3, 1024, 1024)  # ç¤ºä¾‹è¾“å…¥å›¾åƒ
            >>> masks, scores, boxes = predictor.generate(im)
        """
        import torchvision  # ç”¨äºæ›´å¿«çš„ 'import ultralytics'

        self.segment_all = True
        ih, iw = im.shape[2:]
        crop_regions, layer_idxs = generate_crop_boxes((ih, iw), crop_n_layers, crop_overlap_ratio)
        if point_grids is None:
            point_grids = build_all_layer_point_grids(points_stride, crop_n_layers, crop_downscale_factor)
        pred_masks, pred_scores, pred_bboxes, region_areas = [], [], [], []
        for crop_region, layer_idx in zip(crop_regions, layer_idxs):
            x1, y1, x2, y2 = crop_region
            w, h = x2 - x1, y2 - y1
            area = torch.tensor(w * h, device=im.device)
            points_scale = np.array([[w, h]])  # w, h
            # è£å‰ªå›¾åƒå¹¶æ’å€¼åˆ°è¾“å…¥å¤§å°
            crop_im = F.interpolate(im[..., y1:y2, x1:x2], (ih, iw), mode="bilinear", align_corners=False)
            # (num_points, 2)
            points_for_image = point_grids[layer_idx] * points_scale
            crop_masks, crop_scores, crop_bboxes = [], [], []
            for (points,) in batch_iterator(points_batch_size, points_for_image):
                pred_mask, pred_score = self.prompt_inference(crop_im, points=points, multimask_output=True)
                # å°†é¢„æµ‹çš„æ©è†œæ’å€¼åˆ°è¾“å…¥å¤§å°
                pred_mask = F.interpolate(pred_mask[None], (h, w), mode="bilinear", align_corners=False)[0]
                idx = pred_score > conf_thres
                pred_mask, pred_score = pred_mask[idx], pred_score[idx]

                stability_score = calculate_stability_score(
                    pred_mask, self.model.mask_threshold, stability_score_offset
                )
                idx = stability_score > stability_score_thresh
                pred_mask, pred_score = pred_mask[idx], pred_score[idx]
                # ä½¿ç”¨å¸ƒå°”ç±»å‹æ›´èŠ‚çœå†…å­˜
                pred_mask = pred_mask > self.model.mask_threshold
                # (N, 4)
                pred_bbox = batched_mask_to_box(pred_mask).float()
                keep_mask = ~is_box_near_crop_edge(pred_bbox, crop_region, [0, 0, iw, ih])
                if not torch.all(keep_mask):
                    pred_bbox, pred_mask, pred_score = pred_bbox[keep_mask], pred_mask[keep_mask], pred_score[keep_mask]

                crop_masks.append(pred_mask)
                crop_bboxes.append(pred_bbox)
                crop_scores.append(pred_score)

            # å¯¹è¯¥è£å‰ªåŒºåŸŸè¿›è¡ŒNMS
            crop_masks = torch.cat(crop_masks)
            crop_bboxes = torch.cat(crop_bboxes)
            crop_scores = torch.cat(crop_scores)
            keep = torchvision.ops.nms(crop_bboxes, crop_scores, self.args.iou)  # NMS
            crop_bboxes = uncrop_boxes_xyxy(crop_bboxes[keep], crop_region)
            crop_masks = uncrop_masks(crop_masks[keep], crop_region, ih, iw)
            crop_scores = crop_scores[keep]

            pred_masks.append(crop_masks)
            pred_bboxes.append(crop_bboxes)
            pred_scores.append(crop_scores)
            region_areas.append(area.expand(len(crop_masks)))

        pred_masks = torch.cat(pred_masks)
        pred_bboxes = torch.cat(pred_bboxes)
        pred_scores = torch.cat(pred_scores)
        region_areas = torch.cat(region_areas)

        # å»é™¤è£å‰ªä¹‹é—´çš„é‡å¤æ©è†œ
        if len(crop_regions) > 1:
            scores = 1 / region_areas
            keep = torchvision.ops.nms(pred_bboxes, scores, crop_nms_thresh)
            pred_masks, pred_bboxes, pred_scores = pred_masks[keep], pred_bboxes[keep], pred_scores[keep]

        return pred_masks, pred_scores, pred_bboxes

    def setup_model(self, model=None, verbose=True):
        """
        åˆå§‹åŒ– Segment Anything Model (SAM) ä»¥è¿›è¡Œæ¨ç†ã€‚

        è¯¥æ–¹æ³•é€šè¿‡å°†æ¨¡å‹åˆ†é…åˆ°é€‚å½“çš„è®¾å¤‡å¹¶åˆå§‹åŒ–å›¾åƒå½’ä¸€åŒ–å’Œå…¶ä»– Ultralytics å…¼å®¹æ€§è®¾ç½®æ¥è®¾ç½® SAM æ¨¡å‹ã€‚

        å‚æ•°:
            model (torch.nn.Module | None): é¢„è®­ç»ƒçš„ SAM æ¨¡å‹ã€‚å¦‚æœä¸º Noneï¼Œåˆ™æ ¹æ®é…ç½®æ„å»ºæ–°æ¨¡å‹ã€‚
            verbose (bool): å¦‚æœä¸º Trueï¼Œåˆ™æ‰“å°é€‰æ‹©çš„è®¾å¤‡ä¿¡æ¯ã€‚

        ç¤ºä¾‹:
            >>> predictor = Predictor()
            >>> predictor.setup_model(model=sam_model, verbose=True)
        """
        device = select_device(self.args.device, verbose=verbose)
        if model is None:
            model = self.get_model()
        model.eval()
        self.model = model.to(device)
        self.device = device
        self.mean = torch.tensor([123.675, 116.28, 103.53]).view(-1, 1, 1).to(device)
        self.std = torch.tensor([58.395, 57.12, 57.375]).view(-1, 1, 1).to(device)

        # Ultralytics å…¼å®¹æ€§è®¾ç½®
        self.model.pt = False
        self.model.triton = False
        self.model.stride = 32
        self.model.fp16 = False
        self.done_warmup = True

    def get_model(self):
        """è·å–æˆ–æ„å»ºç”¨äºå›¾åƒåˆ†å‰²ä»»åŠ¡çš„ Segment Anything Model (SAM)ã€‚"""
        return build_sam(self.args.model)

    def postprocess(self, preds, img, orig_imgs):
        """
        åå¤„ç† SAM æ¨ç†è¾“å‡ºï¼Œä»¥ç”Ÿæˆç›®æ ‡æ£€æµ‹æ©è†œå’Œè¾¹ç•Œæ¡†ã€‚

        è¯¥æ–¹æ³•å°†æ©è†œå’Œè¾¹æ¡†ç¼©æ”¾åˆ°åŸå§‹å›¾åƒå¤§å°ï¼Œå¹¶åº”ç”¨æ©è†œé¢„æµ‹çš„é˜ˆå€¼ã€‚å®ƒåˆ©ç”¨ SAM çš„å…ˆè¿›æ¶æ„è¿›è¡Œå®æ—¶çš„ã€å¯æç¤ºçš„åˆ†å‰²ä»»åŠ¡ã€‚

        å‚æ•°:
            preds (Tuple[torch.Tensor]): æ¥è‡ª SAM æ¨¡å‹æ¨ç†çš„è¾“å‡ºï¼ŒåŒ…å«ï¼š
                - pred_masks (torch.Tensor): é¢„æµ‹çš„æ©è†œï¼Œå½¢çŠ¶ä¸º (N, 1, H, W)ã€‚
                - pred_scores (torch.Tensor): æ¯ä¸ªæ©è†œçš„ç½®ä¿¡åº¦åˆ†æ•°ï¼Œå½¢çŠ¶ä¸º (N, 1)ã€‚
                - pred_bboxes (torch.Tensor, å¯é€‰): å¦‚æœ segment_all ä¸º Trueï¼Œåˆ™ä¸ºé¢„æµ‹çš„è¾¹ç•Œæ¡†ã€‚
            img (torch.Tensor): å¤„ç†åçš„è¾“å…¥å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º (C, H, W)ã€‚
            orig_imgs (List[np.ndarray] | torch.Tensor): åŸå§‹çš„æœªå¤„ç†å›¾åƒã€‚

        è¿”å›:
            results (List[Results]): åŒ…å«æ£€æµ‹æ©è†œã€è¾¹ç•Œæ¡†å’Œæ¯ä¸ªå¤„ç†å›¾åƒçš„å…¶ä»–å…ƒæ•°æ®çš„ Results å¯¹è±¡åˆ—è¡¨ã€‚

        ç¤ºä¾‹:
            >>> predictor = Predictor()
            >>> preds = predictor.inference(img)
            >>> results = predictor.postprocess(preds, img, orig_imgs)
        """
        # (N, 1, H, W), (N, 1)
        pred_masks, pred_scores = preds[:2]
        pred_bboxes = preds[2] if self.segment_all else None
        names = dict(enumerate(str(i) for i in range(len(pred_masks))))

        if not isinstance(orig_imgs, list):  # è¾“å…¥å›¾åƒæ˜¯ä¸€ä¸ª torch.Tensorï¼Œè€Œä¸æ˜¯åˆ—è¡¨
            orig_imgs = ops.convert_torch2numpy_batch(orig_imgs)

        results = []
        for masks, orig_img, img_path in zip([pred_masks], orig_imgs, self.batch[0]):
            if len(masks) == 0:
                masks, pred_bboxes = None, torch.zeros((0, 6), device=pred_masks.device)
            else:
                masks = ops.scale_masks(masks[None].float(), orig_img.shape[:2], padding=False)[0]
                masks = masks > self.model.mask_threshold  # è½¬ä¸ºå¸ƒå°”å€¼
                if pred_bboxes is not None:
                    pred_bboxes = ops.scale_boxes(img.shape[2:], pred_bboxes.float(), orig_img.shape, padding=False)
                else:
                    pred_bboxes = batched_mask_to_box(masks)
                # æ³¨æ„ï¼šSAM æ¨¡å‹ä¸ä¼šè¿”å›ç±»åˆ«ä¿¡æ¯ã€‚è¿™é‡Œçš„ `cls` åªæ˜¯ä¸ºäº†ä¿æŒä¸€è‡´æ€§ã€‚
                cls = torch.arange(len(pred_masks), dtype=torch.int32, device=pred_masks.device)
                pred_bboxes = torch.cat([pred_bboxes, pred_scores[:, None], cls[:, None]], dim=-1)
            results.append(Results(orig_img, path=img_path, names=names, masks=masks, boxes=pred_bboxes))
        # é‡ç½® segment-all æ¨¡å¼ã€‚
        self.segment_all = False
        return results

    def setup_source(self, source):
        """
        è®¾ç½®æ¨ç†çš„æ•°æ®æºã€‚

        è¯¥æ–¹æ³•é…ç½®å°†ç”¨äºæ¨ç†çš„å›¾åƒæ•°æ®æºã€‚å®ƒæ”¯æŒå¤šç§è¾“å…¥ç±»å‹ï¼Œå¦‚å›¾åƒæ–‡ä»¶ã€ç›®å½•ã€è§†é¢‘æ–‡ä»¶å’Œå…¶ä»–å…¼å®¹çš„æ•°æ®æºã€‚

        å‚æ•°:
            source (str | Path | None): å›¾åƒæ•°æ®æºçš„è·¯å¾„æˆ–æ ‡è¯†ç¬¦ã€‚å¯ä»¥æ˜¯æ–‡ä»¶è·¯å¾„ã€ç›®å½•è·¯å¾„ã€URL æˆ–å…¶ä»–æ”¯æŒçš„æºç±»å‹ã€‚

        ç¤ºä¾‹:
            >>> predictor = Predictor()
            >>> predictor.setup_source("path/to/images")
            >>> predictor.setup_source("video.mp4")
            >>> predictor.setup_source(None)  # å¦‚æœå¯ç”¨ï¼Œä½¿ç”¨é»˜è®¤æº

        æ³¨æ„:
            - å¦‚æœ source ä¸º Noneï¼Œåˆ™è¯¥æ–¹æ³•å¯èƒ½ä¼šä½¿ç”¨é»˜è®¤æºï¼ˆå¦‚æœå·²é…ç½®ï¼‰ã€‚
            - è¯¥æ–¹æ³•é€‚åº”ä¸åŒçš„æºç±»å‹ï¼Œå¹¶ä¸ºåç»­çš„æ¨ç†æ­¥éª¤åšå‡†å¤‡ã€‚
            - æ”¯æŒçš„æºç±»å‹å¯èƒ½åŒ…æ‹¬æœ¬åœ°æ–‡ä»¶ã€ç›®å½•ã€URL å’Œè§†é¢‘æµã€‚
        """
        if source is not None:
            super().setup_source(source)

    def set_image(self, image):
        """
        ä¸ºæ¨ç†é¢„å¤„ç†å¹¶è®¾ç½®å•å¼ å›¾åƒã€‚

        æ­¤æ–¹æ³•é€šè¿‡è®¾ç½®æ¨¡å‹ï¼ˆå¦‚æœå°šæœªåˆå§‹åŒ–ï¼‰ã€é…ç½®æ•°æ®æºï¼Œå¹¶é¢„å¤„ç†å›¾åƒä»¥è¿›è¡Œç‰¹å¾æå–ï¼Œ
        ä¸ºå•å¼ å›¾åƒçš„æ¨ç†åšå‡†å¤‡ã€‚å®ƒç¡®ä¿ä¸€æ¬¡åªè®¾ç½®ä¸€å¼ å›¾åƒï¼Œå¹¶æå–å›¾åƒç‰¹å¾ä»¥ä¾›åç»­ä½¿ç”¨ã€‚

        å‚æ•°ï¼š
            image (str | np.ndarray): å›¾åƒæ–‡ä»¶çš„è·¯å¾„å­—ç¬¦ä¸²ï¼Œæˆ–ä»£è¡¨é€šè¿‡cv2è¯»å–çš„å›¾åƒçš„numpyæ•°ç»„ã€‚

        å¼‚å¸¸ï¼š
            AssertionError: å¦‚æœå°è¯•è®¾ç½®å¤šäºä¸€å¼ å›¾åƒï¼Œåˆ™æŠ›å‡ºæ­¤å¼‚å¸¸ã€‚

        ç¤ºä¾‹ï¼š
            >>> predictor = Predictor()
            >>> predictor.set_image("path/to/image.jpg")
            >>> predictor.set_image(cv2.imread("path/to/image.jpg"))

        æ³¨æ„ï¼š
            - åœ¨å¯¹æ–°å›¾åƒè¿›è¡Œæ¨ç†ä¹‹å‰åº”è°ƒç”¨æ­¤æ–¹æ³•ã€‚
            - æå–çš„ç‰¹å¾å­˜å‚¨åœ¨`self.features`å±æ€§ä¸­ï¼Œä¾›ä»¥åä½¿ç”¨ã€‚
        """
        if self.model is None:
            self.setup_model(model=None)
        self.setup_source(image)
        assert len(self.dataset) == 1, "`set_image` ä»…æ”¯æŒè®¾ç½®ä¸€å¼ å›¾åƒï¼"
        for batch in self.dataset:
            im = self.preprocess(batch[1])
            self.features = self.get_im_features(im)
            break

    def get_im_features(self, im):
        """ä½¿ç”¨SAMæ¨¡å‹çš„å›¾åƒç¼–ç å™¨æå–å›¾åƒç‰¹å¾ï¼Œä»¥ä¾¿åç»­æ©ç é¢„æµ‹ã€‚"""
        assert isinstance(self.imgsz, (tuple, list)) and self.imgsz[0] == self.imgsz[1], (
            f"SAMæ¨¡å‹ä»…æ”¯æŒæ–¹å½¢å›¾åƒå¤§å°ï¼Œä½†å¾—åˆ°äº† {self.imgsz}ã€‚"
        )
        self.model.set_imgsz(self.imgsz)
        return self.model.image_encoder(im)

    def set_prompts(self, prompts):
        """ä¸ºåç»­æ¨ç†æ“ä½œè®¾ç½®æç¤ºã€‚"""
        self.prompts = prompts

    def reset_image(self):
        """é‡ç½®å½“å‰å›¾åƒåŠå…¶ç‰¹å¾ï¼Œä¸ºåç»­æ¨ç†æ¸…é™¤å®ƒä»¬ã€‚"""
        self.im = None
        self.features = None

    @staticmethod
    def remove_small_regions(masks, min_area=0, nms_thresh=0.7):
        """
        ä»åˆ†å‰²æ©ç ä¸­ç§»é™¤å°çš„å­¤ç«‹åŒºåŸŸå’Œå­”æ´ã€‚

        æ­¤å‡½æ•°å¯¹ç”±Segment Anything Modelï¼ˆSAMï¼‰ç”Ÿæˆçš„åˆ†å‰²æ©ç è¿›è¡Œåå¤„ç†ã€‚
        å®ƒç§»é™¤è¾“å…¥æ©ç ä¸­çš„å°å­¤ç«‹åŒºåŸŸå’Œå­”æ´ï¼Œç„¶åæ‰§è¡Œéæœ€å¤§æŠ‘åˆ¶ï¼ˆNMSï¼‰æ¥å»é™¤ä»»ä½•æ–°åˆ›å»ºçš„é‡å¤æ¡†ã€‚

        å‚æ•°ï¼š
            masks (torch.Tensor): è¦å¤„ç†çš„åˆ†å‰²æ©ç ï¼Œå½¢çŠ¶ä¸º(N, H, W)ï¼Œå…¶ä¸­Næ˜¯æ©ç æ•°é‡ï¼ŒHæ˜¯é«˜åº¦ï¼ŒWæ˜¯å®½åº¦ã€‚
            min_area (int): ç”¨äºç§»é™¤å­¤ç«‹åŒºåŸŸå’Œå­”æ´çš„æœ€å°é¢ç§¯é˜ˆå€¼ã€‚å°äºæ­¤é˜ˆå€¼çš„åŒºåŸŸå°†è¢«ç§»é™¤ã€‚
            nms_thresh (float): NMSç®—æ³•çš„IoUé˜ˆå€¼ï¼Œç”¨äºå»é™¤é‡å¤æ¡†ã€‚

        è¿”å›ï¼š
            new_masks (torch.Tensor): å¤„ç†åçš„æ©ç ï¼Œç§»é™¤äº†å°åŒºåŸŸï¼Œå½¢çŠ¶ä¸º(N, H, W)ã€‚
            keep (List[int]): åœ¨NMSä¹‹åä¿ç•™çš„æ©ç ç´¢å¼•ï¼Œç”¨äºè¿‡æ»¤ç›¸åº”çš„æ¡†ã€‚

        ç¤ºä¾‹ï¼š
            >>> masks = torch.rand(5, 640, 640) > 0.5  # 5ä¸ªéšæœºäºŒè¿›åˆ¶æ©ç 
            >>> new_masks, keep = remove_small_regions(masks, min_area=100, nms_thresh=0.7)
            >>> print(f"åŸå§‹æ©ç : {masks.shape}, å¤„ç†åæ©ç : {new_masks.shape}")
            >>> print(f"ä¿ç•™çš„æ©ç ç´¢å¼•: {keep}")
        """
        import torchvision  # ä¸ºäº†æ›´å¿«é€Ÿåœ°å¯¼å…¥ 'import ultralytics'

        if len(masks) == 0:
            return masks

        # è¿‡æ»¤æ‰å°çš„å­¤ç«‹åŒºåŸŸå’Œå­”æ´
        new_masks = []
        scores = []
        for mask in masks:
            mask = mask.cpu().numpy().astype(np.uint8)
            mask, changed = remove_small_regions(mask, min_area, mode="holes")
            unchanged = not changed
            mask, changed = remove_small_regions(mask, min_area, mode="islands")
            unchanged = unchanged and not changed

            new_masks.append(torch.as_tensor(mask).unsqueeze(0))
            # ç»™å˜åŒ–çš„æ©ç åˆ†é…åˆ†æ•°0ï¼Œç»™æœªå˜åŒ–çš„æ©ç åˆ†é…åˆ†æ•°1ï¼Œè¿™æ ·NMSä¼šåå¥½ä¸éœ€è¦åå¤„ç†çš„æ©ç 
            scores.append(float(unchanged))

        # é‡æ–°è®¡ç®—æ¡†å¹¶å»é™¤ä»»ä½•æ–°çš„é‡å¤æ¡†
        new_masks = torch.cat(new_masks, dim=0)
        boxes = batched_mask_to_box(new_masks)
        keep = torchvision.ops.nms(boxes.float(), torch.as_tensor(scores), nms_thresh)

        return new_masks[keep].to(device=masks.device, dtype=masks.dtype), keep


class SAM2Predictor(Predictor):
    """
    SAM2Predictorç±»ç”¨äºåŸºäºSegment Anything Model 2æ¶æ„çš„é«˜çº§å›¾åƒåˆ†å‰²ã€‚

    è¯¥ç±»æ‰©å±•äº†åŸºç¡€Predictorç±»ï¼Œä»¥å®ç°SAM2ç‰¹å®šçš„å›¾åƒåˆ†å‰²åŠŸèƒ½ã€‚å®ƒæä¾›äº†æ¨¡å‹åˆå§‹åŒ–ã€ç‰¹å¾æå–å’ŒåŸºäºæç¤ºçš„æ¨ç†æ–¹æ³•ã€‚

    å±æ€§ï¼š
        _bb_feat_sizes (List[Tuple[int, int]]): ä¸åŒéª¨å¹²ç½‘ç»œå±‚çš„ç‰¹å¾å¤§å°ã€‚
        model (torch.nn.Module): åŠ è½½çš„SAM2æ¨¡å‹ã€‚
        device (torch.device): æ¨¡å‹åŠ è½½çš„è®¾å¤‡ï¼ˆCPUæˆ–GPUï¼‰ã€‚
        features (Dict[str, torch.Tensor]): ç”¨äºé«˜æ•ˆæ¨ç†çš„ç¼“å­˜å›¾åƒç‰¹å¾ã€‚
        segment_all (bool): æ ‡å¿—ï¼ŒæŒ‡ç¤ºæ˜¯å¦åº”é¢„æµ‹æ‰€æœ‰åˆ†å‰²åŒºåŸŸã€‚
        prompts (Dict): ç”¨äºæ¨ç†çš„å„ç§æç¤ºçš„å­—å…¸ã€‚

    æ–¹æ³•ï¼š
        get_model: è·å–å¹¶åˆå§‹åŒ–SAM2æ¨¡å‹ã€‚
        prompt_inference: åŸºäºå„ç§æç¤ºæ‰§è¡Œå›¾åƒåˆ†å‰²æ¨ç†ã€‚
        set_image: é¢„å¤„ç†å¹¶è®¾ç½®å•å¼ å›¾åƒä»¥è¿›è¡Œæ¨ç†ã€‚
        get_im_features: ä½¿ç”¨SAM2çš„å›¾åƒç¼–ç å™¨æå–å’Œå¤„ç†å›¾åƒç‰¹å¾ã€‚

    ç¤ºä¾‹ï¼š
        >>> predictor = SAM2Predictor(cfg)
        >>> predictor.set_image("path/to/image.jpg")
        >>> bboxes = [[100, 100, 200, 200]]
        >>> result = predictor(bboxes=bboxes)[0]
        >>> print(f"é¢„æµ‹äº†{len(result.masks)}ä¸ªæ©ç ï¼Œå¹³å‡å¾—åˆ† {result.boxes.conf.mean():.2f}")
    """

    _bb_feat_sizes = [
        (256, 256),
        (128, 128),
        (64, 64),
    ]

    def get_model(self):
        """è·å–å¹¶åˆå§‹åŒ–Segment Anything Model 2ï¼ˆSAM2ï¼‰ç”¨äºå›¾åƒåˆ†å‰²ä»»åŠ¡ã€‚"""
        return build_sam(self.args.model)

    def prompt_inference(
        self,
        im,
        bboxes=None,
        points=None,
        labels=None,
        masks=None,
        multimask_output=False,
        img_idx=-1,
    ):
        """
        åŸºäºå„ç§æç¤ºä½¿ç”¨SAM2æ¶æ„æ‰§è¡Œå›¾åƒåˆ†å‰²æ¨ç†ã€‚

        æ­¤æ–¹æ³•åˆ©ç”¨Segment Anything Model 2ï¼ˆSAM2ï¼‰æ ¹æ®æä¾›çš„æç¤ºï¼ˆå¦‚è¾¹ç•Œæ¡†ã€ç‚¹æˆ–ç°æœ‰æ©ç ï¼‰ç”Ÿæˆè¾“å…¥å›¾åƒçš„åˆ†å‰²æ©ç ã€‚å®ƒæ”¯æŒå•å¯¹è±¡å’Œå¤šå¯¹è±¡é¢„æµ‹åœºæ™¯ã€‚

        å‚æ•°ï¼š
            im (torch.Tensor): é¢„å¤„ç†åçš„è¾“å…¥å›¾åƒtensorï¼Œå½¢çŠ¶ä¸º(N, C, H, W)ã€‚
            bboxes (np.ndarray | List[List[float]] | None): è¾¹ç•Œæ¡†ï¼Œå½¢çŠ¶ä¸º(N, 4)ï¼ŒXYXYæ ¼å¼ã€‚
            points (np.ndarray | List[List[float]] | None): ç‰©ä½“ä½ç½®ç‚¹ï¼Œå½¢çŠ¶ä¸º(N, 2)ï¼Œå•ä½ä¸ºåƒç´ ã€‚
            labels (np.ndarray | List[int] | None): ç‚¹æç¤ºæ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º(N,)ï¼Œ1è¡¨ç¤ºå‰æ™¯ï¼Œ0è¡¨ç¤ºèƒŒæ™¯ã€‚
            masks (np.ndarray | None): æ¥è‡ªå…ˆå‰é¢„æµ‹çš„ä½åˆ†è¾¨ç‡æ©ç ï¼Œå½¢çŠ¶ä¸º(N, H, W)ã€‚
            multimask_output (bool): æ˜¯å¦è¿”å›å¤šä¸ªæ©ç ï¼Œç”¨äºæ¨¡ç³Šæç¤ºã€‚
            img_idx (int): æ‰¹é‡ä¸­è¦å¤„ç†çš„å›¾åƒç´¢å¼•ã€‚

        è¿”å›ï¼š
            (np.ndarray): è¾“å‡ºæ©ç ï¼Œå½¢çŠ¶ä¸º(C, H, W)ï¼Œå…¶ä¸­Cæ˜¯ç”Ÿæˆçš„æ©ç æ•°é‡ã€‚
            (np.ndarray): æ¯ä¸ªæ©ç çš„è´¨é‡åˆ†æ•°ï¼Œé•¿åº¦ä¸ºCã€‚

        ç¤ºä¾‹ï¼š
            >>> predictor = SAM2Predictor(cfg)
            >>> image = torch.rand(1, 3, 640, 640)
            >>> bboxes = [[100, 100, 200, 200]]
            >>> result = predictor(image, bboxes=bboxes)[0]
            >>> print(f"ç”Ÿæˆäº†{result.masks.shape[0]}ä¸ªæ©ç ï¼Œå¹³å‡å¾—åˆ† {result.boxes.conf.mean():.2f}")

        æ³¨æ„ï¼š
            - è¯¥æ–¹æ³•æ”¯æŒæä¾›ç‚¹æˆ–è¾¹ç•Œæ¡†æ—¶çš„æ‰¹é‡æ¨ç†ï¼Œç”¨äºå¤šå¯¹è±¡é¢„æµ‹ã€‚
            - è¾“å…¥æç¤ºï¼ˆè¾¹ç•Œæ¡†ã€ç‚¹ï¼‰ä¼šè‡ªåŠ¨ç¼©æ”¾ä»¥åŒ¹é…è¾“å…¥å›¾åƒçš„å°ºå¯¸ã€‚
            - å½“åŒæ—¶æä¾›è¾¹ç•Œæ¡†å’Œç‚¹æ—¶ï¼Œå®ƒä»¬ä¼šåˆå¹¶æˆä¸€ä¸ªå•ç‹¬çš„â€œç‚¹â€è¾“å…¥ç»™æ¨¡å‹ã€‚

        å‚è€ƒæ–‡çŒ®ï¼š
            - SAM2è®ºæ–‡ï¼š[æ·»åŠ SAM2è®ºæ–‡é“¾æ¥ï¼ˆå¦‚æœå¯ç”¨ï¼‰]
        """
        features = self.get_im_features(im) if self.features is None else self.features

        points, labels, masks = self._prepare_prompts(im.shape[2:], bboxes, points, labels, masks)
        points = (points, labels) if points is not None else None

        sparse_embeddings, dense_embeddings = self.model.sam_prompt_encoder(
            points=points,
            boxes=None,
            masks=masks,
        )
        # é¢„æµ‹æ©ç 
        batched_mode = points is not None and points[0].shape[0] > 1  # å¤šå¯¹è±¡é¢„æµ‹
        high_res_features = [feat_level[img_idx].unsqueeze(0) for feat_level in features["high_res_feats"]]
        pred_masks, pred_scores, _, _ = self.model.sam_mask_decoder(
            image_embeddings=features["image_embed"][img_idx].unsqueeze(0),
            image_pe=self.model.sam_prompt_encoder.get_dense_pe(),
            sparse_prompt_embeddings=sparse_embeddings,
            dense_prompt_embeddings=dense_embeddings,
            multimask_output=multimask_output,
            repeat_image=batched_mode,
            high_res_features=high_res_features,
        )
        # (N, d, H, W) --> (N*d, H, W), (N, d) --> (N*d, )
        # `d` å¯èƒ½æ˜¯1æˆ–3ï¼Œå–å†³äº`multimask_output`ã€‚
        return pred_masks.flatten(0, 1), pred_scores.flatten(0, 1)

    def _prepare_prompts(self, dst_shape, bboxes=None, points=None, labels=None, masks=None):
        """
        æ ¹æ®ç›®æ ‡å°ºå¯¸å¯¹è¾“å…¥æç¤ºè¿›è¡Œé¢„å¤„ç†å’Œå˜æ¢ã€‚

        å‚æ•°ï¼š
            dst_shape (tuple): ç›®æ ‡å›¾åƒçš„å°ºå¯¸ (height, width)ã€‚
            bboxes (np.ndarray | List | None): è¾¹ç•Œæ¡†ï¼ŒXYXY æ ¼å¼ï¼Œå½¢çŠ¶ä¸º (N, 4)ã€‚
            points (np.ndarray | List | None): æŒ‡ç¤ºç‰©ä½“ä½ç½®çš„ç‚¹ï¼Œå½¢çŠ¶ä¸º (N, 2) æˆ– (N, num_points, 2)ï¼Œå•ä½ä¸ºåƒç´ ã€‚
            labels (np.ndarray | List | None): ç‚¹çš„æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º (N,) æˆ– (N, num_points)ã€‚å‰æ™¯ä¸º 1ï¼ŒèƒŒæ™¯ä¸º 0ã€‚
            masks (List | np.ndarray, å¯é€‰): ç‰©ä½“å¯¹åº”çš„æ©ç ï¼Œæ¯ä¸ªæ©ç æ˜¯ä¸€ä¸ªäºŒç»´æ•°ç»„ã€‚

        å¼‚å¸¸ï¼š
            AssertionError: å¦‚æœä¼ å…¥äº†æ ‡ç­¾ï¼Œä½†ç‚¹çš„æ•°é‡ä¸æ ‡ç­¾ä¸åŒ¹é…ï¼Œåˆ™ä¼šæŠ›å‡ºå¼‚å¸¸ã€‚

        è¿”å›ï¼š
            (tuple): è¿”å›ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«å¤„ç†åçš„ pointsã€labels å’Œ masksã€‚
        """
        bboxes, points, labels, masks = super()._prepare_prompts(dst_shape, bboxes, points, labels, masks)
        if bboxes is not None:
            bboxes = bboxes.view(-1, 2, 2)
            bbox_labels = torch.tensor([[2, 3]], dtype=torch.int32, device=bboxes.device).expand(len(bboxes), -1)
            # æ³¨æ„ï¼šå°†â€œboxesâ€å’Œâ€œpointsâ€åˆå¹¶ä¸ºä¸€ä¸ªâ€œpointsâ€è¾“å…¥ï¼Œ
            # å¹¶ä½œä¸ºæ•´ä½“è¾“å…¥ä¼ å…¥ model.sam_prompt_encoderã€‚
            if points is not None:
                points = torch.cat([bboxes, points], dim=1)
                labels = torch.cat([bbox_labels, labels], dim=1)
            else:
                points, labels = bboxes, bbox_labels
        return points, labels, masks

    def set_image(self, image):
        """
        é¢„å¤„ç†å¹¶è®¾ç½®å•å¼ å›¾åƒï¼Œç”¨äº SAM2 æ¨¡å‹æ¨ç†ã€‚

        æ­¤æ–¹æ³•ä¼šåœ¨æ¨¡å‹æœªåˆå§‹åŒ–æ—¶è¿›è¡Œåˆå§‹åŒ–ï¼Œé…ç½®æ•°æ®æºï¼Œå¹¶æå–ç‰¹å¾å‘é‡ã€‚
        æ¯æ¬¡åªèƒ½è®¾ç½®ä¸€å¼ å›¾åƒã€‚

        å‚æ•°ï¼š
            image (str | np.ndarray): å›¾åƒè·¯å¾„ï¼ˆå­—ç¬¦ä¸²ï¼‰æˆ–å›¾åƒæ•°æ®ï¼ˆNumPy æ•°ç»„ï¼‰ã€‚

        å¼‚å¸¸ï¼š
            AssertionError: å¦‚æœå°è¯•è®¾ç½®è¶…è¿‡ä¸€å¼ å›¾åƒï¼Œå°†æŠ›å‡ºå¼‚å¸¸ã€‚

        ç¤ºä¾‹ï¼š
            >>> predictor = SAM2Predictor()
            >>> predictor.set_image("path/to/image.jpg")
            >>> predictor.set_image(np.array([...]))  # ç›´æ¥ä½¿ç”¨ numpy å›¾åƒæ•°ç»„

        æ³¨æ„ï¼š
            - åœ¨å¯¹æ–°å›¾åƒè¿›è¡Œæ¨ç†ä¹‹å‰ï¼Œå¿…é¡»è°ƒç”¨æ­¤æ–¹æ³•ã€‚
            - æå–çš„å›¾åƒç‰¹å¾å°†è¢«ç¼“å­˜ï¼Œä»¥åŠ é€Ÿå¯¹ç›¸åŒå›¾åƒçš„é‡å¤æ¨ç†ã€‚
            - ä¸€æ¬¡ä»…æ”¯æŒä¸€å¼ å›¾åƒã€‚å¦‚éœ€å¤„ç†å¤šå¼ å›¾åƒï¼Œè¯·é€å¼ è°ƒç”¨è¯¥æ–¹æ³•ã€‚
        """
        if self.model is None:
            self.setup_model(model=None)
        self.setup_source(image)
        assert len(self.dataset) == 1, "`set_image` åªæ”¯æŒä¸€æ¬¡è®¾ç½®ä¸€å¼ å›¾åƒï¼"
        for batch in self.dataset:
            im = self.preprocess(batch[1])
            self.features = self.get_im_features(im)
            break

    def get_im_features(self, im):
        """ä» SAM å›¾åƒç¼–ç å™¨ä¸­æå–å›¾åƒç‰¹å¾ï¼Œç”¨äºåç»­å¤„ç†ã€‚"""
        assert isinstance(self.imgsz, (tuple, list)) and self.imgsz[0] == self.imgsz[1], (
            f"SAM 2 æ¨¡å‹ä»…æ”¯æŒæ–¹å½¢å›¾åƒå°ºå¯¸ï¼Œä½†å½“å‰ä¸º {self.imgsz}ã€‚"
        )
        self.model.set_imgsz(self.imgsz)
        self._bb_feat_sizes = [[x // (4 * i) for x in self.imgsz] for i in [1, 2, 4]]

        backbone_out = self.model.forward_image(im)
        _, vision_feats, _, _ = self.model._prepare_backbone_features(backbone_out)
        if self.model.directly_add_no_mem_embed:
            vision_feats[-1] = vision_feats[-1] + self.model.no_mem_embed
        feats = [
            feat.permute(1, 2, 0).view(1, -1, *feat_size)
            for feat, feat_size in zip(vision_feats[::-1], self._bb_feat_sizes[::-1])
        ][::-1]
        return {"image_embed": feats[-1], "high_res_feats": feats[:-1]}


class SAM2VideoPredictor(SAM2Predictor):
    """
    SAM2VideoPredictor ç”¨äºå¤„ç†è§†é¢‘ä¸­çš„ç”¨æˆ·äº¤äº’å¹¶ç®¡ç†æ¨ç†çŠ¶æ€ã€‚

    æœ¬ç±»åœ¨ SAM2Predictor çš„åŸºç¡€ä¸Šæ‰©å±•ï¼Œæ”¯æŒè§†é¢‘å¤„ç†ï¼Œå¹¶ç»´æŠ¤æ¨ç†æ“ä½œçš„çŠ¶æ€ã€‚
    å®ƒåŒ…å«äº†ä¸€äº›ç”¨äºç®¡ç†æ©ç ä¸é‡å ã€æ¸…é™¤éæ¡ä»¶å†…å­˜ã€è®¾ç½®é¢„æµ‹äº‹ä»¶å›è°ƒç­‰çš„é…ç½®é¡¹ã€‚

    å±æ€§ï¼š
        inference_state (Dict): å­—å…¸ï¼Œç”¨äºå­˜å‚¨å½“å‰çš„æ¨ç†çŠ¶æ€ã€‚
        non_overlap_masks (bool): æ˜¯å¦å¯ç”¨éé‡å æ©ç çš„æ ‡å¿—ã€‚
        clear_non_cond_mem_around_input (bool): æ˜¯å¦åœ¨è¾“å…¥å‘¨å›´æ¸…é™¤éæ¡ä»¶æ€§è®°å¿†çš„æ ‡å¿—ã€‚
        clear_non_cond_mem_for_multi_obj (bool): æ˜¯å¦åœ¨å¤šç›®æ ‡åœºæ™¯ä¸‹æ¸…é™¤éæ¡ä»¶æ€§è®°å¿†çš„æ ‡å¿—ã€‚
        callbacks (Dict): åŒ…å«å¤šä¸ªé¢„æµ‹ç”Ÿå‘½å‘¨æœŸäº‹ä»¶çš„å›è°ƒå‡½æ•°å­—å…¸ã€‚

    å‚æ•°ï¼š
        cfg (Dict, å¯é€‰): ç”¨äºåˆå§‹åŒ–çš„é…ç½®é¡¹ï¼Œé»˜è®¤ä¸º DEFAULT_CFGã€‚
        overrides (Dict, å¯é€‰): ç”¨äºè¦†ç›–é»˜è®¤é…ç½®çš„é¢å¤–è®¾ç½®ã€‚
        _callbacks (List, å¯é€‰): è‡ªå®šä¹‰å›è°ƒåˆ—è¡¨ï¼Œé»˜è®¤ä¸º Noneã€‚

    æ³¨æ„ï¼š
        å±æ€§ `fill_hole_area` å·²å®šä¹‰ä½†å½“å‰å®ç°ä¸­æœªä½¿ç”¨ã€‚
    """

    # fill_hole_area = 8  # æœªä½¿ç”¨

    def __init__(self, cfg=DEFAULT_CFG, overrides=None, _callbacks=None):
        """
        ä½¿ç”¨é…ç½®é¡¹åˆå§‹åŒ–è§†é¢‘é¢„æµ‹å™¨ï¼Œå¹¶è®¾ç½®å¿…è¦çš„çŠ¶æ€ä¸å›è°ƒã€‚

        æ­¤æ„é€ å‡½æ•°ä¼šæ ¹æ®ä¼ å…¥é…ç½®è¿›è¡Œåˆå§‹åŒ–ï¼Œåº”ç”¨ä»»ä½• override çš„å‚æ•°ï¼Œ
        å¹¶è®¾ç½®æ§åˆ¶è¡Œä¸ºçš„æ ‡å¿—åŠåˆå§‹åŒ–æ¨ç†çŠ¶æ€ã€‚

        å‚æ•°ï¼š
            cfg (Dict): åŒ…å«é»˜è®¤è®¾ç½®çš„é…ç½®å­—å…¸ã€‚
            overrides (Dict | None): ç”¨äºè¦†ç›–é»˜è®¤é…ç½®çš„å‚æ•°å­—å…¸ã€‚
            _callbacks (Dict | None): åŒ…å«è‡ªå®šä¹‰è¡Œä¸ºå‡½æ•°çš„å›è°ƒå­—å…¸ã€‚

        ç¤ºä¾‹ï¼š
            >>> predictor = SAM2VideoPredictor(cfg=DEFAULT_CFG)
            >>> predictor_example_with_imgsz = SAM2VideoPredictor(overrides={"imgsz": 640})
            >>> predictor_example_with_callback = SAM2VideoPredictor(_callbacks={"on_predict_start": custom_callback})
        """
        super().__init__(cfg, overrides, _callbacks)
        self.inference_state = {}
        self.non_overlap_masks = True
        self.clear_non_cond_mem_around_input = False
        self.clear_non_cond_mem_for_multi_obj = False
        self.callbacks["on_predict_start"].append(self.init_state)

    def get_model(self):
        """
        è·å–å¹¶é…ç½®å¯ç”¨äº†äºŒå€¼åŒ–çš„æ¨¡å‹ã€‚

        æ³¨æ„:
            è¯¥æ–¹æ³•é‡å†™äº†åŸºç±»å®ç°ï¼Œå°†äºŒå€¼åŒ–æ ‡å¿—è®¾ç½®ä¸º Trueã€‚
        """
        model = super().get_model()
        model.set_binarize(True)
        return model

    def inference(self, im, bboxes=None, points=None, labels=None, masks=None):
        """
        åŸºäºç»™å®šçš„è¾“å…¥æç¤ºæ‰§è¡Œå›¾åƒåˆ†å‰²æ¨ç†ï¼Œä½¿ç”¨å½“å‰åŠ è½½çš„å›¾åƒã€‚è¯¥æ–¹æ³•åˆ©ç”¨ SAMï¼ˆSegment Anything Modelï¼‰æ¶æ„ï¼ŒåŒ…æ‹¬å›¾åƒç¼–ç å™¨ã€æç¤ºç¼–ç å™¨å’Œæ©è†œè§£ç å™¨ï¼Œç”¨äºå®æ—¶å’Œå¯æç¤ºçš„åˆ†å‰²ä»»åŠ¡ã€‚

        å‚æ•°:
            im (torch.Tensor): é¢„å¤„ç†åçš„è¾“å…¥å›¾åƒå¼ é‡ï¼Œå½¢çŠ¶ä¸º (N, C, H, W)ã€‚
            bboxes (np.ndarray | List, å¯é€‰): è¾¹ç•Œæ¡†ï¼Œå½¢çŠ¶ä¸º (N, 4)ï¼ŒXYXY æ ¼å¼ã€‚
            points (np.ndarray | List, å¯é€‰): æŒ‡ç¤ºç‰©ä½“ä½ç½®çš„ç‚¹ï¼Œå½¢çŠ¶ä¸º (N, 2)ï¼Œå•ä½ä¸ºåƒç´ ã€‚
            labels (np.ndarray | List, å¯é€‰): ç‚¹æç¤ºçš„æ ‡ç­¾ï¼Œå½¢çŠ¶ä¸º (N, )ã€‚1 = å‰æ™¯ï¼Œ0 = èƒŒæ™¯ã€‚
            masks (np.ndarray, å¯é€‰): æ¥è‡ªå…ˆå‰é¢„æµ‹çš„ä½åˆ†è¾¨ç‡æ©è†œï¼Œå½¢çŠ¶ä¸º (N, H, W)ã€‚å¯¹äº SAMï¼ŒH=W=256ã€‚

        è¿”å›:
            (np.ndarray): è¾“å‡ºæ©è†œï¼Œå½¢çŠ¶ä¸º CxHxWï¼Œå…¶ä¸­ C æ˜¯ç”Ÿæˆçš„æ©è†œæ•°é‡ã€‚
            (np.ndarray): é•¿åº¦ä¸º C çš„æ•°ç»„ï¼ŒåŒ…å«æ¨¡å‹ä¸ºæ¯ä¸ªæ©è†œé¢„æµ‹çš„è´¨é‡åˆ†æ•°ã€‚
        """
        # å¦‚æœ self.prompts ä¸­æœ‰ä»»ä½•å­˜å‚¨çš„æç¤ºï¼Œåˆ™è¦†ç›–å®ƒä»¬
        bboxes = self.prompts.pop("bboxes", bboxes)
        points = self.prompts.pop("points", points)
        masks = self.prompts.pop("masks", masks)

        frame = self.dataset.frame
        self.inference_state["im"] = im
        output_dict = self.inference_state["output_dict"]
        if len(output_dict["cond_frame_outputs"]) == 0:  # åˆå§‹åŒ–æç¤º
            points, labels, masks = self._prepare_prompts(im.shape[2:], bboxes, points, labels, masks)
            if points is not None:
                for i in range(len(points)):
                    self.add_new_prompts(obj_id=i, points=points[[i]], labels=labels[[i]], frame_idx=frame)
            elif masks is not None:
                for i in range(len(masks)):
                    self.add_new_prompts(obj_id=i, masks=masks[[i]], frame_idx=frame)
        self.propagate_in_video_preflight()

        consolidated_frame_inds = self.inference_state["consolidated_frame_inds"]
        batch_size = len(self.inference_state["obj_idx_to_id"])
        if len(output_dict["cond_frame_outputs"]) == 0:
            raise RuntimeError("æ²¡æœ‰æä¾›ç‚¹ï¼›è¯·å…ˆæ·»åŠ ç‚¹")

        if frame in consolidated_frame_inds["cond_frame_outputs"]:
            storage_key = "cond_frame_outputs"
            current_out = output_dict[storage_key][frame]
            if self.clear_non_cond_mem_around_input and (self.clear_non_cond_mem_for_multi_obj or batch_size <= 1):
                # æ¸…é™¤å‘¨å›´å¸§çš„éæ¡ä»¶å†…å­˜
                self._clear_non_cond_mem_around_input(frame)
        elif frame in consolidated_frame_inds["non_cond_frame_outputs"]:
            storage_key = "non_cond_frame_outputs"
            current_out = output_dict[storage_key][frame]
        else:
            storage_key = "non_cond_frame_outputs"
            current_out = self._run_single_frame_inference(
                output_dict=output_dict,
                frame_idx=frame,
                batch_size=batch_size,
                is_init_cond_frame=False,
                point_inputs=None,
                mask_inputs=None,
                reverse=False,
                run_mem_encoder=True,
            )
            output_dict[storage_key][frame] = current_out
        # ä¸ºåç»­ä¸æ¯ä¸ªå•ç‹¬ç‰©ä½“çš„äº¤äº’åˆ›å»ºæ¯ä¸ªç‰©ä½“è¾“å‡ºçš„åˆ‡ç‰‡
        self._add_output_per_object(frame, current_out, storage_key)
        self.inference_state["frames_already_tracked"].append(frame)
        pred_masks = current_out["pred_masks"].flatten(0, 1)
        pred_masks = pred_masks[(pred_masks > self.model.mask_threshold).sum((1, 2)) > 0]  # è¿‡æ»¤ç©ºç™½æ©è†œ

        return pred_masks, torch.ones(len(pred_masks), dtype=pred_masks.dtype, device=pred_masks.device)

    def postprocess(self, preds, img, orig_imgs):
        """
        å¯¹é¢„æµ‹ç»“æœè¿›è¡Œåå¤„ç†ï¼Œåº”ç”¨éé‡å çº¦æŸï¼ˆå¦‚æœéœ€è¦ï¼‰ã€‚

        è¯¥æ–¹æ³•é€šè¿‡åœ¨ `non_overlap_masks` æ ‡å¿—è®¾ç½®ä¸º True æ—¶ï¼Œå¯¹é¢„æµ‹çš„æ©è†œåº”ç”¨éé‡å çº¦æŸï¼Œæ¥æ‰©å±•åå¤„ç†åŠŸèƒ½ã€‚è¿™æ ·å¯ä»¥ç¡®ä¿æ©è†œä¸é‡å ï¼Œè¿™å¯¹äºæŸäº›åº”ç”¨éå¸¸æœ‰ç”¨ã€‚

        å‚æ•°:
            preds (Tuple[torch.Tensor]): æ¨¡å‹çš„é¢„æµ‹ç»“æœã€‚
            img (torch.Tensor): å¤„ç†åçš„å›¾åƒå¼ é‡ã€‚
            orig_imgs (List[np.ndarray]): å¤„ç†å‰çš„åŸå§‹å›¾åƒã€‚

        è¿”å›:
            results (list): åå¤„ç†åçš„é¢„æµ‹ç»“æœã€‚

        æ³¨æ„:
            å¦‚æœ `non_overlap_masks` ä¸º Trueï¼Œåˆ™è¯¥æ–¹æ³•ä¼šåº”ç”¨çº¦æŸä»¥ç¡®ä¿æ©è†œä¸é‡å ã€‚
        """
        results = super().postprocess(preds, img, orig_imgs)
        if self.non_overlap_masks:
            for result in results:
                if result.masks is None or len(result.masks) == 0:
                    continue
                result.masks.data = self.model._apply_non_overlapping_constraints(result.masks.data.unsqueeze(0))[0]
        return results

    @smart_inference_mode()
    def add_new_prompts(
        self,
        obj_id,
        points=None,
        labels=None,
        masks=None,
        frame_idx=0,
    ):
        """
        å‘ç‰¹å®šå¸§ä¸ºç»™å®šçš„å¯¹è±¡IDæ·»åŠ æ–°çš„ç‚¹æˆ–æ©è†œã€‚

        æ­¤æ–¹æ³•é€šè¿‡æ–°æç¤ºï¼ˆç‚¹æˆ–æ©è†œï¼‰æ›´æ–°æ¨ç†çŠ¶æ€ï¼Œé’ˆå¯¹æŒ‡å®šçš„å¯¹è±¡å’Œå¸§ç´¢å¼•è¿›è¡Œæ“ä½œã€‚ç¡®ä¿æ¯æ¬¡è°ƒç”¨åªèƒ½æ·»åŠ ä¸€ç§ç±»å‹çš„æç¤ºï¼ˆç‚¹æˆ–æ©è†œï¼‰ï¼Œå¹¶æ ¹æ®æä¾›çš„æç¤ºå’Œç°æœ‰çŠ¶æ€æ›´æ–°å†…éƒ¨çŠ¶æ€ã€‚å®ƒè¿˜å¤„ç†åŸºäºæä¾›çš„æç¤ºç”Ÿæˆæ–°çš„åˆ†å‰²ã€‚

        å‚æ•°ï¼š
            obj_id (int): ä¸æç¤ºç›¸å…³è”çš„å¯¹è±¡IDã€‚
            points (torch.Tensor, å¯é€‰): æ„Ÿå…´è¶£ç‚¹çš„åæ ‡ã€‚é»˜è®¤ä¸º Noneã€‚
            labels (torch.Tensor, å¯é€‰): å¯¹åº”ç‚¹çš„æ ‡ç­¾ã€‚é»˜è®¤ä¸º Noneã€‚
            masks (torch.Tensor, å¯é€‰): å¯¹è±¡çš„äºŒè¿›åˆ¶æ©è†œã€‚é»˜è®¤ä¸º Noneã€‚
            frame_idx (int, å¯é€‰): åº”ç”¨æç¤ºçš„å¸§çš„ç´¢å¼•ã€‚é»˜è®¤ä¸º 0ã€‚

        è¿”å›ï¼š
            (tuple): ä¸€ä¸ªå…ƒç»„ï¼ŒåŒ…å«å±•å¹³çš„é¢„æµ‹æ©è†œå’Œä¸€ä¸ªè¡¨ç¤ºå¯¹è±¡æ•°é‡çš„å…¨1å¼ é‡ã€‚

        å¼‚å¸¸ï¼š
            AssertionError: å¦‚æœåŒæ—¶æä¾›äº† `masks` å’Œ `points`ï¼Œæˆ–ä¸¤è€…éƒ½æ²¡æœ‰æä¾›ã€‚

        æ³¨æ„ï¼š
            - æ¯æ¬¡è°ƒç”¨æ—¶åªèƒ½æ·»åŠ ä¸€ç§ç±»å‹çš„æç¤ºï¼ˆç‚¹æˆ–æ©è†œï¼‰ã€‚
            - å¦‚æœè¯¥å¸§æ˜¯é¦–æ¬¡è¢«è¿½è¸ªï¼Œåˆ™è§†ä¸ºåˆå§‹æ¡ä»¶å¸§ã€‚
            - æ­¤æ–¹æ³•å¤„ç†è¾“å‡ºçš„åˆå¹¶å’Œæ©è†œçš„é‡æ–°è°ƒæ•´ï¼Œä»¥åŒ¹é…åŸè§†é¢‘åˆ†è¾¨ç‡ã€‚
        """
        assert (masks is None) ^ (points is None), "'masks' å’Œ 'points' æç¤ºä¸èƒ½åŒæ—¶æä¾›ï¼Œä¹Ÿä¸èƒ½éƒ½ä¸æä¾›ã€‚"
        obj_idx = self._obj_id_to_idx(obj_id)

        point_inputs = None
        pop_key = "point_inputs_per_obj"
        if points is not None:
            point_inputs = {"point_coords": points, "point_labels": labels}
            self.inference_state["point_inputs_per_obj"][obj_idx][frame_idx] = point_inputs
            pop_key = "mask_inputs_per_obj"
        self.inference_state["mask_inputs_per_obj"][obj_idx][frame_idx] = masks
        self.inference_state[pop_key][obj_idx].pop(frame_idx, None)
        
        # å¦‚æœæ­¤å¸§å°šæœªè¢«è¿½è¸ªè¿‡ï¼Œåˆ™è§†ä¸ºåˆå§‹æ¡ä»¶å¸§ï¼Œ
        # å³è¾“å…¥çš„ç‚¹ç”¨äºåœ¨æ­¤å¸§ç”Ÿæˆåˆ†å‰²ï¼Œä¸ä¾èµ–å…¶ä»–å¸§çš„å†…å­˜ï¼Œç±»ä¼¼äº SAM çš„è¡Œä¸ºã€‚
        # å¦åˆ™ï¼ˆå¦‚æœå·²ç»è¢«è¿½è¸ªè¿‡ï¼‰ï¼Œè¾“å…¥çš„ç‚¹ç”¨äºä¿®æ­£å·²è¿½è¸ªçš„æ©è†œã€‚
        is_init_cond_frame = frame_idx not in self.inference_state["frames_already_tracked"]
        obj_output_dict = self.inference_state["output_dict_per_obj"][obj_idx]
        obj_temp_output_dict = self.inference_state["temp_output_dict_per_obj"][obj_idx]
        
        # å¦‚æœæ˜¯åˆå§‹æ¡ä»¶å¸§ï¼Œæˆ–è€…æ¨¡å‹è¦æ±‚æ‰€æœ‰å¸§éƒ½ä½œä¸ºä¿®æ­£æ¡ä»¶å¸§ï¼Œåˆ™å°†è¯¥å¸§æ ‡è®°ä¸ºæ¡ä»¶å¸§
        is_cond = is_init_cond_frame or self.model.add_all_frames_to_correct_as_cond
        storage_key = "cond_frame_outputs" if is_cond else "non_cond_frame_outputs"

        # è·å–å…ˆå‰é¢„æµ‹çš„æ©è†œlogitsï¼Œå¹¶å°†å…¶ä¸æ–°çš„ç‚¹å‡»ä¸€èµ·è¾“å…¥åˆ° SAM æ©è†œè§£ç å™¨ã€‚
        prev_sam_mask_logits = None
        # å…ˆæŸ¥è¯¢ä¸´æ—¶è¾“å‡ºå­—å…¸ï¼Œå®ƒåŒ…å«æœ€æ–°çš„è¾“å‡º
        # å¦‚æœæœªæ‰¾åˆ°ï¼Œå†æŸ¥è¯¢æ¡ä»¶å¸§è¾“å‡ºå’Œéæ¡ä»¶å¸§è¾“å‡ºã€‚
        if point_inputs is not None:
            prev_out = (
                obj_temp_output_dict[storage_key].get(frame_idx)
                or obj_output_dict["cond_frame_outputs"].get(frame_idx)
                or obj_output_dict["non_cond_frame_outputs"].get(frame_idx)
            )

            if prev_out is not None and prev_out.get("pred_masks") is not None:
                prev_sam_mask_logits = prev_out["pred_masks"].to(device=self.device, non_blocking=True)
                # é™åˆ¶prev_sam_mask_logitsçš„å€¼åŸŸï¼Œé¿å…ç½•è§çš„æ•°å€¼é—®é¢˜ã€‚
                prev_sam_mask_logits.clamp_(-32.0, 32.0)
        current_out = self._run_single_frame_inference(
            output_dict=obj_output_dict,  # åœ¨å•ä¸ªå¯¹è±¡çš„åˆ‡ç‰‡ä¸Šè¿è¡Œ
            frame_idx=frame_idx,
            batch_size=1,  # åœ¨å•ä¸ªå¯¹è±¡çš„åˆ‡ç‰‡ä¸Šè¿è¡Œ
            is_init_cond_frame=is_init_cond_frame,
            point_inputs=point_inputs,
            mask_inputs=masks,
            reverse=False,
            # æ·»åŠ ç‚¹å‡»æˆ–æ©è†œæ—¶è·³è¿‡å†…å­˜ç¼–ç å™¨ã€‚æˆ‘ä»¬åœ¨`propagate_in_video`çš„å¼€å¤´æ‰§è¡Œå†…å­˜ç¼–ç å™¨ï¼ˆç”¨æˆ·ç¡®è®¤ä»–ä»¬çš„ç‚¹å‡»ä¹‹åï¼‰ã€‚
            # è¿™æ ·å¯ä»¥ç¡®ä¿æ‰€æœ‰å¯¹è±¡çš„éé‡å çº¦æŸè¢«æ‰§è¡Œï¼Œè€Œä¸æ˜¯ç¼–ç æˆå†…å­˜ã€‚
            run_mem_encoder=False,
            prev_sam_mask_logits=prev_sam_mask_logits,
        )
        # å°†è¾“å‡ºæ·»åŠ åˆ°è¾“å‡ºå­—å…¸ä¸­ï¼Œä»¥ä¾›æœªæ¥ä½œä¸ºå†…å­˜ä½¿ç”¨
        obj_temp_output_dict[storage_key][frame_idx] = current_out

        # å°†è¾“å‡ºæ©è†œè°ƒæ•´ä¸ºåŸå§‹è§†é¢‘åˆ†è¾¨ç‡
        consolidated_out = self._consolidate_temp_output_across_obj(
            frame_idx,
            is_cond=is_cond,
            run_mem_encoder=False,
        )
        pred_masks = consolidated_out["pred_masks"].flatten(0, 1)
        return pred_masks.flatten(0, 1), torch.ones(1, dtype=pred_masks.dtype, device=pred_masks.device)

    @smart_inference_mode()
    def propagate_in_video_preflight(self):
        """
        å‡†å¤‡æ¨ç†çŠ¶æ€å¹¶åˆå¹¶ä¸´æ—¶è¾“å‡ºï¼Œå¯åŠ¨è·Ÿè¸ªã€‚

        è¯¥æ–¹æ³•æ ‡å¿—ç€è·Ÿè¸ªçš„å¼€å§‹ï¼Œå¹¶ä¸”ä¸å…è®¸åœ¨ä¼šè¯é‡ç½®ä¹‹å‰æ·»åŠ æ–°å¯¹è±¡ã€‚
        å®ƒå°† `temp_output_dict_per_obj` ä¸­çš„ä¸´æ—¶è¾“å‡ºåˆå¹¶åˆ° `output_dict` ä¸­ã€‚
        æ­¤å¤–ï¼Œå®ƒè¿˜æ¸…é™¤äº†è¾“å…¥å¸§å‘¨å›´çš„éæ¡ä»¶å†…å­˜ï¼Œå¹¶ç¡®ä¿çŠ¶æ€ä¸æä¾›çš„è¾“å…¥ä¸€è‡´ã€‚
        """
        # è·Ÿè¸ªå·²å¼€å§‹ï¼Œå¹¶ä¸”åœ¨ä¼šè¯é‡ç½®ä¹‹å‰ä¸å…è®¸æ·»åŠ æ–°å¯¹è±¡ã€‚
        self.inference_state["tracking_has_started"] = True
        batch_size = len(self.inference_state["obj_idx_to_id"])

        # åˆå¹¶æ¯ä¸ªå¯¹è±¡çš„ä¸´æ—¶è¾“å‡ºï¼ˆå­˜å‚¨åœ¨ "temp_output_dict_per_obj" ä¸­ï¼‰ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ° "output_dict" ä¸­ã€‚
        temp_output_dict_per_obj = self.inference_state["temp_output_dict_per_obj"]
        output_dict = self.inference_state["output_dict"]
        # "consolidated_frame_inds" åŒ…å«å·²åˆå¹¶ä¸´æ—¶è¾“å‡ºçš„å¸§ç´¢å¼•ï¼ˆè¿™äº›å¸§å¯èƒ½æ˜¯æœ¬æ¬¡è°ƒç”¨åˆå¹¶çš„ï¼Œä¹Ÿå¯èƒ½æ˜¯ä¹‹å‰è°ƒç”¨çš„ `propagate_in_video_preflight` åˆå¹¶çš„ï¼‰ã€‚
        consolidated_frame_inds = self.inference_state["consolidated_frame_inds"]
        for is_cond in {False, True}:
            # åˆ†åˆ«åˆå¹¶æ¡ä»¶å’Œéæ¡ä»¶çš„ä¸´æ—¶è¾“å‡º
            storage_key = "cond_frame_outputs" if is_cond else "non_cond_frame_outputs"
            # æŸ¥æ‰¾åŒ…å«ä»»ä½•å¯¹è±¡ä¸´æ—¶è¾“å‡ºçš„å¸§ï¼ˆè¿™äº›åº”è¯¥æ˜¯åˆšåˆšæ¥æ”¶åˆ°ç‚¹å‡»ä½œä¸ºæ©è†œè¾“å…¥çš„å¸§ï¼‰
            temp_frame_inds = set()
            for obj_temp_output_dict in temp_output_dict_per_obj.values():
                temp_frame_inds.update(obj_temp_output_dict[storage_key].keys())
            consolidated_frame_inds[storage_key].update(temp_frame_inds)
            # åˆå¹¶è¯¥å¸§ä¸Šæ‰€æœ‰å¯¹è±¡çš„ä¸´æ—¶è¾“å‡º
            for frame_idx in temp_frame_inds:
                consolidated_out = self._consolidate_temp_output_across_obj(
                    frame_idx, is_cond=is_cond, run_mem_encoder=True
                )
                # å°†å®ƒä»¬åˆå¹¶åˆ° "output_dict" ä¸­ï¼Œå¹¶ä¸ºæ¯ä¸ªå¯¹è±¡åˆ›å»ºåˆ‡ç‰‡
                output_dict[storage_key][frame_idx] = consolidated_out
                self._add_output_per_object(frame_idx, consolidated_out, storage_key)
                if self.clear_non_cond_mem_around_input and (self.clear_non_cond_mem_for_multi_obj or batch_size <= 1):
                    # æ¸…é™¤è¾“å…¥å¸§å‘¨å›´çš„éæ¡ä»¶å†…å­˜
                    self._clear_non_cond_mem_around_input(frame_idx)

            # æ¸…é™¤ `temp_output_dict_per_obj` ä¸­çš„ä¸´æ—¶è¾“å‡º
            for obj_temp_output_dict in temp_output_dict_per_obj.values():
                obj_temp_output_dict[storage_key].clear()

        # ç‰¹æ®Šæƒ…å†µï¼šå¦‚æœå·²å‘ "cond_frame_outputs" æ·»åŠ è¾“å‡ºï¼Œåˆ™åº”åˆ é™¤ç›¸åŒå¸§åœ¨ "non_cond_frame_outputs" ä¸­çš„ä»»ä½•è¾“å‡º
        for frame_idx in output_dict["cond_frame_outputs"]:
            output_dict["non_cond_frame_outputs"].pop(frame_idx, None)
        for obj_output_dict in self.inference_state["output_dict_per_obj"].values():
            for frame_idx in obj_output_dict["cond_frame_outputs"]:
                obj_output_dict["non_cond_frame_outputs"].pop(frame_idx, None)
        for frame_idx in consolidated_frame_inds["cond_frame_outputs"]:
            assert frame_idx in output_dict["cond_frame_outputs"]
            consolidated_frame_inds["non_cond_frame_outputs"].discard(frame_idx)

        # ç¡®ä¿ "consolidated_frame_inds" ä¸­çš„å¸§ç´¢å¼•æ­£å¥½æ˜¯é‚£äº›æœ‰ç‚¹æˆ–æ©è†œè¾“å…¥çš„å¸§ï¼ˆåœ¨æ­£ç¡®çš„å·¥ä½œæµä¸‹åº”å½“æˆç«‹ï¼‰ã€‚
        all_consolidated_frame_inds = (
            consolidated_frame_inds["cond_frame_outputs"] | consolidated_frame_inds["non_cond_frame_outputs"]
        )
        input_frames_inds = set()
        for point_inputs_per_frame in self.inference_state["point_inputs_per_obj"].values():
            input_frames_inds.update(point_inputs_per_frame.keys())
        for mask_inputs_per_frame in self.inference_state["mask_inputs_per_obj"].values():
            input_frames_inds.update(mask_inputs_per_frame.keys())
        assert all_consolidated_frame_inds == input_frames_inds

    @staticmethod
    def init_state(predictor):
        """
        åˆå§‹åŒ–é¢„æµ‹å™¨çš„æ¨ç†çŠ¶æ€ã€‚

        æ­¤å‡½æ•°è®¾ç½®æ‰§è¡Œè§†é¢‘æ•°æ®æ¨ç†æ‰€éœ€çš„åˆå§‹çŠ¶æ€ã€‚å®ƒåŒ…æ‹¬åˆå§‹åŒ–å„ç§å­—å…¸å’Œæœ‰åºå­—å…¸ï¼Œç”¨äºå­˜å‚¨ä¸è·Ÿè¸ªè¿‡ç¨‹ç›¸å…³çš„è¾“å…¥ã€è¾“å‡ºå’Œå…¶ä»–å…ƒæ•°æ®ã€‚

        å‚æ•°:
            predictor (SAM2VideoPredictor): ç”¨äºåˆå§‹åŒ–çŠ¶æ€çš„é¢„æµ‹å™¨å¯¹è±¡ã€‚
        """
        if len(predictor.inference_state) > 0:  # è¡¨ç¤ºå·²åˆå§‹åŒ–
            return
        assert predictor.dataset is not None
        assert predictor.dataset.mode == "video"

        inference_state = {
            "num_frames": predictor.dataset.frames,
            "point_inputs_per_obj": {},  # æ¯å¸§ä¸Šçš„è¾“å…¥ç‚¹
            "mask_inputs_per_obj": {},  # æ¯å¸§ä¸Šçš„è¾“å…¥æ©ç 
            "constants": {},  # ä¸éšå¸§å˜åŒ–çš„å€¼ï¼ˆå› æ­¤åªéœ€ä¿ç•™ä¸€ä»½ï¼‰
            # å®¢æˆ·ç«¯å¯¹è±¡ ID ä¸æ¨¡å‹ç«¯å¯¹è±¡ç´¢å¼•ä¹‹é—´çš„æ˜ å°„
            "obj_id_to_idx": OrderedDict(),
            "obj_idx_to_id": OrderedDict(),
            "obj_ids": [],
            # ç”¨äºå­˜å‚¨æ¯å¸§ä¸Šçš„æ¨¡å‹è·Ÿè¸ªç»“æœå’ŒçŠ¶æ€çš„å­˜å‚¨ç©ºé—´
            "output_dict": {
                "cond_frame_outputs": {},  # å­—å…¸ï¼ŒåŒ…å« {frame_idx: <out>}
                "non_cond_frame_outputs": {},  # å­—å…¸ï¼ŒåŒ…å« {frame_idx: <out>}
            },
            # æ¯ä¸ªå¯¹è±¡è·Ÿè¸ªç»“æœçš„åˆ‡ç‰‡ï¼ˆè§†å›¾ï¼‰ï¼Œä¸ "output_dict" å…±äº«å†…å­˜
            "output_dict_per_obj": {},
            # ä¸´æ—¶å­˜å‚¨ï¼Œç”¨äºä¿å­˜ç”¨æˆ·ä¸å¸§äº¤äº’æ—¶çš„æ–°è¾“å‡º
            # ï¼ˆå®ƒåœ¨ä¼ æ’­å¼€å§‹å‰åˆå¹¶åˆ° "output_dict" ä¸­ï¼‰
            "temp_output_dict_per_obj": {},
            # å·²ç»ä¿å­˜äº†ç‚¹å‡»æˆ–æ©ç è¾“å…¥åˆå¹¶ç»“æœçš„å¸§
            # ï¼ˆæˆ‘ä»¬ç›´æ¥åœ¨è·Ÿè¸ªè¿‡ç¨‹ä¸­ä½¿ç”¨å®ƒä»¬çš„åˆå¹¶è¾“å‡ºï¼‰
            "consolidated_frame_inds": {
                "cond_frame_outputs": set(),  # åŒ…å«å¸§ç´¢å¼•çš„é›†åˆ
                "non_cond_frame_outputs": set(),  # åŒ…å«å¸§ç´¢å¼•çš„é›†åˆ
            },
            # æ¯å¸§è·Ÿè¸ªçš„å…ƒæ•°æ®ï¼ˆä¾‹å¦‚ï¼šè·Ÿè¸ªçš„æ–¹å‘ï¼‰
            "tracking_has_started": False,
            "frames_already_tracked": [],
        }
        predictor.inference_state = inference_state

    def get_im_features(self, im, batch=1):
        """
        ä½¿ç”¨ SAM2 çš„å›¾åƒç¼–ç å™¨æå–å¹¶å¤„ç†å›¾åƒç‰¹å¾ï¼Œä»¥ä¾¿åç»­çš„åˆ†å‰²ä»»åŠ¡ã€‚

        å‚æ•°:
            im (torch.Tensor): è¾“å…¥çš„å›¾åƒå¼ é‡ã€‚
            batch (int, å¯é€‰): å¦‚æœæœ‰å¤šä¸ªæç¤ºï¼Œåˆ™æ‰©å±•ç‰¹å¾çš„æ‰¹æ¬¡å¤§å°ã€‚é»˜è®¤ä¸º 1ã€‚

        è¿”å›:
            vis_feats (torch.Tensor): ä»å›¾åƒä¸­æå–çš„è§†è§‰ç‰¹å¾ã€‚
            vis_pos_embed (torch.Tensor): è§†è§‰ç‰¹å¾çš„ä½ç½®ä¿¡æ¯åµŒå…¥ã€‚
            feat_sizes (List(Tuple[int])): åŒ…å«æå–ç‰¹å¾å¤§å°çš„åˆ—è¡¨ã€‚

        æ³¨æ„:
            - å¦‚æœ `batch` å¤§äº 1ï¼Œç‰¹å¾å°†æ‰©å±•ä»¥é€‚åº”æ‰¹æ¬¡å¤§å°ã€‚
            - è¯¥æ–¹æ³•åˆ©ç”¨æ¨¡å‹çš„ `_prepare_backbone_features` æ–¹æ³•æ¥å‡†å¤‡éª¨å¹²ç½‘ç»œç‰¹å¾ã€‚
        """
        backbone_out = self.model.forward_image(im)
        if batch > 1:  # å¦‚æœæœ‰å¤šä¸ªæç¤ºï¼Œæ‰©å±•ç‰¹å¾
            for i, feat in enumerate(backbone_out["backbone_fpn"]):
                backbone_out["backbone_fpn"][i] = feat.expand(batch, -1, -1, -1)
            for i, pos in enumerate(backbone_out["vision_pos_enc"]):
                pos = pos.expand(batch, -1, -1, -1)
                backbone_out["vision_pos_enc"][i] = pos
        _, vis_feats, vis_pos_embed, feat_sizes = self.model._prepare_backbone_features(backbone_out)
        return vis_feats, vis_pos_embed, feat_sizes

    def _obj_id_to_idx(self, obj_id):
        """
        å°†å®¢æˆ·ç«¯å¯¹è±¡ ID æ˜ å°„åˆ°æ¨¡å‹ç«¯å¯¹è±¡ç´¢å¼•ã€‚

        å‚æ•°:
            obj_id (int): å®¢æˆ·ç«¯æä¾›çš„å¯¹è±¡çš„å”¯ä¸€æ ‡è¯†ç¬¦ã€‚

        è¿”å›:
            obj_idx (int): æ¨¡å‹ç«¯çš„å¯¹è±¡ç´¢å¼•ã€‚

        å¼‚å¸¸:
            RuntimeError: å¦‚æœåœ¨è·Ÿè¸ªå¼€å§‹åå°è¯•æ·»åŠ æ–°å¯¹è±¡ï¼Œåˆ™æŠ›å‡ºæ­¤å¼‚å¸¸ã€‚

        æ³¨æ„:
            - è¯¥æ–¹æ³•æ›´æ–°æˆ–æ£€ç´¢å­˜å‚¨åœ¨ `inference_state` ä¸­çš„å¯¹è±¡ ID ä¸ç´¢å¼•ä¹‹é—´çš„æ˜ å°„ã€‚
            - å®ƒç¡®ä¿åªèƒ½åœ¨è·Ÿè¸ªå¼€å§‹ä¹‹å‰æ·»åŠ æ–°å¯¹è±¡ã€‚
            - å®ƒç»´æŠ¤ ID å’Œç´¢å¼•ä¹‹é—´çš„åŒå‘æ˜ å°„ï¼ˆ`obj_id_to_idx` å’Œ `obj_idx_to_id`ï¼‰ã€‚
            - ä¸ºæ–°å¯¹è±¡åˆå§‹åŒ–äº†é¢å¤–çš„æ•°æ®ç»“æ„ï¼Œç”¨äºå­˜å‚¨è¾“å…¥å’Œè¾“å‡ºã€‚
        """
        obj_idx = self.inference_state["obj_id_to_idx"].get(obj_id, None)
        if obj_idx is not None:
            return obj_idx

        # è¿™æ˜¯ä¸€ä¸ªæ–°å¯¹è±¡ IDï¼Œåœ¨æœåŠ¡å™¨ä¹‹å‰æ²¡æœ‰å‘é€è¿‡ã€‚æˆ‘ä»¬åªå…è®¸åœ¨
        # è·Ÿè¸ªå¼€å§‹å‰æ·»åŠ æ–°å¯¹è±¡ã€‚
        allow_new_object = not self.inference_state["tracking_has_started"]
        if allow_new_object:
            # è·å–ä¸‹ä¸€ä¸ªå¯¹è±¡æ§½ä½
            obj_idx = len(self.inference_state["obj_id_to_idx"])
            self.inference_state["obj_id_to_idx"][obj_id] = obj_idx
            self.inference_state["obj_idx_to_id"][obj_idx] = obj_id
            self.inference_state["obj_ids"] = list(self.inference_state["obj_id_to_idx"])
            # ä¸ºè¯¥å¯¹è±¡è®¾ç½®è¾“å…¥è¾“å‡ºç»“æ„
            self.inference_state["point_inputs_per_obj"][obj_idx] = {}
            self.inference_state["mask_inputs_per_obj"][obj_idx] = {}
            self.inference_state["output_dict_per_obj"][obj_idx] = {
                "cond_frame_outputs": {},  # å­—å…¸ï¼ŒåŒ…å« {frame_idx: <out>}
                "non_cond_frame_outputs": {},  # å­—å…¸ï¼ŒåŒ…å« {frame_idx: <out>}
            }
            self.inference_state["temp_output_dict_per_obj"][obj_idx] = {
                "cond_frame_outputs": {},  # å­—å…¸ï¼ŒåŒ…å« {frame_idx: <out>}
                "non_cond_frame_outputs": {},  # å­—å…¸ï¼ŒåŒ…å« {frame_idx: <out>}
            }
            return obj_idx
        else:
            raise RuntimeError(
                f"è·Ÿè¸ªå¼€å§‹åæ— æ³•æ·»åŠ æ–°å¯¹è±¡ ID {obj_id}. "
                f"æ‰€æœ‰ç°æœ‰å¯¹è±¡ ID: {self.inference_state['obj_ids']}. "
                f"è¯·è°ƒç”¨ 'reset_state' é‡æ–°å¼€å§‹ã€‚"
            )

    def _run_single_frame_inference(
        self,
        output_dict,
        frame_idx,
        batch_size,
        is_init_cond_frame,
        point_inputs,
        mask_inputs,
        reverse,
        run_mem_encoder,
        prev_sam_mask_logits=None,
    ):
        """
        æ ¹æ®å½“å‰è¾“å…¥å’Œå…ˆå‰çš„è®°å¿†åœ¨å•å¸§ä¸Šè¿è¡Œè·Ÿè¸ªã€‚

        å‚æ•°ï¼š
            output_dict (Dict): åŒ…å«è·Ÿè¸ªè¿‡ç¨‹è¾“å‡ºçŠ¶æ€çš„å­—å…¸ã€‚
            frame_idx (int): å½“å‰å¸§çš„ç´¢å¼•ã€‚
            batch_size (int): å¤„ç†è¯¥å¸§æ—¶çš„æ‰¹å¤§å°ã€‚
            is_init_cond_frame (bool): è¡¨ç¤ºå½“å‰å¸§æ˜¯å¦ä¸ºåˆå§‹åŒ–æ¡ä»¶å¸§ã€‚
            point_inputs (Dict, å¯é€‰): è¾“å…¥ç‚¹åŠå…¶æ ‡ç­¾ã€‚é»˜è®¤ä¸ºNoneã€‚
            mask_inputs (torch.Tensor, å¯é€‰): è¾“å…¥çš„äºŒå€¼æ©ç ã€‚é»˜è®¤ä¸ºNoneã€‚
            reverse (bool): è¡¨ç¤ºæ˜¯å¦åº”ä»¥åå‘é¡ºåºæ‰§è¡Œè·Ÿè¸ªã€‚
            run_mem_encoder (bool): è¡¨ç¤ºæ˜¯å¦åº”æ‰§è¡Œå†…å­˜ç¼–ç å™¨ã€‚
            prev_sam_mask_logits (torch.Tensor, å¯é€‰): å½“å‰å¯¹è±¡çš„å…ˆå‰æ©ç é€»è¾‘ã€‚é»˜è®¤ä¸ºNoneã€‚

        è¿”å›ï¼š
            current_out (dict): åŒ…å«è·Ÿè¸ªæ­¥éª¤è¾“å‡ºçš„å­—å…¸ï¼ŒåŒ…æ‹¬æ›´æ–°çš„ç‰¹å¾å’Œé¢„æµ‹ã€‚

        å¼‚å¸¸ï¼š
            AssertionError: å¦‚æœåŒæ—¶æä¾›`point_inputs`å’Œ`mask_inputs`ï¼Œæˆ–è€…ä¸¤è€…éƒ½æ²¡æœ‰æä¾›ã€‚

        æ³¨æ„ï¼š
            - è¯¥æ–¹æ³•å‡å®š`point_inputs`å’Œ`mask_inputs`æ˜¯äº’æ–¥çš„ã€‚
            - è¯¥æ–¹æ³•ä½¿ç”¨`get_im_features`æ–¹æ³•æ£€ç´¢å›¾åƒç‰¹å¾ã€‚
            - `maskmem_pos_enc`å‡å®šåœ¨å„å¸§é—´ä¿æŒä¸å˜ï¼Œå› æ­¤åªå­˜å‚¨ä¸€ä»½ã€‚
            - ç”±äºéœ€è¦CUDAæ‰©å±•ï¼Œ`fill_holes_in_mask_scores`å‡½æ•°è¢«æ³¨é‡Šæ‰ï¼Œå½“å‰ä¸æ”¯æŒã€‚
        """
        # è·å–æ­£ç¡®çš„å›¾åƒç‰¹å¾
        current_vision_feats, current_vision_pos_embeds, feat_sizes = self.get_im_features(
            self.inference_state["im"], batch_size
        )

        # ç¡®ä¿åŒä¸€å¸§ä¸­`point_inputs`å’Œ`mask_inputs`ä¸èƒ½åŒæ—¶å‡ºç°
        assert point_inputs is None or mask_inputs is None
        current_out = self.model.track_step(
            frame_idx=frame_idx,
            is_init_cond_frame=is_init_cond_frame,
            current_vision_feats=current_vision_feats,
            current_vision_pos_embeds=current_vision_pos_embeds,
            feat_sizes=feat_sizes,
            point_inputs=point_inputs,
            mask_inputs=mask_inputs,
            output_dict=output_dict,
            num_frames=self.inference_state["num_frames"],
            track_in_reverse=reverse,
            run_mem_encoder=run_mem_encoder,
            prev_sam_mask_logits=prev_sam_mask_logits,
        )

        maskmem_features = current_out["maskmem_features"]
        if maskmem_features is not None:
            current_out["maskmem_features"] = maskmem_features.to(
                dtype=torch.float16, device=self.device, non_blocking=True
            )
        # æ³¨æ„ï¼šç”±äºéœ€è¦CUDAæ‰©å±•ï¼Œå½“å‰ä¸æ”¯æŒ`fill_holes_in_mask_scores`å‡½æ•°
        # å¯èƒ½ä¼šå¡«å……é¢„æµ‹æ©ç ä¸­çš„ç©ºæ´
        # if self.fill_hole_area > 0:
        #     pred_masks = current_out["pred_masks"].to(self.device, non_blocking=True)
        #     pred_masks = fill_holes_in_mask_scores(pred_masks, self.fill_hole_area)

        # "maskmem_pos_enc"åœ¨å„å¸§é—´ä¿æŒä¸€è‡´ï¼Œå› æ­¤åªéœ€è¦å­˜å‚¨ä¸€ä»½
        current_out["maskmem_pos_enc"] = self._get_maskmem_pos_enc(current_out["maskmem_pos_enc"])
        return current_out

    def _get_maskmem_pos_enc(self, out_maskmem_pos_enc):
        """
        åœ¨å„å¸§å’Œå¯¹è±¡ä¹‹é—´ç¼“å­˜å’Œç®¡ç†æ©ç è®°å¿†çš„ä½ç½®ä¿¡æ¯ã€‚

        æ­¤æ–¹æ³•é€šè¿‡ç¼“å­˜æ©ç è®°å¿†çš„ä½ç½®ä¿¡æ¯ï¼ˆ`maskmem_pos_enc`ï¼‰æ¥ä¼˜åŒ–å­˜å‚¨ï¼Œ
        å› ä¸ºè¯¥ä¿¡æ¯åœ¨å„å¸§å’Œå¯¹è±¡ä¹‹é—´æ˜¯æ’å®šçš„ï¼Œå› æ­¤å‡å°‘äº†åœ¨æ¨ç†è¿‡ç¨‹ä¸­å­˜å‚¨å†—ä½™ä¿¡æ¯çš„éœ€æ±‚ã€‚
        å®ƒä¼šæ£€æŸ¥ä½ç½®ä¿¡æ¯æ˜¯å¦å·²ç»ç¼“å­˜ï¼Œå¦‚æœæ²¡æœ‰ï¼Œåˆ™ç¼“å­˜æä¾›çš„ç¼–ç ç‰‡æ®µã€‚
        å¦‚æœæ‰¹é‡å¤§å°å¤§äºä¸€ï¼Œåˆ™æ‰©å±•ç¼“å­˜çš„ä½ç½®ä¿¡æ¯ä»¥åŒ¹é…å½“å‰çš„æ‰¹é‡å¤§å°ã€‚

        å‚æ•°ï¼š
            out_maskmem_pos_enc (List[torch.Tensor] æˆ– None): æ©ç è®°å¿†çš„ä½ç½®ä¿¡æ¯ã€‚
                åº”è¯¥æ˜¯ä¸€ä¸ªå¼ é‡åˆ—è¡¨æˆ–è€…Noneã€‚

        è¿”å›ï¼š
            out_maskmem_pos_enc (List[torch.Tensor]): æ©ç è®°å¿†çš„ä½ç½®ä¿¡æ¯ï¼Œå¯èƒ½æ˜¯ç¼“å­˜çš„æˆ–è€…æ‰©å±•åçš„ã€‚

        æ³¨æ„ï¼š
            - è¯¥æ–¹æ³•å‡å®š`out_maskmem_pos_enc`æ˜¯ä¸€ä¸ªå¼ é‡åˆ—è¡¨æˆ–è€…Noneã€‚
            - ç”±äºè¯¥ç¼–ç åœ¨å¯¹è±¡ä¹‹é—´ç›¸åŒï¼Œå› æ­¤åªç¼“å­˜ä¸€ä¸ªå¯¹è±¡çš„åˆ‡ç‰‡ã€‚
            - è¯¥æ–¹æ³•æ£€æŸ¥ä½ç½®ä¿¡æ¯æ˜¯å¦å·²ç»ç¼“å­˜ï¼Œå¹¶å°†å…¶å­˜å‚¨åœ¨ä¼šè¯çš„å¸¸é‡ä¸­ã€‚
            - å¦‚æœæ‰¹é‡å¤§å°å¤§äºä¸€ï¼Œåˆ™æ‰©å±•ç¼“å­˜çš„`maskmem_pos_enc`ä»¥é€‚åº”æ‰¹é‡å¤§å°ã€‚
        """
        model_constants = self.inference_state["constants"]
        # "out_maskmem_pos_enc"åº”è¯¥æ˜¯ä¸€ä¸ªå¼ é‡åˆ—è¡¨æˆ–None
        if out_maskmem_pos_enc is not None:
            if "maskmem_pos_enc" not in model_constants:
                assert isinstance(out_maskmem_pos_enc, list)
                # åªå–ä¸€ä¸ªå¯¹è±¡çš„åˆ‡ç‰‡ï¼Œå› ä¸ºå®ƒåœ¨å„å¯¹è±¡é—´æ˜¯ç›¸åŒçš„
                maskmem_pos_enc = [x[:1].clone() for x in out_maskmem_pos_enc]
                model_constants["maskmem_pos_enc"] = maskmem_pos_enc
            else:
                maskmem_pos_enc = model_constants["maskmem_pos_enc"]
            # å°†ç¼“å­˜çš„maskmem_pos_encæ‰©å±•åˆ°å®é™…çš„æ‰¹é‡å¤§å°
            batch_size = out_maskmem_pos_enc[0].size(0)
            if batch_size > 1:
                out_maskmem_pos_enc = [x.expand(batch_size, -1, -1, -1) for x in maskmem_pos_enc]
        return out_maskmem_pos_enc

    def _consolidate_temp_output_across_obj(
        self,
        frame_idx,
        is_cond=False,
        run_mem_encoder=False,
    ):
        """
        åˆå¹¶æ¯ä¸ªå¯¹è±¡çš„ä¸´æ—¶è¾“å‡ºï¼Œç”Ÿæˆæ‰€æœ‰å¯¹è±¡çš„ç»Ÿä¸€è¾“å‡ºã€‚

        è¯¥æ–¹æ³•å°†æ¯ä¸ªå¯¹è±¡åœ¨ç»™å®šå¸§ä¸Šçš„ä¸´æ—¶è¾“å‡ºåˆå¹¶æˆä¸€ä¸ªç»Ÿä¸€çš„è¾“å‡ºã€‚
        å®ƒä¼šå¡«å……ç¼ºå¤±çš„å¯¹è±¡ï¼Œæ— è®ºæ˜¯ä»ä¸»è¾“å‡ºå­—å…¸ä¸­è·å–ï¼Œè¿˜æ˜¯åœ¨ä¸»è¾“å‡ºä¸­ä¸å­˜åœ¨æ—¶ç•™ä¸‹å ä½ç¬¦ã€‚å¦‚æœéœ€è¦ï¼Œæ–¹æ³•è¿˜å¯ä»¥åœ¨åº”ç”¨éé‡å çº¦æŸåé‡æ–°è¿è¡Œå†…å­˜ç¼–ç å™¨ã€‚

        å‚æ•°ï¼š
            frame_idx (int): è¦åˆå¹¶è¾“å‡ºçš„å¸§çš„ç´¢å¼•ã€‚
            is_cond (bool, å¯é€‰): æŒ‡ç¤ºè¯¥å¸§æ˜¯å¦è¢«è§†ä¸ºæ¡ä»¶å¸§ã€‚é»˜è®¤å€¼ä¸ºFalseã€‚
            run_mem_encoder (bool, å¯é€‰): æŒ‡å®šæ˜¯å¦åœ¨åˆå¹¶è¾“å‡ºåé‡æ–°è¿è¡Œå†…å­˜ç¼–ç å™¨ã€‚é»˜è®¤å€¼ä¸ºFalseã€‚

        è¿”å›ï¼š
            consolidated_out (dict): ä¸€ä¸ªåˆå¹¶åçš„è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«æ‰€æœ‰å¯¹è±¡çš„åˆå¹¶ç»“æœã€‚

        å¤‡æ³¨ï¼š
            - è¯¥æ–¹æ³•åˆå§‹åŒ–åˆå¹¶è¾“å‡ºæ—¶ä½¿ç”¨å ä½ç¬¦å€¼æ¥å¤„ç†ç¼ºå¤±çš„å¯¹è±¡ã€‚
            - å®ƒåœ¨ä¸´æ—¶è¾“å‡ºå­—å…¸å’Œä¸»è¾“å‡ºå­—å…¸ä¸­æŸ¥æ‰¾æ¯ä¸ªå¯¹è±¡çš„è¾“å‡ºã€‚
            - å¦‚æœ `run_mem_encoder` ä¸ºTrueï¼Œå®ƒä¼šåº”ç”¨éé‡å çº¦æŸï¼Œå¹¶é‡æ–°è¿è¡Œå†…å­˜ç¼–ç å™¨ã€‚
            - `maskmem_features` å’Œ `maskmem_pos_enc` åªæœ‰åœ¨ `run_mem_encoder` ä¸ºTrueæ—¶æ‰ä¼šè¢«å¡«å……ã€‚
        """
        batch_size = len(self.inference_state["obj_idx_to_id"])
        storage_key = "cond_frame_outputs" if is_cond else "non_cond_frame_outputs"

        # åˆå§‹åŒ– `consolidated_out`ã€‚å®ƒçš„ "maskmem_features" å’Œ "maskmem_pos_enc"
        # å°†åœ¨é‡æ–°è¿è¡Œå†…å­˜ç¼–ç å™¨å¹¶åº”ç”¨éé‡å çº¦æŸåå¡«å……ã€‚å®ƒçš„ "pred_masks" è¢«é¢„å¡«å……ä¸ºä¸€ä¸ªå¤§
        # è´Ÿå€¼ï¼ˆNO_OBJ_SCOREï¼‰ä»¥è¡¨ç¤ºç¼ºå¤±çš„å¯¹è±¡ã€‚
        consolidated_out = {
            "maskmem_features": None,
            "maskmem_pos_enc": None,
            "pred_masks": torch.full(
                size=(batch_size, 1, self.imgsz[0] // 4, self.imgsz[1] // 4),
                fill_value=-1024.0,
                dtype=torch.float32,
                device=self.device,
            ),
            "obj_ptr": torch.full(
                size=(batch_size, self.model.hidden_dim),
                fill_value=-1024.0,
                dtype=torch.float32,
                device=self.device,
            ),
            "object_score_logits": torch.full(
                size=(batch_size, 1),
                # é»˜è®¤æƒ…å†µä¸‹ä¸º10.0ï¼Œè¡¨ç¤ºå¯¹è±¡å­˜åœ¨ï¼Œå³sigmoid(10)=1ï¼Œå’Œ `MaskDecoder` ä¸­çš„ `predict_masks` ç›¸åŒã€‚
                fill_value=10.0,
                dtype=torch.float32,
                device=self.device,
            ),
        }
        for obj_idx in range(batch_size):
            obj_temp_output_dict = self.inference_state["temp_output_dict_per_obj"][obj_idx]
            obj_output_dict = self.inference_state["output_dict_per_obj"][obj_idx]
            out = (
                obj_temp_output_dict[storage_key].get(frame_idx)
                # å¦‚æœè¯¥å¯¹è±¡åœ¨ "temp_output_dict_per_obj" ä¸­æ²¡æœ‰å‡ºç°åœ¨è¯¥å¸§ï¼Œ
                # æˆ‘ä»¬ä¼šå›é€€å¹¶åœ¨ "output_dict_per_obj" ä¸­æŸ¥æ‰¾å®ƒçš„å‰ä¸€ä¸ªè¾“å‡ºã€‚
                # æˆ‘ä»¬åœ¨ "output_dict_per_obj" ä¸­æŸ¥æ‰¾ "cond_frame_outputs" å’Œ "non_cond_frame_outputs"
                # æ¥å¯»æ‰¾è¯¥å¯¹è±¡çš„å‰ä¸€ä¸ªè¾“å‡ºã€‚
                or obj_output_dict["cond_frame_outputs"].get(frame_idx)
                or obj_output_dict["non_cond_frame_outputs"].get(frame_idx)
            )
            # å¦‚æœè¯¥å¯¹è±¡åœ¨ "output_dict_per_obj" ä¸­ä¹Ÿæ²¡æœ‰æ‰¾åˆ°ï¼Œåˆ™è·³è¿‡å®ƒ
            # å¹¶å°†å…¶æ©ç å¾—åˆ†ä¿æŒä¸ºé»˜è®¤å¾—åˆ†ï¼ˆå³ä¸Šé¢çš„ NO_OBJ_SCORE å ä½ç¬¦ï¼‰ï¼Œ
            # åŒæ—¶å°†å¯¹è±¡æŒ‡é’ˆè®¾ç½®ä¸ºä¸€ä¸ªè™šæ‹ŸæŒ‡é’ˆã€‚
            if out is None:
                # å¯¹äºé‚£äº›åœ¨å½“å‰å¸§æ²¡æœ‰ä»»ä½•è¾“å…¥æˆ–è·Ÿè¸ªç»“æœçš„å¯¹è±¡ï¼ˆä»…åœ¨ `run_mem_encoder=True` ä¸‹ï¼Œ
                # å³éœ€è¦ä¸ºè·Ÿè¸ªæ„å»ºå†…å­˜æ—¶ï¼‰ï¼Œå¡«å……è™šæ‹Ÿå¯¹è±¡æŒ‡é’ˆã€‚
                if run_mem_encoder:
                    # å¡«å……å¯¹è±¡æŒ‡é’ˆä¸ºä¸€ä¸ªè™šæ‹ŸæŒ‡é’ˆï¼ˆåŸºäºç©ºæ©ç ï¼‰
                    consolidated_out["obj_ptr"][obj_idx : obj_idx + 1] = self._get_empty_mask_ptr(frame_idx)
                continue
            # å°†ä¸´æ—¶å¯¹è±¡è¾“å‡ºæ©ç æ·»åŠ åˆ°åˆå¹¶è¾“å‡ºçš„æ©ç ä¸­
            consolidated_out["pred_masks"][obj_idx : obj_idx + 1] = out["pred_masks"]
            consolidated_out["obj_ptr"][obj_idx : obj_idx + 1] = out["obj_ptr"]

        # å¯é€‰åœ°ï¼Œå¯¹åˆå¹¶åçš„å¾—åˆ†åº”ç”¨éé‡å çº¦æŸï¼Œå¹¶é‡æ–°è¿è¡Œå†…å­˜ç¼–ç å™¨
        if run_mem_encoder:
            high_res_masks = F.interpolate(
                consolidated_out["pred_masks"],
                size=self.imgsz,
                mode="bilinear",
                align_corners=False,
            )
            if self.model.non_overlap_masks_for_mem_enc:
                high_res_masks = self.model._apply_non_overlapping_constraints(high_res_masks)
            consolidated_out["maskmem_features"], consolidated_out["maskmem_pos_enc"] = self._run_memory_encoder(
                batch_size=batch_size,
                high_res_masks=high_res_masks,
                is_mask_from_pts=True,  # è¿™äº›å¸§æ˜¯ç”¨æˆ·äº¤äº’çš„å¸§
                object_score_logits=consolidated_out["object_score_logits"],
            )

        return consolidated_out

    def _get_empty_mask_ptr(self, frame_idx):
        """
        åŸºäºå½“å‰å¸§çš„ç©ºæ©ç ç”Ÿæˆä¸€ä¸ªè™šæ‹Ÿå¯¹è±¡æŒ‡é’ˆã€‚

        å‚æ•°ï¼š
            frame_idx (int): å½“å‰å¸§çš„ç´¢å¼•ï¼Œç”¨äºç”Ÿæˆè™šæ‹Ÿå¯¹è±¡æŒ‡é’ˆã€‚

        è¿”å›ï¼š
            (torch.Tensor): ä¸€ä¸ªåŸºäºç©ºæ©ç ç”Ÿæˆçš„è™šæ‹Ÿå¯¹è±¡æŒ‡é’ˆçš„å¼ é‡ã€‚
        """
        # è·å–æ­£ç¡®çš„å›¾åƒç‰¹å¾
        current_vision_feats, current_vision_pos_embeds, feat_sizes = self.get_im_features(self.inference_state["im"])

        # å°†ç©ºæ©ç å’Œä¸Šè¿°å›¾åƒç‰¹å¾è¾“å…¥ï¼Œè·å–è™šæ‹Ÿå¯¹è±¡æŒ‡é’ˆ
        current_out = self.model.track_step(
            frame_idx=frame_idx,
            is_init_cond_frame=True,
            current_vision_feats=current_vision_feats,
            current_vision_pos_embeds=current_vision_pos_embeds,
            feat_sizes=feat_sizes,
            point_inputs=None,
            # ä½¿ç”¨ä¸€ä¸ªè™šæ‹Ÿï¼ˆç©ºï¼‰æ©ç å’Œå•ä¸ªå¯¹è±¡
            mask_inputs=torch.zeros((1, 1, *self.imgsz), dtype=torch.float32, device=self.device),
            output_dict={},
            num_frames=self.inference_state["num_frames"],
            track_in_reverse=False,
            run_mem_encoder=False,
            prev_sam_mask_logits=None,
        )
        return current_out["obj_ptr"]

    def _run_memory_encoder(self, batch_size, high_res_masks, object_score_logits, is_mask_from_pts):
        """
        åœ¨æ©è†œä¸Šè¿è¡Œå†…å­˜ç¼–ç å™¨ã€‚

        è¿™é€šå¸¸å‘ç”Ÿåœ¨å¯¹å¯¹è±¡åˆ†æ•°åº”ç”¨éé‡å çº¦æŸä¹‹åã€‚ç”±äºå®ƒä»¬çš„åˆ†æ•°å‘ç”Ÿäº†å˜åŒ–ï¼Œå†…å­˜ä¹Ÿéœ€è¦ä½¿ç”¨å†…å­˜ç¼–ç å™¨é‡æ–°è®¡ç®—ã€‚

        å‚æ•°ï¼š
            batch_size (int): å¤„ç†å¸§çš„æ‰¹æ¬¡å¤§å°ã€‚
            high_res_masks (torch.Tensor): ç”¨äºè®¡ç®—å†…å­˜çš„é«˜åˆ†è¾¨ç‡æ©è†œã€‚
            object_score_logits (torch.Tensor): è¡¨ç¤ºå¯¹è±¡åˆ†æ•°çš„logitsã€‚
            is_mask_from_pts (bool): æŒ‡ç¤ºæ©è†œæ˜¯å¦æ¥è‡ªç‚¹äº¤äº’ã€‚

        è¿”å›ï¼š
            (tuple[torch.Tensor, torch.Tensor]): ä¸€ä¸ªåŒ…å«ç¼–ç åçš„æ©è†œç‰¹å¾å’Œä½ç½®ç¼–ç çš„å…ƒç»„ã€‚
        """
        # è·å–æ­£ç¡®çš„å›¾åƒç‰¹å¾
        current_vision_feats, _, feat_sizes = self.get_im_features(self.inference_state["im"], batch_size)
        maskmem_features, maskmem_pos_enc = self.model._encode_new_memory(
            current_vision_feats=current_vision_feats,
            feat_sizes=feat_sizes,
            pred_masks_high_res=high_res_masks,
            is_mask_from_pts=is_mask_from_pts,
            object_score_logits=object_score_logits,
        )

        # "maskmem_pos_enc" åœ¨æ‰€æœ‰å¸§ä¸­æ˜¯ç›¸åŒçš„ï¼Œå› æ­¤æˆ‘ä»¬åªéœ€è¦å­˜å‚¨ä¸€ä¸ªå‰¯æœ¬
        maskmem_pos_enc = self._get_maskmem_pos_enc(maskmem_pos_enc)
        return maskmem_features.to(dtype=torch.float16, device=self.device, non_blocking=True), maskmem_pos_enc

    def _add_output_per_object(self, frame_idx, current_out, storage_key):
        """
        å°†å¤šå¯¹è±¡è¾“å‡ºæ‹†åˆ†ä¸ºæ¯ä¸ªå¯¹è±¡çš„è¾“å‡ºåˆ‡ç‰‡ï¼Œå¹¶å°†å®ƒä»¬æ·»åŠ åˆ° Output_Dict_Per_Obj ä¸­ã€‚

        ç”Ÿæˆçš„åˆ‡ç‰‡å…±äº«ç›¸åŒçš„å¼ é‡å­˜å‚¨ã€‚

        å‚æ•°ï¼š
            frame_idx (int): å½“å‰å¸§çš„ç´¢å¼•ã€‚
            current_out (Dict): å½“å‰è¾“å‡ºå­—å…¸ï¼ŒåŒ…å«å¤šå¯¹è±¡çš„è¾“å‡ºã€‚
            storage_key (str): ç”¨äºå°†è¾“å‡ºå­˜å‚¨åœ¨æ¯ä¸ªå¯¹è±¡è¾“å‡ºå­—å…¸ä¸­çš„é”®ã€‚
        """
        maskmem_features = current_out["maskmem_features"]
        assert maskmem_features is None or isinstance(maskmem_features, torch.Tensor)

        maskmem_pos_enc = current_out["maskmem_pos_enc"]
        assert maskmem_pos_enc is None or isinstance(maskmem_pos_enc, list)

        for obj_idx, obj_output_dict in self.inference_state["output_dict_per_obj"].items():
            obj_slice = slice(obj_idx, obj_idx + 1)
            obj_out = {
                "maskmem_features": None,
                "maskmem_pos_enc": None,
                "pred_masks": current_out["pred_masks"][obj_slice],
                "obj_ptr": current_out["obj_ptr"][obj_slice],
            }
            if maskmem_features is not None:
                obj_out["maskmem_features"] = maskmem_features[obj_slice]
            if maskmem_pos_enc is not None:
                obj_out["maskmem_pos_enc"] = [x[obj_slice] for x in maskmem_pos_enc]
            obj_output_dict[storage_key][frame_idx] = obj_out

    def _clear_non_cond_mem_around_input(self, frame_idx):
        """
        æ¸…é™¤è¾“å…¥å¸§å‘¨å›´çš„éæ¡ä»¶å†…å­˜ã€‚

        å½“ç”¨æˆ·æä¾›ä¿®æ­£ç‚¹å‡»æ—¶ï¼Œå‘¨å›´å¸§çš„éæ¡ä»¶å†…å­˜å¯èƒ½ä»ç„¶åŒ…å«è¿‡æ—¶çš„å¯¹è±¡å¤–è§‚ä¿¡æ¯ï¼Œå¹¶å¯èƒ½ä¼šæ··æ·†æ¨¡å‹ã€‚
        è¿™ä¸ªæ–¹æ³•æ¸…é™¤ä¸äº¤äº’å¸§ç›¸é‚»çš„éæ¡ä»¶å†…å­˜ï¼Œä»¥é¿å…ç»™æ¨¡å‹æä¾›å…³äºå¯¹è±¡çš„æ—§ä¿¡æ¯å’Œæ–°ä¿¡æ¯ã€‚

        å‚æ•°ï¼š
            frame_idx (int): å½“å‰ç”¨æˆ·äº¤äº’å‘ç”Ÿçš„å¸§çš„ç´¢å¼•ã€‚
        """
        r = self.model.memory_temporal_stride_for_eval
        frame_idx_begin = frame_idx - r * self.model.num_maskmem
        frame_idx_end = frame_idx + r * self.model.num_maskmem
        for t in range(frame_idx_begin, frame_idx_end + 1):
            self.inference_state["output_dict"]["non_cond_frame_outputs"].pop(t, None)
            for obj_output_dict in self.inference_state["output_dict_per_obj"].values():
                obj_output_dict["non_cond_frame_outputs"].pop(t, None)

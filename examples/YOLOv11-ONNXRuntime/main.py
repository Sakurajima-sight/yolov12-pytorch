# Ultralytics ğŸš€ AGPL-3.0 è®¸å¯è¯ - https://ultralytics.com/license

import argparse
import cv2
import numpy as np
import onnxruntime as ort
import torch

from ultralytics.utils import ASSETS, yaml_load
from ultralytics.utils.checks import check_requirements, check_yaml
import tkinter as tk


class YOLOv11:
    """YOLOv11 ç›®æ ‡æ£€æµ‹æ¨¡å‹ç±»ï¼Œç”¨äºå¤„ç†æ¨ç†å’Œå¯è§†åŒ–ä»»åŠ¡"""

    def __init__(self, onnx_model, input_image, confidence_thres, iou_thres):
        """
        åˆå§‹åŒ– YOLOv11 ç±»çš„å®ä¾‹ã€‚

        å‚æ•°:
            onnx_model: ONNX æ¨¡å‹çš„è·¯å¾„ã€‚
            input_image: è¾“å…¥å›¾åƒçš„è·¯å¾„ã€‚
            confidence_thres: ç½®ä¿¡åº¦é˜ˆå€¼ï¼Œç”¨äºç­›é€‰æ£€æµ‹ç»“æœã€‚
            iou_thres: IoUï¼ˆäº¤å¹¶æ¯”ï¼‰é˜ˆå€¼ï¼Œç”¨äºéæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰ã€‚
        """
        self.onnx_model = onnx_model
        self.input_image = input_image
        self.confidence_thres = confidence_thres
        self.iou_thres = iou_thres

        # ä» COCO æ•°æ®é›†åŠ è½½ç±»åˆ«åç§°
        self.classes = yaml_load(check_yaml("coco8.yaml"))["names"]

        # ä¸ºç±»åˆ«ç”Ÿæˆéšæœºé¢œè‰²è°ƒè‰²æ¿
        self.color_palette = np.random.uniform(0, 255, size=(len(self.classes), 3))

    def draw_detections(self, img, box, score, class_id):
        """
        åœ¨è¾“å…¥å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹åˆ°çš„è¾¹ç•Œæ¡†å’Œæ ‡ç­¾ã€‚

        å‚æ•°:
            img: éœ€è¦ç»˜åˆ¶æ£€æµ‹æ¡†çš„è¾“å…¥å›¾åƒã€‚
            box: æ£€æµ‹åˆ°çš„è¾¹ç•Œæ¡†åæ ‡ã€‚
            score: è¯¥æ£€æµ‹æ¡†çš„ç½®ä¿¡åº¦åˆ†æ•°ã€‚
            class_id: è¯¥æ£€æµ‹ç›®æ ‡çš„ç±»åˆ« IDã€‚

        è¿”å›:
            None
        """
        # æå–è¾¹ç•Œæ¡†çš„åæ ‡
        x1, y1, w, h = box

        # è·å–ç±»åˆ« ID å¯¹åº”çš„é¢œè‰²
        color = self.color_palette[class_id]

        # åœ¨å›¾åƒä¸Šç»˜åˆ¶è¾¹ç•Œæ¡†
        cv2.rectangle(img, (int(x1), int(y1)), (int(x1 + w), int(y1 + h)), color, 2)

        # ç”Ÿæˆç±»åˆ«æ ‡ç­¾æ–‡æœ¬ï¼ˆåŒ…å«ç±»åˆ«åç§°å’Œç½®ä¿¡åº¦åˆ†æ•°ï¼‰
        label = f"{self.classes[class_id]}: {score:.2f}"

        # è®¡ç®—æ–‡æœ¬çš„å°ºå¯¸
        (label_width, label_height), _ = cv2.getTextSize(label, cv2.FONT_HERSHEY_SIMPLEX, 0.5, 1)

        # è®¡ç®—æ–‡æœ¬çš„ä½ç½®
        label_x = x1
        label_y = y1 - 10 if y1 - 10 > label_height else y1 + 10

        # ç»˜åˆ¶ä¸€ä¸ªå¡«å……çŸ©å½¢ä½œä¸ºæ ‡ç­¾èƒŒæ™¯
        cv2.rectangle(
            img, (label_x, label_y - label_height), (label_x + label_width, label_y + label_height), color, cv2.FILLED
        )

        # åœ¨å›¾åƒä¸Šç»˜åˆ¶æ–‡æœ¬æ ‡ç­¾
        cv2.putText(img, label, (label_x, label_y), cv2.FONT_HERSHEY_SIMPLEX, 0.5, (0, 0, 0), 1, cv2.LINE_AA)

    def preprocess(self):
        """
        å¯¹è¾“å…¥å›¾åƒè¿›è¡Œé¢„å¤„ç†ï¼Œä»¥ä¾¿è¿›è¡Œæ¨ç†ã€‚

        è¿”å›:
            image_data: é¢„å¤„ç†åçš„å›¾åƒæ•°æ®ï¼Œå‡†å¤‡ç”¨äºæ¨ç†ã€‚
        """
        # ä½¿ç”¨ OpenCV è¯»å–è¾“å…¥å›¾åƒ
        self.img = cv2.imread(self.input_image)

        # è·å–è¾“å…¥å›¾åƒçš„é«˜åº¦å’Œå®½åº¦
        self.img_height, self.img_width = self.img.shape[:2]

        # å°†å›¾åƒçš„é¢œè‰²ç©ºé—´ä» BGR è½¬æ¢ä¸º RGB
        img = cv2.cvtColor(self.img, cv2.COLOR_BGR2RGB)

        # è°ƒæ•´å›¾åƒå¤§å°ä»¥åŒ¹é…æ¨¡å‹çš„è¾“å…¥å°ºå¯¸
        img = cv2.resize(img, (self.input_width, self.input_height))

        # å½’ä¸€åŒ–å›¾åƒæ•°æ®ï¼ˆå°†åƒç´ å€¼ç¼©æ”¾åˆ° [0,1]ï¼‰
        image_data = np.array(img) / 255.0

        # é‡æ–°æ’åˆ—ç»´åº¦ï¼Œå°†é€šé“ç»´åº¦æ”¾åˆ°æœ€å‰é¢
        image_data = np.transpose(image_data, (2, 0, 1))  # é€šé“ä¼˜å…ˆ

        # å¢åŠ  batch ç»´åº¦ï¼Œä½¿å…¶ç¬¦åˆæ¨¡å‹è¾“å…¥å½¢çŠ¶
        image_data = np.expand_dims(image_data, axis=0).astype(np.float32)

        # è¿”å›é¢„å¤„ç†åçš„å›¾åƒæ•°æ®
        return image_data

    def postprocess(self, input_image, output):
        """
        å¯¹æ¨¡å‹çš„è¾“å‡ºç»“æœè¿›è¡Œåå¤„ç†ï¼Œæå–è¾¹ç•Œæ¡†ã€ç½®ä¿¡åº¦å’Œç±»åˆ« IDã€‚

        å‚æ•°:
            input_image (numpy.ndarray): åŸå§‹è¾“å…¥å›¾åƒã€‚
            output (numpy.ndarray): æ¨¡å‹è¾“å‡ºçš„æ•°æ®ã€‚

        è¿”å›:
            numpy.ndarray: å¸¦æœ‰æ£€æµ‹ç»“æœçš„å›¾åƒã€‚
        """
        # è½¬ç½®å¹¶å‹ç¼©è¾“å‡ºæ•°æ®ï¼Œä½¿å…¶ç¬¦åˆé¢„æœŸå½¢çŠ¶
        outputs = np.transpose(np.squeeze(output[0]))

        # è·å–è¾“å‡ºæ•°æ®çš„è¡Œæ•°
        rows = outputs.shape[0]

        # å­˜å‚¨æ£€æµ‹åˆ°çš„è¾¹ç•Œæ¡†ã€ç½®ä¿¡åº¦å’Œç±»åˆ« ID
        boxes = []
        scores = []
        class_ids = []

        # è®¡ç®—è¾¹ç•Œæ¡†åæ ‡çš„ç¼©æ”¾å› å­
        x_factor = self.img_width / self.input_width
        y_factor = self.img_height / self.input_height

        # éå†æ¯ä¸€è¡Œçš„è¾“å‡ºæ•°æ®
        for i in range(rows):
            # æå–è¯¥è¡Œçš„ç±»åˆ«å¾—åˆ†
            classes_scores = outputs[i][4:]

            # è·å–æœ€é«˜ç±»åˆ«å¾—åˆ†
            max_score = np.amax(classes_scores)

            # å¦‚æœæœ€é«˜å¾—åˆ†è¶…è¿‡ç½®ä¿¡åº¦é˜ˆå€¼
            if max_score >= self.confidence_thres:
                # è·å–å¾—åˆ†æœ€é«˜çš„ç±»åˆ« ID
                class_id = np.argmax(classes_scores)

                # æå–è¾¹ç•Œæ¡†åæ ‡
                x, y, w, h = outputs[i][0], outputs[i][1], outputs[i][2], outputs[i][3]

                # è®¡ç®—ç¼©æ”¾åçš„è¾¹ç•Œæ¡†åæ ‡
                left = int((x - w / 2) * x_factor)
                top = int((y - h / 2) * y_factor)
                width = int(w * x_factor)
                height = int(h * y_factor)

                # å­˜å‚¨ç±»åˆ« IDã€å¾—åˆ†å’Œè¾¹ç•Œæ¡†
                class_ids.append(class_id)
                scores.append(max_score)
                boxes.append([left, top, width, height])

        # åº”ç”¨éæå¤§å€¼æŠ‘åˆ¶ï¼ˆNMSï¼‰è¿‡æ»¤é‡å çš„è¾¹ç•Œæ¡†
        indices = cv2.dnn.NMSBoxes(boxes, scores, self.confidence_thres, self.iou_thres)

        # éå† NMS è¿‡æ»¤åçš„ç»“æœ
        for i in indices:
            # è·å–å¯¹åº”çš„è¾¹ç•Œæ¡†ã€å¾—åˆ†å’Œç±»åˆ« ID
            box = boxes[i]
            score = scores[i]
            class_id = class_ids[i]

            # åœ¨å›¾åƒä¸Šç»˜åˆ¶æ£€æµ‹ç»“æœ
            self.draw_detections(input_image, box, score, class_id)

        # è¿”å›å¸¦æœ‰æ£€æµ‹ç»“æœçš„å›¾åƒ
        return input_image

    def main(self):
        """
        ä½¿ç”¨ ONNX æ¨¡å‹æ‰§è¡Œç›®æ ‡æ£€æµ‹ï¼Œå¹¶è¿”å›å¸¦æœ‰æ£€æµ‹æ¡†çš„å›¾åƒã€‚

        è¿”å›:
            output_img: å¤„ç†åçš„è¾“å‡ºå›¾åƒï¼Œå¸¦æœ‰æ£€æµ‹æ¡†ã€‚
        """
        # åˆ›å»º ONNX è¿è¡Œæ—¶ä¼šè¯ï¼Œæ”¯æŒ GPUï¼ˆCUDAï¼‰æˆ– CPU è¿è¡Œ
        session = ort.InferenceSession(self.onnx_model, providers=["CUDAExecutionProvider", "CPUExecutionProvider"])

        # è·å–æ¨¡å‹è¾“å…¥ä¿¡æ¯
        model_inputs = session.get_inputs()

        # è¯»å–è¾“å…¥å¼ é‡çš„å°ºå¯¸
        input_shape = model_inputs[0].shape
        self.input_width = input_shape[2]
        self.input_height = input_shape[3]

        # é¢„å¤„ç†è¾“å…¥å›¾åƒ
        img_data = self.preprocess()

        # æ‰§è¡Œæ¨ç†
        outputs = session.run(None, {model_inputs[0].name: img_data})

        # è¿›è¡Œåå¤„ç†å¹¶è¿”å›ç»“æœ
        return self.postprocess(self.img, outputs)


if __name__ == "__main__":
    # è§£æå‘½ä»¤è¡Œå‚æ•°
    parser = argparse.ArgumentParser()
    parser.add_argument("--model", type=str, default="yolov11n.onnx", help="è¾“å…¥ ONNX æ¨¡å‹è·¯å¾„ã€‚")
    parser.add_argument("--img", type=str, default=str(ASSETS / "bus.jpg"), help="è¾“å…¥å›¾åƒè·¯å¾„ã€‚")
    parser.add_argument("--conf-thres", type=float, default=0.5, help="ç½®ä¿¡åº¦é˜ˆå€¼")
    parser.add_argument("--iou-thres", type=float, default=0.5, help="NMS IoU é˜ˆå€¼")
    args = parser.parse_args()

    # åˆ›å»º YOLOv11 å®ä¾‹
    detector = YOLOv11(
        onnx_model=args.model,
        input_image=args.img,
        confidence_thres=args.conf_thres,
        iou_thres=args.iou_thres
    )

    # æ‰§è¡Œæ£€æµ‹
    output_image = detector.main()

    # è·å–å±å¹•å¤§å°
    root = tk.Tk()
    screen_width = root.winfo_screenwidth()  # å±å¹•å®½åº¦
    screen_height = root.winfo_screenheight()  # å±å¹•é«˜åº¦
    root.destroy()  # å…³é—­ Tkinter çª—å£

    # è®¡ç®—ç¼©æ”¾æ¯”ä¾‹ï¼Œä¿æŒå®½é«˜æ¯”
    height, width = output_image.shape[:2]
    scale = min((screen_width / width) * 0.8, (screen_height / height) * 0.8)

    # ä»…å½“å›¾ç‰‡è¿‡å¤§æ—¶ç¼©å°
    if scale < 1:
        new_width = int(width * scale)
        new_height = int(height * scale)
        output_image = cv2.resize(output_image, (new_width, new_height), interpolation=cv2.INTER_AREA)

    # æ˜¾ç¤ºå¸¦æœ‰æ ‡æ³¨çš„è¾“å‡ºå›¾åƒ
    cv2.imshow("Output", output_image)
    cv2.waitKey(0)


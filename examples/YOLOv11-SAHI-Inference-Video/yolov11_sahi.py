# Ultralytics ğŸš€ AGPL-3.0 è®¸å¯è¯ - https://ultralytics.com/license

import argparse
from pathlib import Path

import cv2
from sahi import AutoDetectionModel
from sahi.predict import get_sliced_prediction
from sahi.utils.ultralytics import download_yolo11n_model

from ultralytics.utils.files import increment_path
from ultralytics.utils.plotting import Annotator, colors

import multiprocessing
import torch

cv2.setNumThreads(multiprocessing.cpu_count())  # ğŸš€ è®© OpenCV å……åˆ†ä½¿ç”¨ CPU çº¿ç¨‹

class SAHIInference:
    """è¿è¡Œ Ultralytics YOLO11 å’Œ SAHI è¿›è¡Œè§†é¢‘ç›®æ ‡æ£€æµ‹ï¼Œå¹¶æä¾›å¯è§†åŒ–ã€ä¿å­˜å’Œè·Ÿè¸ªç»“æœçš„åŠŸèƒ½ã€‚"""

    def __init__(self):
        """åˆå§‹åŒ– SAHIInference ç±»ï¼Œä½¿ç”¨ SAHI å’Œ YOLO11 æ¨¡å‹è¿›è¡Œåˆ‡ç‰‡æ¨ç†ã€‚"""
        self.detection_model = None

    def load_model(self, weights):
        """åŠ è½½ YOLO11 æ¨¡å‹ï¼Œå¹¶ä½¿ç”¨ SAHI è¿›è¡Œç›®æ ‡æ£€æµ‹ã€‚"""
        yolo11_model_path = f"models/{weights}"
        download_yolo11n_model(yolo11_model_path)  # ä¸‹è½½ YOLO11 é¢„è®­ç»ƒæ¨¡å‹

        self.detection_model = AutoDetectionModel.from_pretrained(
            model_type="ultralytics", model_path=yolo11_model_path, device="cpu"
        )
        # ğŸ› ï¸ å¼€å¯ CUDNN åŠ é€Ÿï¼ˆå¦‚æœ GPU æ”¯æŒï¼‰
        torch.backends.cudnn.benchmark = True
        torch.backends.cudnn.enabled = True
 
    def inference(
        self,
        weights="yolo11n.pt",
        source="test.mp4",
        view_img=False,
        save_img=False,
        exist_ok=False,
        classes=None,  # ğŸ†• ä»å‘½ä»¤è¡Œè·å–
    ):
        """
        è¿è¡Œ YOLO11 å’Œ SAHI è¿›è¡Œè§†é¢‘ç›®æ ‡æ£€æµ‹ï¼Œå¹¶æ”¯æŒæŒ‰ç±»åˆ«ç­›é€‰ã€‚

        å‚æ•°:
            weights (str): æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„ã€‚
            source (str): è§†é¢‘æ–‡ä»¶è·¯å¾„ã€‚
            view_img (bool): æ˜¯å¦åœ¨çª—å£ä¸­æ˜¾ç¤ºæ£€æµ‹ç»“æœã€‚
            save_img (bool): æ˜¯å¦ä¿å­˜æ£€æµ‹ç»“æœçš„è§†é¢‘ã€‚
            exist_ok (bool): æ˜¯å¦å…è®¸è¦†ç›–å·²æœ‰çš„è¾“å‡ºæ–‡ä»¶ã€‚
            classes (list): åªæ£€æµ‹è¿™äº›ç±»åˆ«ï¼Œä¾‹å¦‚ ["person", "car"]
        """
        # è§†é¢‘è®¾ç½®
        cap = cv2.VideoCapture(source)
        assert cap.isOpened(), "æ— æ³•è¯»å–è§†é¢‘æ–‡ä»¶"
        frame_width, frame_height = int(cap.get(3)), int(cap.get(4))  # è·å–è§†é¢‘å¸§çš„å®½åº¦å’Œé«˜åº¦

        # è¾“å‡ºç›®å½•è®¾ç½®
        save_dir = increment_path(Path("ultralytics_results_with_sahi") / "exp", exist_ok)
        save_dir.mkdir(parents=True, exist_ok=True)
        video_writer = cv2.VideoWriter(
            str(save_dir / f"{Path(source).stem}.mp4"),
            cv2.VideoWriter_fourcc(*"MJPG"),
            int(cap.get(5)),  # è·å–è§†é¢‘å¸§ç‡
            (frame_width, frame_height),
        )

        # åŠ è½½æ¨¡å‹
        self.load_model(weights)
        print("SAHI æ­£åœ¨ä½¿ç”¨çš„è®¾å¤‡:", self.detection_model.device)
        if classes:
            print(f"ğŸ” ä»…æ£€æµ‹ä»¥ä¸‹ç±»åˆ«: {classes}")

        # é€å¸§å¤„ç†è§†é¢‘
        while cap.isOpened():
            success, frame = cap.read()
            if not success:
                break
            annotator = Annotator(frame)  # åˆå§‹åŒ–æ ‡æ³¨å·¥å…·ï¼Œç”¨äºç»˜åˆ¶æ£€æµ‹å’Œè·Ÿè¸ªç»“æœ

            # è¿›è¡Œç›®æ ‡æ£€æµ‹ï¼Œå¹¶æŒ‰ç±»åˆ«ç­›é€‰
            results = get_sliced_prediction(
                frame[..., ::-1],  # BGR -> RGB é¢œè‰²è½¬æ¢
                self.detection_model,
                slice_height=512,  # åˆ‡ç‰‡é«˜åº¦
                slice_width=512,  # åˆ‡ç‰‡å®½åº¦
                overlap_height_ratio=0.1,  # ğŸš€ å‡å°‘é‡å ï¼Œæé«˜é€Ÿåº¦
                overlap_width_ratio=0.1,
                perform_standard_pred=True,
            )

            # æå–æ£€æµ‹ç»“æœæ•°æ®
            detection_data = [
                (det.category.name, det.category.id, (det.bbox.minx, det.bbox.miny, det.bbox.maxx, det.bbox.maxy))
                for det in results.object_prediction_list
                if classes is None or det.category.name in classes  # ğŸ†• åªä¿ç•™éœ€è¦çš„ç±»åˆ«
            ]

            # åœ¨è§†é¢‘å¸§ä¸Šç»˜åˆ¶æ£€æµ‹æ¡†
            for det in detection_data:
                annotator.box_label(det[2], label=str(det[0]), color=colors(int(det[1]), True))

            # æ˜¾ç¤ºæ£€æµ‹ç»“æœ
            if view_img:
                cv2.imshow(Path(source).stem, frame)
            # ä¿å­˜æ£€æµ‹ç»“æœ
            if save_img:
                video_writer.write(frame)

            # æŒ‰ "q" é€€å‡º
            if cv2.waitKey(1) & 0xFF == ord("q"):
                break

        # é‡Šæ”¾èµ„æº
        video_writer.release()
        cap.release()
        cv2.destroyAllWindows()

    def parse_opt(self):
        """è§£æå‘½ä»¤è¡Œå‚æ•°ã€‚"""
        parser = argparse.ArgumentParser()
        parser.add_argument("--weights", type=str, default="yolo11n.pt", help="æ¨¡å‹æƒé‡æ–‡ä»¶è·¯å¾„")
        parser.add_argument("--source", type=str, required=True, help="è§†é¢‘æ–‡ä»¶è·¯å¾„")
        parser.add_argument("--view-img", action="store_true", help="æ˜¯å¦æ˜¾ç¤ºæ£€æµ‹ç»“æœ")
        parser.add_argument("--save-img", action="store_true", help="æ˜¯å¦ä¿å­˜æ£€æµ‹ç»“æœè§†é¢‘")
        parser.add_argument("--exist-ok", action="store_true", help="æ˜¯å¦å…è®¸è¦†ç›–å·²æœ‰çš„è¾“å‡ºæ–‡ä»¶")
        parser.add_argument("--classes", nargs="+", type=str, help="è¦æ£€æµ‹çš„ç±»åˆ«åç§°ï¼Œä¾‹å¦‚ --classes person car")
        return parser.parse_args()


if __name__ == "__main__":
    inference = SAHIInference()
    inference.inference(**vars(inference.parse_opt()))
